{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29ecfc97",
      "metadata": {
        "id": "29ecfc97"
      },
      "source": [
        "## Домашнее задание 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "760703c8",
      "metadata": {
        "id": "760703c8"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133684bd",
      "metadata": {
        "id": "133684bd"
      },
      "source": [
        "## Решение:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc2c424",
      "metadata": {
        "id": "2cc2c424"
      },
      "source": [
        "Загрузим библиотеки необходимые для дальнейшего обучения."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7AAohSFnqn",
        "outputId": "bad11f8e-42cc-467b-fd57-20ec43309f1a"
      },
      "id": "Np7AAohSFnqn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 158 kB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 49.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: intervaltree, docopt\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=4b51ed3b1526cecc049a7827e754b5e10813405ce4799e021a8a421ff0f39c13\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=97f102708f7f48882289803ea9e9604b04dd803f2b43718fa3e586fec125fa2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built intervaltree docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, razdel, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVWPrgoPVsr7",
        "outputId": "a95144b4-972f-4adb-e481-bf2eeaa8772c"
      },
      "id": "TVWPrgoPVsr7",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0b0219a7",
      "metadata": {
        "id": "0b0219a7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86623282",
      "metadata": {
        "id": "86623282"
      },
      "source": [
        "Создадим функцию для лемматизации текста на основе библиотеки Natasha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3e9fda63",
      "metadata": {
        "id": "3e9fda63"
      },
      "outputs": [],
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "def natasha_lemmatize(text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "    return \" \".join(_.lemma for _ in doc.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "092ee930",
      "metadata": {
        "id": "092ee930"
      },
      "source": [
        "Сформируем общий датасет на основе наборов для обучения и теста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9db7518d",
      "metadata": {
        "id": "9db7518d"
      },
      "outputs": [],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)\n",
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a37e8fbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a37e8fbf",
        "outputId": "8e63f788-b52f-4a52-a470-d151ba9190b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df['text'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d06fdb73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d06fdb73",
        "outputId": "14d4532d-27b2-4d45-a557-7513c21a343b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'да , все-таки он немного похожий на он . но мой мальчик весь равный лучше : d'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "natasha_lemmatize(df['text'][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8104d36",
      "metadata": {
        "id": "f8104d36"
      },
      "source": [
        "Проведем лемматизацию твитов в датасете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "63ee9d89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "63ee9d89",
        "outputId": "e2324cce-254e-4e0f-bff9-840f7ad134fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                               text     label  \\\n",
              "0      0  @first_timee хоть я и школота, но поверь, у на...  positive   \n",
              "1      1  Да, все-таки он немного похож на него. Но мой ...  positive   \n",
              "2      2  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...  positive   \n",
              "3      3  RT @digger2912: \"Кто то в углу сидит и погибае...  positive   \n",
              "4      4  @irina_dyshkant Вот что значит страшилка :D\\nН...  positive   \n",
              "\n",
              "                                     lemmatized_text  \n",
              "0  @ first_timee хоть я и школоть , но поверь , у...  \n",
              "1  да , все-таки он немного похожий на он . но мо...  \n",
              "2  rt @ katiacheh : ну ты идиотка ) я испугаться ...  \n",
              "3  rt @ digger 2912 : \" кто тот в угол сидеть и п...  \n",
              "4  @ irina_dyshkant вот что значить страшилка : d...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a6cef8c1-03cc-4da3-b2ce-53b9e0593f3f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>lemmatized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>positive</td>\n",
              "      <td>@ first_timee хоть я и школоть , но поверь , у...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>да , все-таки он немного похожий на он . но мо...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>rt @ katiacheh : ну ты идиотка ) я испугаться ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>positive</td>\n",
              "      <td>rt @ digger 2912 : \" кто тот в угол сидеть и п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>positive</td>\n",
              "      <td>@ irina_dyshkant вот что значить страшилка : d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6cef8c1-03cc-4da3-b2ce-53b9e0593f3f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6cef8c1-03cc-4da3-b2ce-53b9e0593f3f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6cef8c1-03cc-4da3-b2ce-53b9e0593f3f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df['lemmatized_text'] = df['text'].apply(natasha_lemmatize)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f889f663",
      "metadata": {
        "id": "f889f663"
      },
      "source": [
        "Создадим корпус используемых в твитах слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "923aab84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "923aab84",
        "outputId": "70cbc528-f9b1-4974-d658-8f3da3032997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2059377\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first_timee',\n",
              " 'школоть',\n",
              " 'поверь',\n",
              " 'самый',\n",
              " 'd',\n",
              " 'общество',\n",
              " 'профилировать',\n",
              " 'предмет',\n",
              " 'тип',\n",
              " 'все-таки']"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)\n",
        "corpus = [token for tweet in df['lemmatized_text'] \n",
        "          for token in nltk.tokenize.word_tokenize(tweet) if token not in noise]\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b4ef43",
      "metadata": {
        "id": "e9b4ef43"
      },
      "source": [
        "Посмотрим на самые часто встречающиеся слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "a0bb6130",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0bb6130",
        "outputId": "0348131c-2a9a-4b48-a37a-7bebcfb9149d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rt', 38132),\n",
              " ('t', 33881),\n",
              " ('http', 33072),\n",
              " ('co', 33063),\n",
              " ('весь', 29865),\n",
              " ('``', 23933),\n",
              " ('это', 22598),\n",
              " ('...', 22363),\n",
              " ('d', 18954),\n",
              " ('хотеть', 11790)]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916e46a2",
      "metadata": {
        "id": "916e46a2"
      },
      "source": [
        "Выделим три вида токенов в зависимости от частоты их появления в твитах и обучим логистическую регрессию на основе каждого вида."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "24ecdba7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ecdba7",
        "outputId": "9ddc4532-8a91-4c20-be5e-3e9366e058c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt', 't', 'http', 'co', 'весь', '``', 'это', '...', 'd', 'хотеть']"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "most_frequent_tokens = [item[0] for item in freq_dict.most_common(90000)][:30000]\n",
        "avg_frequent_tokens = [item[0] for item in freq_dict.most_common(90000)][30000:60000]\n",
        "less_frequent_tokens = [item[0] for item in freq_dict.most_common(90000)][60000:]\n",
        "most_frequent_tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим датасет на тренировчную и валидационную части."
      ],
      "metadata": {
        "id": "HSATXrPbbe5o"
      },
      "id": "HSATXrPbbe5o"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df['lemmatized_text'], df.label, random_state=42)"
      ],
      "metadata": {
        "id": "G5EFyHz2beMU"
      },
      "id": "G5EFyHz2beMU",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0a44b5fd",
      "metadata": {
        "id": "0a44b5fd"
      },
      "source": [
        "Классификатор на токенах с самой высокой частотой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "aac4ee69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aac4ee69",
        "outputId": "be09e294-c089-494e-e41c-2439a9c93426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.75      0.77     29043\n",
            "    positive       0.75      0.78      0.76     27666\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.77      0.77      0.76     56709\n",
            "weighted avg       0.77      0.76      0.77     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.tokenize.word_tokenize, vocabulary=most_frequent_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde78848",
      "metadata": {
        "id": "cde78848"
      },
      "source": [
        "Классификатор на токенах со средней частотой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "c333443e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c333443e",
        "outputId": "b74ca4de-9a4e-49f2-8851-ef19a5c71b9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.53      0.67     48686\n",
            "    positive       0.20      0.71      0.31      8023\n",
            "\n",
            "    accuracy                           0.55     56709\n",
            "   macro avg       0.56      0.62      0.49     56709\n",
            "weighted avg       0.81      0.55      0.62     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.tokenize.word_tokenize, vocabulary=avg_frequent_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1941e540",
      "metadata": {
        "id": "1941e540"
      },
      "source": [
        "Классификатор на токенах с низкой частотой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "4a3a0822",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a3a0822",
        "outputId": "537307d4-2396-43d0-e8ba-92430466cbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.08      0.72      0.15      3230\n",
            "    positive       0.97      0.52      0.68     53479\n",
            "\n",
            "    accuracy                           0.53     56709\n",
            "   macro avg       0.53      0.62      0.41     56709\n",
            "weighted avg       0.92      0.53      0.65     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.tokenize.word_tokenize, vocabulary=less_frequent_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc318db",
      "metadata": {
        "id": "3dc318db"
      },
      "source": [
        "Как видим наилучший результат классификатора получается при обучении на самых часто встречаемых токенах. На мой взгляд это логично, так как благодаря работе с наиболее часто встречающимися токенами мы получаем больший датасет для обучения модели."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0efb3a30",
      "metadata": {
        "id": "0efb3a30"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad47258",
      "metadata": {
        "id": "3ad47258"
      },
      "source": [
        "## Решение:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa329d2",
      "metadata": {
        "id": "9fa329d2"
      },
      "source": [
        "Вернемся к классификатору на самых частотных токенах как к показавшему наилучшие результаты и определим наиболее значимые признаки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "14e310e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14e310e2",
        "outputId": "c7beddc1-3141-41be-d980-4f05880b991c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.75      0.77     29043\n",
            "    positive       0.75      0.78      0.76     27666\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.77      0.77      0.76     56709\n",
            "weighted avg       0.77      0.76      0.77     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.tokenize.word_tokenize, vocabulary=most_frequent_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96caf7fc",
      "metadata": {
        "id": "96caf7fc"
      },
      "source": [
        "Напечатаем наиболее значимые токены. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "d3b6090c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3b6090c",
        "outputId": "0e2c73fa-1aa1-4c96-d7ca-03b36cb940b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5354,  3906, 10438,  1019,  2493,  8889,  5356,  4752,  3978,\n",
              "       10521,  8485, 18192, 18135,  1482,  2576,  1369,     8,  1161,\n",
              "         399,   154])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "coefs = clf.coef_[0]\n",
        "top_coefs = np.argsort(coefs)[-20:]\n",
        "top_coefs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "2979c674",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2979c674",
        "outputId": "9d01c3f4-43e9-4b68-ac9a-0b6f5a1f875a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('dd', 154)\n",
            "('ddd', 399)\n",
            "('dddd', 1161)\n",
            "('d', 8)\n",
            "('xd', 1369)\n",
            "('ddddd', 2576)\n",
            "('ахахи', 1482)\n",
            "('zaharikov', 18135)\n",
            "('запарен', 18192)\n",
            "('эротический', 8485)\n",
            "('dddddddd', 10521)\n",
            "('juljulianapai', 3978)\n",
            "('jaredleto', 4752)\n",
            "('dddddd', 5356)\n",
            "('листаться', 8889)\n",
            "('ржа', 2493)\n",
            "('хахах', 1019)\n",
            "('iron_wings', 10438)\n",
            "('спасииибо', 3906)\n",
            "('постепенно', 5354)\n"
          ]
        }
      ],
      "source": [
        "tokens = list(vec.vocabulary_.items())\n",
        "for _ in reversed(top_coefs):\n",
        "    print(tokens[_])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e575fc",
      "metadata": {
        "id": "85e575fc"
      },
      "source": [
        "Как видим наиболее значимыми оказываются токены, которые получились после преобразования смайлов, а также те, которые имеют явно выраженную эмоциональную окраску (\"ржа\", \"спасииибо\", \"запарен\")."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5641e7b7",
      "metadata": {
        "id": "5641e7b7"
      },
      "source": [
        "### Задание 3.\n",
        "\n",
        "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
        "\n",
        "2) подобрать оптимальный размер для hashing векторайзера \n",
        "\n",
        "3) убедиться что для сетки нет переобучения"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88801e74",
      "metadata": {
        "id": "88801e74"
      },
      "source": [
        "## Решение:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b226da8",
      "metadata": {
        "id": "0b226da8"
      },
      "source": [
        "Построим модели на основе других векторайзеров и сравним получившиеся результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "dbe89a37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbe89a37",
        "outputId": "62200f09-73ad-4e9f-ea1f-31811e0555d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.76      0.76     28004\n",
            "    positive       0.77      0.76      0.76     28705\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=nltk.tokenize.word_tokenize, vocabulary=most_frequent_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "8fca7d91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fca7d91",
        "outputId": "6b91115a-6493-4a73-9e44-4046e4614a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of features - 524288:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.76      0.74     26921\n",
            "    positive       0.77      0.74      0.75     29788\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n",
            "Number of features - 1048576:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.76      0.74     26935\n",
            "    positive       0.77      0.74      0.76     29774\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n",
            "Number of features - 2097152:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.76      0.74     26954\n",
            "    positive       0.77      0.74      0.75     29755\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n",
            "Number of features - 4194304:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.76      0.74     26938\n",
            "    positive       0.77      0.74      0.76     29771\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "features_num = [2**19, 2**20, 2**21, 2**22]\n",
        "\n",
        "for n_features in features_num:\n",
        "    print(f'Number of features - {n_features}:')\n",
        "    vec = HashingVectorizer(n_features=n_features,)\n",
        "    bow = vec.fit_transform(x_train)\n",
        "    clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "    clf.fit(bow, y_train)\n",
        "    pred = clf.predict(vec.transform(x_test))\n",
        "    print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимальное количество фичей в модели с HashingVectorizer - *1048576*."
      ],
      "metadata": {
        "id": "60bhf0gufuUC"
      },
      "id": "60bhf0gufuUC"
    },
    {
      "cell_type": "code",
      "source": [
        "vec = HashingVectorizer(n_features=2**19,)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42, max_iter=500)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a2c5Tmhcn3h",
        "outputId": "7775314c-b39c-4a5c-fa0a-640a04536672"
      },
      "id": "8a2c5Tmhcn3h",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.76      0.74     26921\n",
            "    positive       0.77      0.74      0.75     29788\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь обучим нейронную сеть."
      ],
      "metadata": {
        "id": "m-8ahqJHvZB-"
      },
      "id": "m-8ahqJHvZB-"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_bin = np.where(y_train=='positive', 1, 0)\n",
        "y_test_bin = np.where(y_test=='positive', 1, 0)"
      ],
      "metadata": {
        "id": "tYDK3Hk735Qn"
      },
      "id": "tYDK3Hk735Qn",
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train_bin))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((x_test, y_test_bin))"
      ],
      "metadata": {
        "id": "OO1XhNxuf31a"
      },
      "id": "OO1XhNxuf31a",
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.batch(32)\n",
        "valid_data = valid_data.batch(32)"
      ],
      "metadata": {
        "id": "6yeSbBuHf9Q0"
      },
      "id": "6yeSbBuHf9Q0",
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "tnPEQ21Wf_3O"
      },
      "id": "tnPEQ21Wf_3O",
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    \n",
        "    return input_data\n",
        "\n",
        "vocab_size = 15000\n",
        "seq_len = 140\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ],
      "metadata": {
        "id": "K-J539QUgBjT"
      },
      "id": "K-J539QUgBjT",
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim=250\n",
        "\n",
        "model = Sequential([\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    Conv1D(embedding_dim, 3),\n",
        "    Conv1D(embedding_dim, 2),\n",
        "    GRU(350),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "QK6g4JXngDmj"
      },
      "id": "QK6g4JXngDmj",
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-3)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-pspXm7wgKZu"
      },
      "id": "-pspXm7wgKZu",
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data, validation_data=valid_data, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPS9VvQ9gMLA",
        "outputId": "70722198-db8a-4b94-c2a6-df31aed12f34"
      },
      "id": "VPS9VvQ9gMLA",
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "5317/5317 [==============================] - 114s 21ms/step - loss: 0.1829 - accuracy: 0.9216 - val_loss: 0.2963 - val_accuracy: 0.8497\n",
            "Epoch 2/2\n",
            "5317/5317 [==============================] - 108s 20ms/step - loss: 0.2361 - accuracy: 0.9007 - val_loss: 0.1894 - val_accuracy: 0.9173\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f216f014e90>"
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.predict(valid_data)\n",
        "prediction = tf.round(tf.nn.sigmoid(logits))\n",
        "class_preds = np.where(prediction==1, 'positive', 'negative')"
      ],
      "metadata": {
        "id": "eMmf8qu95aWi"
      },
      "id": "eMmf8qu95aWi",
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построим для данной модели матрицу ошибок."
      ],
      "metadata": {
        "id": "im8ZweHkcdqK"
      },
      "id": "im8ZweHkcdqK"
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(class_preds, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXeFDB9O_0ja",
        "outputId": "8e64a0f9-3a5d-4b0c-8056-372ac97d293a"
      },
      "id": "oXeFDB9O_0ja",
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.93      0.93      0.93     28191\n",
            "    positive       0.93      0.93      0.93     28518\n",
            "\n",
            "    accuracy                           0.93     56709\n",
            "   macro avg       0.93      0.93      0.93     56709\n",
            "weighted avg       0.93      0.93      0.93     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим данная нейросеть показала самые лучшие результаты по сравнению с остальными моделями."
      ],
      "metadata": {
        "id": "0bEMaNNZcyj8"
      },
      "id": "0bEMaNNZcyj8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "name": "HW2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}