{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Домашнее задание 10","metadata":{}},{"cell_type":"markdown","source":"Разобраться с моделькой перевода как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков) два варианта с вниманием и без внимания. Оценить качество насколько корректно переводит (для теста отобрать примеры с увеличением длины текста) (так как оценка визуальная достаточно 20-ти примеров в тестовой выборке).","metadata":{}},{"cell_type":"markdown","source":"## Решение","metadata":{}},{"cell_type":"markdown","source":"Импортируем необходимые библиотеки.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\n\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport io\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-17T13:33:10.722703Z","iopub.execute_input":"2022-09-17T13:33:10.723124Z","iopub.status.idle":"2022-09-17T13:33:15.425523Z","shell.execute_reply.started":"2022-09-17T13:33:10.723041Z","shell.execute_reply":"2022-09-17T13:33:15.424517Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Для работы будем использовать польско-английский датасет. Загрузим и распакуем данные.","metadata":{}},{"cell_type":"code","source":"!wget http://www.manythings.org/anki/pol-eng.zip\n\n!rm -rf pol-eng\n!mkdir pol-eng\n!unzip pol-eng.zip -d pol-eng/","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:15.427401Z","iopub.execute_input":"2022-09-17T13:33:15.428135Z","iopub.status.idle":"2022-09-17T13:33:19.833870Z","shell.execute_reply.started":"2022-09-17T13:33:15.428105Z","shell.execute_reply":"2022-09-17T13:33:19.832241Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-09-17 13:33:16--  http://www.manythings.org/anki/pol-eng.zip\nResolving www.manythings.org (www.manythings.org)... 173.254.30.110\nConnecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1843409 (1.8M) [application/zip]\nSaving to: ‘pol-eng.zip’\n\npol-eng.zip         100%[===================>]   1.76M  7.04MB/s    in 0.2s    \n\n2022-09-17 13:33:16 (7.04 MB/s) - ‘pol-eng.zip’ saved [1843409/1843409]\n\nArchive:  pol-eng.zip\n  inflating: pol-eng/pol.txt         \n  inflating: pol-eng/_about.txt      \n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/working/pol-eng/ -lah","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:19.836202Z","iopub.execute_input":"2022-09-17T13:33:19.837114Z","iopub.status.idle":"2022-09-17T13:33:20.953452Z","shell.execute_reply.started":"2022-09-17T13:33:19.837067Z","shell.execute_reply":"2022-09-17T13:33:20.952215Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"total 6.4M\ndrwxr-xr-x 2 root root 4.0K Sep 17 13:33 .\ndrwxr-xr-x 4 root root 4.0K Sep 17 13:33 ..\n-rw-r--r-- 1 root root 1.5K Sep  6 03:10 _about.txt\n-rw-r--r-- 1 root root 6.4M Sep  6 03:10 pol.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Создадим процедуру для предобработки текста на основе представленной на лекции.","metadata":{}},{"cell_type":"code","source":"path_to_file = \"/kaggle/working/pol-eng/pol.txt\"\n\ndef preprocess_sentence(w):\n  w = w.lower().strip()\n  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n  w = re.sub(r'[\" \"]+', \" \", w)\n\n  # Оставляем только интересующие нас символы\n  w = re.sub(r\"[^AaĄąBbCcĆćDdEeĘęFfGgHhIiJjKkLlŁłMmNnŃńOoÓóPpQqRrSsŚśTtUuVvWwXxYyZzŹźŻż?.!,']+\", \" \", w)\n\n  w = w.strip()\n\n  # Добавляем начальный и конечный токены\n  w = '<start> ' + w + ' <end>'\n  return w\n\n\n# Пример предобработки\npreprocess_sentence(\"Kocham Cię!\")","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:20.957608Z","iopub.execute_input":"2022-09-17T13:33:20.957936Z","iopub.status.idle":"2022-09-17T13:33:20.971817Z","shell.execute_reply.started":"2022-09-17T13:33:20.957905Z","shell.execute_reply":"2022-09-17T13:33:20.970725Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'<start> kocham cię ! <end>'"},"metadata":{}}]},{"cell_type":"code","source":"# Создаем датасет распределяя текст по парам\n\ndef create_dataset(path, num_examples):\n  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n\n  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n\n  return zip(*word_pairs)\n\nen, pol = create_dataset(path_to_file, None)\nprint(en[0])\nprint(pol[0])","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:20.973760Z","iopub.execute_input":"2022-09-17T13:33:20.974475Z","iopub.status.idle":"2022-09-17T13:33:22.300628Z","shell.execute_reply.started":"2022-09-17T13:33:20.974439Z","shell.execute_reply":"2022-09-17T13:33:22.298673Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<start> go . <end>\n<start> idź . <end>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Дополнительные вспомогательные процедуры для токенизации текста и создания тензоров из датасета.","metadata":{}},{"cell_type":"code","source":"def tokenize(lang):\n  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n      filters='')\n  lang_tokenizer.fit_on_texts(lang)\n\n  tensor = lang_tokenizer.texts_to_sequences(lang)\n\n  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n                                                         padding='post')\n\n  return tensor, lang_tokenizer\n\n\ndef load_dataset(path, num_examples=None):\n  targ_lang, inp_lang = create_dataset(path, num_examples)\n\n  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n\n  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:22.301961Z","iopub.execute_input":"2022-09-17T13:33:22.302366Z","iopub.status.idle":"2022-09-17T13:33:22.310749Z","shell.execute_reply.started":"2022-09-17T13:33:22.302327Z","shell.execute_reply":"2022-09-17T13:33:22.309713Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Размер датасета\nlen(en), len(pol)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:22.312224Z","iopub.execute_input":"2022-09-17T13:33:22.312780Z","iopub.status.idle":"2022-09-17T13:33:22.326292Z","shell.execute_reply.started":"2022-09-17T13:33:22.312744Z","shell.execute_reply":"2022-09-17T13:33:22.324894Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(46424, 46424)"},"metadata":{}}]},{"cell_type":"markdown","source":"Как видим выборка относительно небольшая (по сравнению с русско-английском набором), поэтому не будем никак дополнительно уменьшать ее для целей обучения.","metadata":{}},{"cell_type":"code","source":"input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\n\nmax_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n\n# Оставляем 20% данных на валидацию\ninput_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n\n# Размеры выборок\nprint(f'Трейн выборка - {len(input_tensor_train)} примеров.')\nprint(f'Валидационная выборка - {len(input_tensor_val)} примеров.')","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:33:22.328093Z","iopub.execute_input":"2022-09-17T13:33:22.328476Z","iopub.status.idle":"2022-09-17T13:33:26.383924Z","shell.execute_reply.started":"2022-09-17T13:33:22.328432Z","shell.execute_reply":"2022-09-17T13:33:26.382757Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Трейн выборка - 37139 примеров.\nВалидационная выборка - 9285 примеров.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Продолжим подготовку данных для обучения/валидации.","metadata":{}},{"cell_type":"code","source":"# гиперпараметры\nBUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 300\nunits = 2048\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\n\n# Разделяем датасет на батчи\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:44.181812Z","iopub.execute_input":"2022-09-17T13:34:44.182540Z","iopub.status.idle":"2022-09-17T13:34:44.196774Z","shell.execute_reply.started":"2022-09-17T13:34:44.182501Z","shell.execute_reply":"2022-09-17T13:34:44.195811Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Перейдем к созданию частей модели без внимания.","metadata":{}},{"cell_type":"code","source":"# Создаем энкодер\n\nclass Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=False,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n#     self.gru1 = tf.keras.layers.GRU(self.enc_units,\n#                                    return_sequences=False,\n#                                    return_state=True,\n#                                    recurrent_initializer='glorot_uniform')\n    \n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n#     x = self.gru(x, initial_state=hidden)\n    output, state = self.gru(x, initial_state=hidden)\n    return state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\n\n# Создаем декодер\n\nclass Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n  def call(self, x, hidden):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x, initial_state=hidden)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:45.689328Z","iopub.execute_input":"2022-09-17T13:34:45.691187Z","iopub.status.idle":"2022-09-17T13:34:45.703303Z","shell.execute_reply.started":"2022-09-17T13:34:45.691123Z","shell.execute_reply":"2022-09-17T13:34:45.702186Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Оптимайзер\n\noptimizer = tf.keras.optimizers.Adam()\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n# Лосс\n\ndef loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:46.661329Z","iopub.execute_input":"2022-09-17T13:34:46.661740Z","iopub.status.idle":"2022-09-17T13:34:46.670109Z","shell.execute_reply.started":"2022-09-17T13:34:46.661708Z","shell.execute_reply":"2022-09-17T13:34:46.668110Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\ncheckpoint_dir = './training_nmt_checkpoints'\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:47.324610Z","iopub.execute_input":"2022-09-17T13:34:47.324945Z","iopub.status.idle":"2022-09-17T13:34:47.343402Z","shell.execute_reply.started":"2022-09-17T13:34:47.324916Z","shell.execute_reply":"2022-09-17T13:34:47.342405Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_hidden = encoder(inp, enc_hidden)\n\n    dec_hidden = enc_hidden\n\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n    # Teacher forcing - feeding the target as the next input\n    for t in range(1, targ.shape[1]):\n      # passing enc_output to the decoder\n      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      # using teacher forcing\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n\n  return batch_loss","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:48.121169Z","iopub.execute_input":"2022-09-17T13:34:48.122022Z","iopub.status.idle":"2022-09-17T13:34:48.132180Z","shell.execute_reply.started":"2022-09-17T13:34:48.121976Z","shell.execute_reply":"2022-09-17T13:34:48.131292Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n  # saving (checkpoint) the model every 2 epochs\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T13:34:48.900866Z","iopub.execute_input":"2022-09-17T13:34:48.901223Z","iopub.status.idle":"2022-09-17T14:04:09.118603Z","shell.execute_reply.started":"2022-09-17T13:34:48.901193Z","shell.execute_reply":"2022-09-17T14:04:09.117619Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"2022-09-17 13:35:26.232682: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n2022-09-17 13:35:33.592750: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Batch 0 Loss 1.3804\nEpoch 1 Batch 100 Loss 0.7177\nEpoch 1 Batch 200 Loss 0.6051\nEpoch 1 Batch 300 Loss 0.5186\nEpoch 1 Batch 400 Loss 0.5284\nEpoch 1 Batch 500 Loss 0.5569\nEpoch 1 Loss 0.6201\nTime taken for 1 epoch 222.03042006492615 sec\n\nEpoch 2 Batch 0 Loss 0.4312\nEpoch 2 Batch 100 Loss 0.3703\nEpoch 2 Batch 200 Loss 0.4320\nEpoch 2 Batch 300 Loss 0.3594\nEpoch 2 Batch 400 Loss 0.4070\nEpoch 2 Batch 500 Loss 0.3931\nEpoch 2 Loss 0.4125\nTime taken for 1 epoch 173.11782789230347 sec\n\nEpoch 3 Batch 0 Loss 0.2995\nEpoch 3 Batch 100 Loss 0.3129\nEpoch 3 Batch 200 Loss 0.3664\nEpoch 3 Batch 300 Loss 0.3320\nEpoch 3 Batch 400 Loss 0.3242\nEpoch 3 Batch 500 Loss 0.2945\nEpoch 3 Loss 0.2871\nTime taken for 1 epoch 170.09934258460999 sec\n\nEpoch 4 Batch 0 Loss 0.1786\nEpoch 4 Batch 100 Loss 0.1571\nEpoch 4 Batch 200 Loss 0.1301\nEpoch 4 Batch 300 Loss 0.1791\nEpoch 4 Batch 400 Loss 0.1923\nEpoch 4 Batch 500 Loss 0.2065\nEpoch 4 Loss 0.1838\nTime taken for 1 epoch 171.36855840682983 sec\n\nEpoch 5 Batch 0 Loss 0.1130\nEpoch 5 Batch 100 Loss 0.1043\nEpoch 5 Batch 200 Loss 0.1164\nEpoch 5 Batch 300 Loss 0.1050\nEpoch 5 Batch 400 Loss 0.1140\nEpoch 5 Batch 500 Loss 0.1209\nEpoch 5 Loss 0.1099\nTime taken for 1 epoch 170.02972221374512 sec\n\nEpoch 6 Batch 0 Loss 0.0677\nEpoch 6 Batch 100 Loss 0.0588\nEpoch 6 Batch 200 Loss 0.0625\nEpoch 6 Batch 300 Loss 0.0756\nEpoch 6 Batch 400 Loss 0.0635\nEpoch 6 Batch 500 Loss 0.0858\nEpoch 6 Loss 0.0668\nTime taken for 1 epoch 171.52845239639282 sec\n\nEpoch 7 Batch 0 Loss 0.0475\nEpoch 7 Batch 100 Loss 0.0411\nEpoch 7 Batch 200 Loss 0.0294\nEpoch 7 Batch 300 Loss 0.0446\nEpoch 7 Batch 400 Loss 0.0404\nEpoch 7 Batch 500 Loss 0.0480\nEpoch 7 Loss 0.0439\nTime taken for 1 epoch 169.76101279258728 sec\n\nEpoch 8 Batch 0 Loss 0.0266\nEpoch 8 Batch 100 Loss 0.0269\nEpoch 8 Batch 200 Loss 0.0321\nEpoch 8 Batch 300 Loss 0.0324\nEpoch 8 Batch 400 Loss 0.0319\nEpoch 8 Batch 500 Loss 0.0392\nEpoch 8 Loss 0.0324\nTime taken for 1 epoch 171.16321802139282 sec\n\nEpoch 9 Batch 0 Loss 0.0273\nEpoch 9 Batch 100 Loss 0.0250\nEpoch 9 Batch 200 Loss 0.0227\nEpoch 9 Batch 300 Loss 0.0295\nEpoch 9 Batch 400 Loss 0.0255\nEpoch 9 Batch 500 Loss 0.0245\nEpoch 9 Loss 0.0266\nTime taken for 1 epoch 169.94830536842346 sec\n\nEpoch 10 Batch 0 Loss 0.0170\nEpoch 10 Batch 100 Loss 0.0185\nEpoch 10 Batch 200 Loss 0.0221\nEpoch 10 Batch 300 Loss 0.0252\nEpoch 10 Batch 400 Loss 0.0189\nEpoch 10 Batch 500 Loss 0.0271\nEpoch 10 Loss 0.0243\nTime taken for 1 epoch 171.1619691848755 sec\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Оценим качество построенной модели.","metadata":{}},{"cell_type":"code","source":"def evaluate_without_att(sentence):\n\n  sentence = preprocess_sentence(sentence)\n\n  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_inp,\n                                                         padding='post')\n  inputs = tf.convert_to_tensor(inputs)\n\n  result = ''\n\n  hidden = [tf.zeros((1, units))]\n  enc_hidden = encoder(inputs, hidden)\n\n  dec_hidden = enc_hidden\n  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n  for t in range(max_length_targ):\n    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n\n    # storing the attention weights to plot later on\n    predicted_id = tf.argmax(predictions[0]).numpy()\n    result += targ_lang.index_word[predicted_id] + ' '\n\n    if targ_lang.index_word[predicted_id] == '<end>':\n      return result, sentence\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n\n  return result, sentence","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:04:27.210250Z","iopub.execute_input":"2022-09-17T14:04:27.210931Z","iopub.status.idle":"2022-09-17T14:04:27.219399Z","shell.execute_reply.started":"2022-09-17T14:04:27.210897Z","shell.execute_reply":"2022-09-17T14:04:27.218341Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def translate(sentence, eval_func):\n  result, sentence = eval_func(sentence)\n\n  print('Input: %s' % (sentence))\n  print('Predicted translation: {}'.format(result))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:04:29.608754Z","iopub.execute_input":"2022-09-17T14:04:29.609112Z","iopub.status.idle":"2022-09-17T14:04:29.614096Z","shell.execute_reply.started":"2022-09-17T14:04:29.609081Z","shell.execute_reply":"2022-09-17T14:04:29.613069Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:04:31.704618Z","iopub.execute_input":"2022-09-17T14:04:31.704977Z","iopub.status.idle":"2022-09-17T14:04:32.205109Z","shell.execute_reply.started":"2022-09-17T14:04:31.704947Z","shell.execute_reply":"2022-09-17T14:04:32.204097Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe7522dbe90>"},"metadata":{}}]},{"cell_type":"code","source":"def convert(lang, tensor):\n    words = []\n    for t in tensor:\n        if t!=0:\n            words.append(lang.index_word[t])\n    return \" \".join(word for word in words if word not in ['<start>', '<end>'])","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:08:52.417257Z","iopub.execute_input":"2022-09-17T14:08:52.417628Z","iopub.status.idle":"2022-09-17T14:08:52.423457Z","shell.execute_reply.started":"2022-09-17T14:08:52.417596Z","shell.execute_reply":"2022-09-17T14:08:52.422467Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"for i in range(30):\n    translate(convert(inp_lang, input_tensor_val[i]), evaluate_without_att)\n    print(f'Target translation: {convert(targ_lang, target_tensor_val[i])}')\n    print('--------------------')","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:10:16.677268Z","iopub.execute_input":"2022-09-17T14:10:16.677638Z","iopub.status.idle":"2022-09-17T14:10:17.936169Z","shell.execute_reply.started":"2022-09-17T14:10:16.677606Z","shell.execute_reply":"2022-09-17T14:10:17.935208Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Input: <start> moja mama potrafi bardzo dobrze grać w golfa . <end>\nPredicted translation: my mom takes a little more time . <end> \nTarget translation: my mother can play golf very well .\n--------------------\nInput: <start> dowiedziałem się , jak rozwiązać problem . <end>\nPredicted translation: i found the key , but it was dangerous . <end> \nTarget translation: i found out how to solve the problem .\n--------------------\nInput: <start> to właśnie tego słownika szukałem . <end>\nPredicted translation: that's the reason i was doing it . <end> \nTarget translation: this is the very dictionary i've been looking for .\n--------------------\nInput: <start> potrzebuję samochodu . <end>\nPredicted translation: i need a car . <end> \nTarget translation: i need a car .\n--------------------\nInput: <start> cieszę się , że cię znalazłem . <end>\nPredicted translation: i'm glad to see you again . <end> \nTarget translation: i'm glad i've found you .\n--------------------\nInput: <start> wychodzę rano . <end>\nPredicted translation: i am reading a book . <end> \nTarget translation: i leave in the morning .\n--------------------\nInput: <start> on jest kapitanem drużyny piłkarskiej . <end>\nPredicted translation: he's a university student in his mouth . <end> \nTarget translation: he is captain of the football team .\n--------------------\nInput: <start> kiedy wydarzyło się to po raz pierwszy ? <end>\nPredicted translation: when did it first start to happen ? <end> \nTarget translation: when did it first start to happen ?\n--------------------\nInput: <start> tom jest jedynym dorosłym , o którym mary wie , że nie potrafi prowadzić . <end>\nPredicted translation: tom is the only one that mary doesn't really like tom . <end> \nTarget translation: tom is the only adult mary knows who can't drive .\n--------------------\nInput: <start> chcę czasu , nie pieniędzy . <end>\nPredicted translation: i just want you to know dinner . <end> \nTarget translation: i want time instead of money .\n--------------------\nInput: <start> włączyliśmy radio . <end>\nPredicted translation: the explosion of the rule are rotten . <end> \nTarget translation: we turned on the radio .\n--------------------\nInput: <start> umiesz jeździć na łyżwach ? <end>\nPredicted translation: can you ride a horse ? <end> \nTarget translation: can you skate ?\n--------------------\nInput: <start> zrobimy wszystko , żeby odnaleźć toma . <end>\nPredicted translation: we're going to tell you about tom . <end> \nTarget translation: we'll do everything we can to find tom .\n--------------------\nInput: <start> podaj mi sól . <end>\nPredicted translation: give me your sponge . <end> \nTarget translation: pass me the salt .\n--------------------\nInput: <start> już wychodzę . <end>\nPredicted translation: i'm reading . <end> \nTarget translation: i'll be right out .\n--------------------\nInput: <start> znaleźliśmy klucze toma . <end>\nPredicted translation: we found mary's umbrella . <end> \nTarget translation: we found tom's keys .\n--------------------\nInput: <start> mogę to zrobić . <end>\nPredicted translation: i can do it . <end> \nTarget translation: i can do this .\n--------------------\nInput: <start> niektórzy ludzie przybierają na wadze kiedy rzucaja palenie . <end>\nPredicted translation: some people believe in eternal life after death . <end> \nTarget translation: some people gain weight when they stop smoking .\n--------------------\nInput: <start> prowadził niedbale i miał wypadek . <end>\nPredicted translation: the man left the restaurant without paying . <end> \nTarget translation: he drove carelessly and had an accident .\n--------------------\nInput: <start> o czym tom mówił mary ? <end>\nPredicted translation: what did tom make mary about ? <end> \nTarget translation: what did tom talk to mary about ?\n--------------------\nInput: <start> źle się to skończyło . <end>\nPredicted translation: it was hard to deal . <end> \nTarget translation: it ended poorly .\n--------------------\nInput: <start> mamy trzy godziny . <end>\nPredicted translation: we've got three hours . <end> \nTarget translation: we have three hours .\n--------------------\nInput: <start> książki mnie fascynują . <end>\nPredicted translation: our visitors are going out . <end> \nTarget translation: books fascinate me .\n--------------------\nInput: <start> przyszło mi do głowy , że on ukradł słownik . <end>\nPredicted translation: he took me one had to pay the letter . <end> \nTarget translation: it occurred to me that he must have stolen the dictionary .\n--------------------\nInput: <start> zwykle wolę płacić kartą , a nie gotówką . <end>\nPredicted translation: i usually prefer to ask people the truth . <end> \nTarget translation: i usually prefer to pay with credit card and not with cash .\n--------------------\nInput: <start> lubię brać co wieczór gorącą kąpiel . <end>\nPredicted translation: i like to eat some vegetables by you . <end> \nTarget translation: i like to take a hot bath every night before bed .\n--------------------\nInput: <start> padał śnieg . <end>\nPredicted translation: it was snowing . <end> \nTarget translation: it snowed .\n--------------------\nInput: <start> byłem na zakupach w sobotę . <end>\nPredicted translation: i was on the mountain side . <end> \nTarget translation: i went shopping last saturday .\n--------------------\nInput: <start> zapach jedzenia sprawił , że zgłodniałem . <end>\nPredicted translation: it looks like the thief saw it . <end> \nTarget translation: the smell of food made me hungry .\n--------------------\nInput: <start> zaraz wracam . <end>\nPredicted translation: i'll be back right away . <end> \nTarget translation: i'll be right back .\n--------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Теперь построим модель с вниманием.","metadata":{}},{"cell_type":"code","source":"# Новый энкодер\nclass Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   return_sequences=True, # возвращаем результаты\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output, state = self.gru(x, initial_state = hidden)\n    return output, state\n\n  def initialize_hidden_state(self):\n    return tf.zeros((self.batch_sz, self.enc_units))\n\n\n# Класс внимания\nclass BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, query, values):\n    # query hidden state shape == (batch_size, hidden size)\n    # query_with_time_axis shape == (batch_size, 1, hidden size)\n    # values shape == (batch_size, max_len, hidden size)\n    # we are doing this to broadcast addition along the time axis to calculate the score\n    query_with_time_axis = tf.expand_dims(query, 1)\n\n    # score shape == (batch_size, max_length, 1)\n    # we get 1 at the last axis because we are applying score to self.V\n    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n    score = self.V(tf.nn.tanh(\n        self.W1(query_with_time_axis) + self.W2(values)))\n\n    # attention_weights shape == (batch_size, max_length, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * values\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n\n\n# Новый декодер\n\nclass Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # used for attention\n    self.attention = BahdanauAttention(self.dec_units)\n\n  def call(self, x, hidden, enc_output):\n    # enc_output shape == (batch_size, max_length, hidden_size)\n    context_vector, attention_weights = self.attention(hidden, enc_output)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # output shape == (batch_size * 1, hidden_size)\n    output = tf.reshape(output, (-1, output.shape[2]))\n\n    # output shape == (batch_size, vocab)\n    x = self.fc(output)\n\n    return x, state, attention_weights","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:12:47.110422Z","iopub.execute_input":"2022-09-17T14:12:47.110787Z","iopub.status.idle":"2022-09-17T14:12:47.126099Z","shell.execute_reply.started":"2022-09-17T14:12:47.110757Z","shell.execute_reply":"2022-09-17T14:12:47.125136Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\nattention_layer = BahdanauAttention(10)\ndecoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\n\ncheckpoint_dir_att = './training_attention_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir_att, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:12:50.044996Z","iopub.execute_input":"2022-09-17T14:12:50.046052Z","iopub.status.idle":"2022-09-17T14:12:50.070386Z","shell.execute_reply.started":"2022-09-17T14:12:50.045999Z","shell.execute_reply":"2022-09-17T14:12:50.069475Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n  loss = 0\n\n  with tf.GradientTape() as tape:\n    enc_output, enc_hidden = encoder(inp, enc_hidden)\n\n    dec_hidden = enc_hidden\n\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n\n    # Teacher forcing - feeding the target as the next input\n    for t in range(1, targ.shape[1]):\n      # passing enc_output to the decoder\n      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n\n      loss += loss_function(targ[:, t], predictions)\n\n      # using teacher forcing\n      dec_input = tf.expand_dims(targ[:, t], 1)\n\n  batch_loss = (loss / int(targ.shape[1]))\n\n  variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, variables)\n\n  optimizer.apply_gradients(zip(gradients, variables))\n\n  return batch_loss","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:12:53.216864Z","iopub.execute_input":"2022-09-17T14:12:53.217258Z","iopub.status.idle":"2022-09-17T14:12:54.226507Z","shell.execute_reply.started":"2022-09-17T14:12:53.217225Z","shell.execute_reply":"2022-09-17T14:12:54.225520Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Обучаем модель с вниманием.","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  enc_hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n    batch_loss = train_step(inp, targ, enc_hidden)\n    total_loss += batch_loss\n\n    if batch % 100 == 0:\n      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                   batch,\n                                                   batch_loss.numpy()))\n  # saving (checkpoint) the model every 2 epochs\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                      total_loss / steps_per_epoch))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T14:12:58.338648Z","iopub.execute_input":"2022-09-17T14:12:58.339469Z","iopub.status.idle":"2022-09-17T15:55:23.881353Z","shell.execute_reply.started":"2022-09-17T14:12:58.339427Z","shell.execute_reply":"2022-09-17T15:55:23.880325Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1 Batch 0 Loss 1.4638\nEpoch 1 Batch 100 Loss 0.7546\nEpoch 1 Batch 200 Loss 0.6855\nEpoch 1 Batch 300 Loss 0.6622\nEpoch 1 Batch 400 Loss 0.6244\nEpoch 1 Batch 500 Loss 0.5929\nEpoch 1 Loss 0.6676\nTime taken for 1 epoch 664.2650091648102 sec\n\nEpoch 2 Batch 0 Loss 0.4891\nEpoch 2 Batch 100 Loss 0.5022\nEpoch 2 Batch 200 Loss 0.4938\nEpoch 2 Batch 300 Loss 0.4925\nEpoch 2 Batch 400 Loss 0.4770\nEpoch 2 Batch 500 Loss 0.4071\nEpoch 2 Loss 0.4470\nTime taken for 1 epoch 610.6391882896423 sec\n\nEpoch 3 Batch 0 Loss 0.3210\nEpoch 3 Batch 100 Loss 0.2990\nEpoch 3 Batch 200 Loss 0.2952\nEpoch 3 Batch 300 Loss 0.2576\nEpoch 3 Batch 400 Loss 0.3162\nEpoch 3 Batch 500 Loss 0.2096\nEpoch 3 Loss 0.2888\nTime taken for 1 epoch 608.4920086860657 sec\n\nEpoch 4 Batch 0 Loss 0.1822\nEpoch 4 Batch 100 Loss 0.2040\nEpoch 4 Batch 200 Loss 0.2164\nEpoch 4 Batch 300 Loss 0.2012\nEpoch 4 Batch 400 Loss 0.1573\nEpoch 4 Batch 500 Loss 0.1761\nEpoch 4 Loss 0.1878\nTime taken for 1 epoch 610.0751712322235 sec\n\nEpoch 5 Batch 0 Loss 0.1024\nEpoch 5 Batch 100 Loss 0.1293\nEpoch 5 Batch 200 Loss 0.1321\nEpoch 5 Batch 300 Loss 0.0919\nEpoch 5 Batch 400 Loss 0.1131\nEpoch 5 Batch 500 Loss 0.1403\nEpoch 5 Loss 0.1259\nTime taken for 1 epoch 608.1985838413239 sec\n\nEpoch 6 Batch 0 Loss 0.0734\nEpoch 6 Batch 100 Loss 0.0912\nEpoch 6 Batch 200 Loss 0.0933\nEpoch 6 Batch 300 Loss 0.0754\nEpoch 6 Batch 400 Loss 0.1007\nEpoch 6 Batch 500 Loss 0.0907\nEpoch 6 Loss 0.0884\nTime taken for 1 epoch 609.4788906574249 sec\n\nEpoch 7 Batch 0 Loss 0.0512\nEpoch 7 Batch 100 Loss 0.0583\nEpoch 7 Batch 200 Loss 0.0564\nEpoch 7 Batch 300 Loss 0.0850\nEpoch 7 Batch 400 Loss 0.0721\nEpoch 7 Batch 500 Loss 0.0742\nEpoch 7 Loss 0.0621\nTime taken for 1 epoch 607.6589732170105 sec\n\nEpoch 8 Batch 0 Loss 0.0512\nEpoch 8 Batch 100 Loss 0.0443\nEpoch 8 Batch 200 Loss 0.0444\nEpoch 8 Batch 300 Loss 0.0437\nEpoch 8 Batch 400 Loss 0.0436\nEpoch 8 Batch 500 Loss 0.0681\nEpoch 8 Loss 0.0466\nTime taken for 1 epoch 609.7032468318939 sec\n\nEpoch 9 Batch 0 Loss 0.0341\nEpoch 9 Batch 100 Loss 0.0335\nEpoch 9 Batch 200 Loss 0.0414\nEpoch 9 Batch 300 Loss 0.0338\nEpoch 9 Batch 400 Loss 0.0498\nEpoch 9 Batch 500 Loss 0.0392\nEpoch 9 Loss 0.0354\nTime taken for 1 epoch 607.591100692749 sec\n\nEpoch 10 Batch 0 Loss 0.0209\nEpoch 10 Batch 100 Loss 0.0270\nEpoch 10 Batch 200 Loss 0.0392\nEpoch 10 Batch 300 Loss 0.0380\nEpoch 10 Batch 400 Loss 0.0205\nEpoch 10 Batch 500 Loss 0.0416\nEpoch 10 Loss 0.0293\nTime taken for 1 epoch 609.3986148834229 sec\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Оценка ошибки для модели с вниманием получилось очень близкой к ошибке без внимания. Сделаем предсказания переводов по аналогии с предыдущей моделью.","metadata":{}},{"cell_type":"code","source":"def evaluate(sentence):\n  attention_plot = np.zeros((max_length_targ, max_length_inp))\n\n  sentence = preprocess_sentence(sentence)\n\n  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n                                                         maxlen=max_length_inp,\n                                                         padding='post')\n  inputs = tf.convert_to_tensor(inputs)\n\n  result = ''\n\n  hidden = [tf.zeros((1, units))]\n  enc_out, enc_hidden = encoder(inputs, hidden)\n\n  dec_hidden = enc_hidden\n  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n\n  for t in range(max_length_targ):\n    predictions, dec_hidden, attention_weights = decoder(dec_input,\n                                                         dec_hidden,\n                                                         enc_out)\n\n    # storing the attention weights to plot later on\n    attention_weights = tf.reshape(attention_weights, (-1, ))\n    attention_plot[t] = attention_weights.numpy()\n\n    predicted_id = tf.argmax(predictions[0]).numpy()\n\n    result += targ_lang.index_word[predicted_id] + ' '\n\n    if targ_lang.index_word[predicted_id] == '<end>':\n      return result, sentence\n\n    # the predicted ID is fed back into the model\n    dec_input = tf.expand_dims([predicted_id], 0)\n\n  return result, sentence","metadata":{"execution":{"iopub.status.busy":"2022-09-17T15:57:16.493101Z","iopub.execute_input":"2022-09-17T15:57:16.493581Z","iopub.status.idle":"2022-09-17T15:57:16.507730Z","shell.execute_reply.started":"2022-09-17T15:57:16.493542Z","shell.execute_reply":"2022-09-17T15:57:16.506584Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir_att))","metadata":{"execution":{"iopub.status.busy":"2022-09-17T15:57:19.339418Z","iopub.execute_input":"2022-09-17T15:57:19.339782Z","iopub.status.idle":"2022-09-17T15:57:19.979886Z","shell.execute_reply.started":"2022-09-17T15:57:19.339753Z","shell.execute_reply":"2022-09-17T15:57:19.978908Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe6c020a190>"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(30):\n    translate(convert(inp_lang, input_tensor_val[i]), evaluate)\n    print(f'Target translation: {convert(targ_lang, target_tensor_val[i])}')\n    print('--------------------')","metadata":{"execution":{"iopub.status.busy":"2022-09-17T15:57:21.259766Z","iopub.execute_input":"2022-09-17T15:57:21.260659Z","iopub.status.idle":"2022-09-17T15:57:23.285694Z","shell.execute_reply.started":"2022-09-17T15:57:21.260621Z","shell.execute_reply":"2022-09-17T15:57:23.284635Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Input: <start> moja mama potrafi bardzo dobrze grać w golfa . <end>\nPredicted translation: my mom can play very well . <end> \nTarget translation: my mother can play golf very well .\n--------------------\nInput: <start> dowiedziałem się , jak rozwiązać problem . <end>\nPredicted translation: i found out how to solve the problem . <end> \nTarget translation: i found out how to solve the problem .\n--------------------\nInput: <start> to właśnie tego słownika szukałem . <end>\nPredicted translation: this is the job for him . <end> \nTarget translation: this is the very dictionary i've been looking for .\n--------------------\nInput: <start> potrzebuję samochodu . <end>\nPredicted translation: i need a car . <end> \nTarget translation: i need a car .\n--------------------\nInput: <start> cieszę się , że cię znalazłem . <end>\nPredicted translation: i'm glad i found you . <end> \nTarget translation: i'm glad i've found you .\n--------------------\nInput: <start> wychodzę rano . <end>\nPredicted translation: i'm leaving in the morning . <end> \nTarget translation: i leave in the morning .\n--------------------\nInput: <start> on jest kapitanem drużyny piłkarskiej . <end>\nPredicted translation: he is the team team . <end> \nTarget translation: he is captain of the football team .\n--------------------\nInput: <start> kiedy wydarzyło się to po raz pierwszy ? <end>\nPredicted translation: when did it occur over this ? <end> \nTarget translation: when did it first start to happen ?\n--------------------\nInput: <start> tom jest jedynym dorosłym , o którym mary wie , że nie potrafi prowadzić . <end>\nPredicted translation: tom is the only reason why mary knows how he doesn't know . <end> \nTarget translation: tom is the only adult mary knows who can't drive .\n--------------------\nInput: <start> chcę czasu , nie pieniędzy . <end>\nPredicted translation: i want time , not money . <end> \nTarget translation: i want time instead of money .\n--------------------\nInput: <start> włączyliśmy radio . <end>\nPredicted translation: they are giving the radio . <end> \nTarget translation: we turned on the radio .\n--------------------\nInput: <start> umiesz jeździć na łyżwach ? <end>\nPredicted translation: can you ride against compete ? <end> \nTarget translation: can you skate ?\n--------------------\nInput: <start> zrobimy wszystko , żeby odnaleźć toma . <end>\nPredicted translation: we've done all to look of tom . <end> \nTarget translation: we'll do everything we can to find tom .\n--------------------\nInput: <start> podaj mi sól . <end>\nPredicted translation: pass me the salt . <end> \nTarget translation: pass me the salt .\n--------------------\nInput: <start> już wychodzę . <end>\nPredicted translation: i'm already . <end> \nTarget translation: i'll be right out .\n--------------------\nInput: <start> znaleźliśmy klucze toma . <end>\nPredicted translation: we found tom's keys . <end> \nTarget translation: we found tom's keys .\n--------------------\nInput: <start> mogę to zrobić . <end>\nPredicted translation: i can do it . <end> \nTarget translation: i can do this .\n--------------------\nInput: <start> niektórzy ludzie przybierają na wadze kiedy rzucaja palenie . <end>\nPredicted translation: politicians people wear weight that raise . <end> \nTarget translation: some people gain weight when they stop smoking .\n--------------------\nInput: <start> prowadził niedbale i miał wypadek . <end>\nPredicted translation: he drove away and had a accident . <end> \nTarget translation: he drove carelessly and had an accident .\n--------------------\nInput: <start> o czym tom mówił mary ? <end>\nPredicted translation: what did tom doing at ? <end> \nTarget translation: what did tom talk to mary about ?\n--------------------\nInput: <start> źle się to skończyło . <end>\nPredicted translation: it is bad end . <end> \nTarget translation: it ended poorly .\n--------------------\nInput: <start> mamy trzy godziny . <end>\nPredicted translation: we've got three hours . <end> \nTarget translation: we have three hours .\n--------------------\nInput: <start> książki mnie fascynują . <end>\nPredicted translation: my physics book am itchy . <end> \nTarget translation: books fascinate me .\n--------------------\nInput: <start> przyszło mi do głowy , że on ukradł słownik . <end>\nPredicted translation: it would have told me that he stole the dictionary . <end> \nTarget translation: it occurred to me that he must have stolen the dictionary .\n--------------------\nInput: <start> zwykle wolę płacić kartą , a nie gotówką . <end>\nPredicted translation: i usually enjoy hearing from the children , but i can't . <end> \nTarget translation: i usually prefer to pay with credit card and not with cash .\n--------------------\nInput: <start> lubię brać co wieczór gorącą kąpiel . <end>\nPredicted translation: i like to take a bath in the game . <end> \nTarget translation: i like to take a hot bath every night before bed .\n--------------------\nInput: <start> padał śnieg . <end>\nPredicted translation: it was snowing . <end> \nTarget translation: it snowed .\n--------------------\nInput: <start> byłem na zakupach w sobotę . <end>\nPredicted translation: i went shopping . <end> \nTarget translation: i went shopping last saturday .\n--------------------\nInput: <start> zapach jedzenia sprawił , że zgłodniałem . <end>\nPredicted translation: i sincerely food made it . <end> \nTarget translation: the smell of food made me hungry .\n--------------------\nInput: <start> zaraz wracam . <end>\nPredicted translation: i will be back soon . <end> \nTarget translation: i'll be right back .\n--------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Несмотря на то, что ошибка для обеих моделей практически идентичная, визуальный анализ позволяет утверждать, что предсказания модели с вниманием более точные, по крайней мере для первых 30 предложений валидационной выборки.","metadata":{}},{"cell_type":"markdown","source":"|    | Input text                                                                 | Prediction without attention                            |                  Prediction with attention                  |                          Target text                         |\n|----|----------------------------------------------------------------------------|---------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------|\n| 1  | moja mama potrafi bardzo dobrze grać w golfa .                             | my mom takes a little more time .                       | _my mom can play very well ._                               | my mother can play golf very well .                          |\n| 2  | dowiedziałem się , jak rozwiązać problem .                                 | i found the key , but it was dangerous .                | **i found out how to solve the problem .**                  | i found out how to solve the problem .                       |\n| 3  | to właśnie tego słownika szukałem .                                        | that's the reason i was doing it .                      | this is the job for him .                                   | this is the very dictionary i've been looking for .          |\n| 4  | potrzebuję samochodu .                                                     | **i need a car .**                                      | **i need a car .**                                          | i need a car .                                               |\n| 5  | cieszę się , że cię znalazłem .                                            | _i'm glad to see you again ._                           | _i'm glad i found you ._                                    | i'm glad i've found you .                                    |\n| 6  | wychodzę rano .                                                            | i am reading a book .                                   | _i'm leaving in the morning ._                              | i leave in the morning .                                     |\n| 7  | on jest kapitanem drużyny piłkarskiej .                                    | he's a university student in his mouth .                | he is the team team .                                       | he is captain of the football team .                         |\n| 8  | kiedy wydarzyło się to po raz pierwszy ?                                   | **when did it first start to happen ?**                 | when did it occur over this ?                               | when did it first start to happen ?                          |\n| 9  | tom jest jedynym dorosłym , o którym mary wie , że nie potrafi prowadzić . | tom is the only one that mary doesn't really like tom . | tom is the only reason why mary knows how he doesn't know . | tom is the only adult mary knows who can't drive .           |\n| 10 | chcę czasu , nie pieniędzy .                                               | i just want you to know dinner .                        | _i want time , not money ._                                 | i want time instead of money .                               |\n| 11 | włączyliśmy radio .                                                        | the explosion of the rule are rotten .                  | they are giving the radio .                                 | we turned on the radio .                                     |\n| 12 | umiesz jeździć na łyżwach ?                                                | can you ride a horse ?                                  | can you ride against compete ?                              | can you skate ?                                              |\n| 13 | zrobimy wszystko , żeby odnaleźć toma .                                    | we're going to tell you about tom .                     | we've done all to look of tom .                             | we'll do everything we can to find tom .                     |\n| 14 | podaj mi sól .                                                             | give me your sponge .                                   | **pass me the salt .**                                      | pass me the salt .                                           |\n| 15 | już wychodzę .                                                             | i'm reading .                                           | i'm already .                                               | i'll be right out .                                          |\n| 16 | znaleźliśmy klucze toma .                                                  | we found mary's umbrella .                              | **we found tom's keys .**                                   | we found tom's keys .                                        |\n| 17 | mogę to zrobić .                                                           | _i can do it ._                                         | _i can do it ._                                             | i can do this .                                              |\n| 18 | niektórzy ludzie przybierają na wadze kiedy rzucaja palenie .              | some people believe in eternal life after death .       | politicians people wear weight that raise .                 | some people gain weight when they stop smoking .             |\n| 19 | prowadził niedbale i miał wypadek .                                        | the man left the restaurant without paying .            | he drove away and had a accident .                          | he drove carelessly and had an accident .                    |\n| 20 | o czym tom mówił mary ?                                                    | what did tom make mary about ?                          | what did tom doing at ?                                     | what did tom talk to mary about ?                            |\n| 21 | źle się to skończyło .                                                     | it was hard to deal .                                   | it is bad end .                                             | it ended poorly .                                            |\n| 22 | mamy trzy godziny .                                                        | _we've got three hours ._                               | _we've got three hours ._                                   | we have three hours .                                        |\n| 23 | książki mnie fascynują .                                                   | our visitors are going out .                            | my physics book am itchy .                                  | books fascinate me .                                         |\n| 24 | przyszło mi do głowy , że on ukradł słownik .                              | he took me one had to pay the letter .                  | it would have told me that he stole the dictionary .        | it occurred to me that he must have stolen the dictionary .  |\n| 25 | zwykle wolę płacić kartą , a nie gotówką .                                 | i usually prefer to ask people the truth .              | i usually enjoy hearing from the children , but i can't .   | i usually prefer to pay with credit card and not with cash . |\n| 26 | lubię brać co wieczór gorącą kąpiel .                                      | i like to eat some vegetables by you .                  | i like to take a bath in the game .                         | i like to take a hot bath every night before bed .           |\n| 27 | padał śnieg .                                                              | _it was snowing ._                                      | _it was snowing ._                                          | it snowed .                                                  |\n| 28 | byłem na zakupach w sobotę .                                               | i was on the mountain side .                            | _i went shopping ._                                         | i went shopping last saturday .                              |\n| 29 | zapach jedzenia sprawił , że zgłodniałem .                                 | it looks like the thief saw it .                        | i sincerely food made it .                                  | the smell of food made me hungry .                           |\n| 30 | zaraz wracam .                                                             | _i'll be back right away ._                             | _i will be back soon ._                                     | i'll be right back .                                         |","metadata":{}}]}