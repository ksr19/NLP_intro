{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Домашнее задание 5\n",
        "\n"
      ],
      "metadata": {
        "id": "qNwog2DqtG9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2.** Проверить, насколько хорошо работает NER\n",
        "\n",
        "Данные брать из http://www.labinform.ru/pub/named_entities/\n",
        "\n",
        "1. проверить NER из nltk/spacy/deeppavlov.\n",
        "2. написать свой NER, попробовать разные подходы.\n",
        "  \n",
        "  a. передаём в сетку токен и его соседей.\n",
        "  \n",
        "  b. передаём в сетку только токен.\n",
        "\n",
        "  c. свой вариант.\n",
        "\n",
        "\n",
        "\n",
        "3. сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score.\n"
      ],
      "metadata": {
        "id": "mwA2WPadtLBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решение:"
      ],
      "metadata": {
        "id": "fuEPqPnqt3q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим датасет."
      ],
      "metadata": {
        "id": "gIq-zT_duHUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTCkGAsytGiz",
        "outputId": "c83017cb-79f9-426c-8107-0493adb267f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-30 19:05:26--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
            "Resolving www.labinform.ru (www.labinform.ru)... 95.181.230.181\n",
            "Connecting to www.labinform.ru (www.labinform.ru)|95.181.230.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899530 (1.8M) [application/zip]\n",
            "Saving to: ‘collection5.zip’\n",
            "\n",
            "collection5.zip     100%[===================>]   1.81M  1.91MB/s    in 0.9s    \n",
            "\n",
            "2022-08-30 19:05:27 (1.91 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
            "\n",
            "Archive:  collection5.zip\n",
            "   creating: Collection5/\n",
            "  inflating: Collection5/001.ann     \n",
            "  inflating: Collection5/001.txt     \n",
            "  inflating: Collection5/002.ann     \n",
            "  inflating: Collection5/002.txt     \n",
            "  inflating: Collection5/003.ann     \n",
            "  inflating: Collection5/003.txt     \n",
            "  inflating: Collection5/004.ann     \n",
            "  inflating: Collection5/004.txt     \n",
            "  inflating: Collection5/005.ann     \n",
            "  inflating: Collection5/005.txt     \n",
            "  inflating: Collection5/006.ann     \n",
            "  inflating: Collection5/006.txt     \n",
            "  inflating: Collection5/007.ann     \n",
            "  inflating: Collection5/007.txt     \n",
            "  inflating: Collection5/008.ann     \n",
            "  inflating: Collection5/008.txt     \n",
            "  inflating: Collection5/009.ann     \n",
            "  inflating: Collection5/009.txt     \n",
            "  inflating: Collection5/010.ann     \n",
            "  inflating: Collection5/010.txt     \n",
            "  inflating: Collection5/011.ann     \n",
            "  inflating: Collection5/011.txt     \n",
            "  inflating: Collection5/012.ann     \n",
            "  inflating: Collection5/012.txt     \n",
            "  inflating: Collection5/013.ann     \n",
            "  inflating: Collection5/013.txt     \n",
            "  inflating: Collection5/014.ann     \n",
            "  inflating: Collection5/014.txt     \n",
            "  inflating: Collection5/015 (!).ann  \n",
            "  inflating: Collection5/015 (!).txt  \n",
            "  inflating: Collection5/016.ann     \n",
            "  inflating: Collection5/016.txt     \n",
            "  inflating: Collection5/017.ann     \n",
            "  inflating: Collection5/017.txt     \n",
            "  inflating: Collection5/018.ann     \n",
            "  inflating: Collection5/018.txt     \n",
            "  inflating: Collection5/019.ann     \n",
            "  inflating: Collection5/019.txt     \n",
            "  inflating: Collection5/020.ann     \n",
            "  inflating: Collection5/020.txt     \n",
            "  inflating: Collection5/021.ann     \n",
            "  inflating: Collection5/021.txt     \n",
            "  inflating: Collection5/022.ann     \n",
            "  inflating: Collection5/022.txt     \n",
            "  inflating: Collection5/023.ann     \n",
            "  inflating: Collection5/023.txt     \n",
            "  inflating: Collection5/025.ann     \n",
            "  inflating: Collection5/025.txt     \n",
            "  inflating: Collection5/026.ann     \n",
            "  inflating: Collection5/026.txt     \n",
            "  inflating: Collection5/027.ann     \n",
            "  inflating: Collection5/027.txt     \n",
            "  inflating: Collection5/028.ann     \n",
            "  inflating: Collection5/028.txt     \n",
            "  inflating: Collection5/029.ann     \n",
            "  inflating: Collection5/029.txt     \n",
            "  inflating: Collection5/030.ann     \n",
            "  inflating: Collection5/030.txt     \n",
            "  inflating: Collection5/031.ann     \n",
            "  inflating: Collection5/031.txt     \n",
            "  inflating: Collection5/032.ann     \n",
            "  inflating: Collection5/032.txt     \n",
            "  inflating: Collection5/033.ann     \n",
            "  inflating: Collection5/033.txt     \n",
            "  inflating: Collection5/034.ann     \n",
            "  inflating: Collection5/034.txt     \n",
            "  inflating: Collection5/035.ann     \n",
            "  inflating: Collection5/035.txt     \n",
            "  inflating: Collection5/036.ann     \n",
            "  inflating: Collection5/036.txt     \n",
            "  inflating: Collection5/037.ann     \n",
            "  inflating: Collection5/037.txt     \n",
            "  inflating: Collection5/038.ann     \n",
            "  inflating: Collection5/038.txt     \n",
            "  inflating: Collection5/039.ann     \n",
            "  inflating: Collection5/039.txt     \n",
            "  inflating: Collection5/03_12_12a.ann  \n",
            "  inflating: Collection5/03_12_12a.txt  \n",
            "  inflating: Collection5/03_12_12b.ann  \n",
            "  inflating: Collection5/03_12_12b.txt  \n",
            "  inflating: Collection5/03_12_12c.ann  \n",
            "  inflating: Collection5/03_12_12c.txt  \n",
            "  inflating: Collection5/03_12_12d.ann  \n",
            "  inflating: Collection5/03_12_12d.txt  \n",
            "  inflating: Collection5/03_12_12g.ann  \n",
            "  inflating: Collection5/03_12_12g.txt  \n",
            "  inflating: Collection5/03_12_12h.ann  \n",
            "  inflating: Collection5/03_12_12h.txt  \n",
            "  inflating: Collection5/040.ann     \n",
            "  inflating: Collection5/040.txt     \n",
            "  inflating: Collection5/041.ann     \n",
            "  inflating: Collection5/041.txt     \n",
            "  inflating: Collection5/042.ann     \n",
            "  inflating: Collection5/042.txt     \n",
            "  inflating: Collection5/043.ann     \n",
            "  inflating: Collection5/043.txt     \n",
            "  inflating: Collection5/044.ann     \n",
            "  inflating: Collection5/044.txt     \n",
            "  inflating: Collection5/045.ann     \n",
            "  inflating: Collection5/045.txt     \n",
            "  inflating: Collection5/046.ann     \n",
            "  inflating: Collection5/046.txt     \n",
            "  inflating: Collection5/047.ann     \n",
            "  inflating: Collection5/047.txt     \n",
            "  inflating: Collection5/048.ann     \n",
            "  inflating: Collection5/048.txt     \n",
            "  inflating: Collection5/049.ann     \n",
            "  inflating: Collection5/049.txt     \n",
            "  inflating: Collection5/04_02_13a_abdulatipov.ann  \n",
            "  inflating: Collection5/04_02_13a_abdulatipov.txt  \n",
            "  inflating: Collection5/04_03_13a_sorokin.ann  \n",
            "  inflating: Collection5/04_03_13a_sorokin.txt  \n",
            "  inflating: Collection5/04_12_12b.ann  \n",
            "  inflating: Collection5/04_12_12b.txt  \n",
            "  inflating: Collection5/04_12_12d.ann  \n",
            "  inflating: Collection5/04_12_12d.txt  \n",
            "  inflating: Collection5/04_12_12f.ann  \n",
            "  inflating: Collection5/04_12_12f.txt  \n",
            "  inflating: Collection5/04_12_12g.ann  \n",
            "  inflating: Collection5/04_12_12g.txt  \n",
            "  inflating: Collection5/04_12_12h_corr.ann  \n",
            "  inflating: Collection5/04_12_12h_corr.txt  \n",
            "  inflating: Collection5/050.ann     \n",
            "  inflating: Collection5/050.txt     \n",
            "  inflating: Collection5/051.ann     \n",
            "  inflating: Collection5/051.txt     \n",
            "  inflating: Collection5/052.ann     \n",
            "  inflating: Collection5/052.txt     \n",
            "  inflating: Collection5/053.ann     \n",
            "  inflating: Collection5/053.txt     \n",
            "  inflating: Collection5/054.ann     \n",
            "  inflating: Collection5/054.txt     \n",
            "  inflating: Collection5/055.ann     \n",
            "  inflating: Collection5/055.txt     \n",
            "  inflating: Collection5/056.ann     \n",
            "  inflating: Collection5/056.txt     \n",
            "  inflating: Collection5/057.ann     \n",
            "  inflating: Collection5/057.txt     \n",
            "  inflating: Collection5/058.ann     \n",
            "  inflating: Collection5/058.txt     \n",
            "  inflating: Collection5/059.ann     \n",
            "  inflating: Collection5/059.txt     \n",
            "  inflating: Collection5/060.ann     \n",
            "  inflating: Collection5/060.txt     \n",
            "  inflating: Collection5/061.ann     \n",
            "  inflating: Collection5/061.txt     \n",
            "  inflating: Collection5/062.ann     \n",
            "  inflating: Collection5/062.txt     \n",
            "  inflating: Collection5/063.ann     \n",
            "  inflating: Collection5/063.txt     \n",
            "  inflating: Collection5/064.ann     \n",
            "  inflating: Collection5/064.txt     \n",
            "  inflating: Collection5/065.ann     \n",
            "  inflating: Collection5/065.txt     \n",
            "  inflating: Collection5/066.ann     \n",
            "  inflating: Collection5/066.txt     \n",
            "  inflating: Collection5/067.ann     \n",
            "  inflating: Collection5/067.txt     \n",
            "  inflating: Collection5/068.ann     \n",
            "  inflating: Collection5/068.txt     \n",
            "  inflating: Collection5/069.ann     \n",
            "  inflating: Collection5/069.txt     \n",
            "  inflating: Collection5/070.ann     \n",
            "  inflating: Collection5/070.txt     \n",
            "  inflating: Collection5/071.ann     \n",
            "  inflating: Collection5/071.txt     \n",
            "  inflating: Collection5/072.ann     \n",
            "  inflating: Collection5/072.txt     \n",
            "  inflating: Collection5/073.ann     \n",
            "  inflating: Collection5/073.txt     \n",
            "  inflating: Collection5/074.ann     \n",
            "  inflating: Collection5/074.txt     \n",
            "  inflating: Collection5/075.ann     \n",
            "  inflating: Collection5/075.txt     \n",
            "  inflating: Collection5/076.ann     \n",
            "  inflating: Collection5/076.txt     \n",
            "  inflating: Collection5/077.ann     \n",
            "  inflating: Collection5/077.txt     \n",
            "  inflating: Collection5/078.ann     \n",
            "  inflating: Collection5/078.txt     \n",
            "  inflating: Collection5/079.ann     \n",
            "  inflating: Collection5/079.txt     \n",
            "  inflating: Collection5/080.ann     \n",
            "  inflating: Collection5/080.txt     \n",
            "  inflating: Collection5/081.ann     \n",
            "  inflating: Collection5/081.txt     \n",
            "  inflating: Collection5/082.ann     \n",
            "  inflating: Collection5/082.txt     \n",
            "  inflating: Collection5/083.ann     \n",
            "  inflating: Collection5/083.txt     \n",
            "  inflating: Collection5/084.ann     \n",
            "  inflating: Collection5/084.txt     \n",
            "  inflating: Collection5/085.ann     \n",
            "  inflating: Collection5/085.txt     \n",
            "  inflating: Collection5/086.ann     \n",
            "  inflating: Collection5/086.txt     \n",
            "  inflating: Collection5/087.ann     \n",
            "  inflating: Collection5/087.txt     \n",
            "  inflating: Collection5/088.ann     \n",
            "  inflating: Collection5/088.txt     \n",
            "  inflating: Collection5/089.ann     \n",
            "  inflating: Collection5/089.txt     \n",
            "  inflating: Collection5/090.ann     \n",
            "  inflating: Collection5/090.txt     \n",
            "  inflating: Collection5/091.ann     \n",
            "  inflating: Collection5/091.txt     \n",
            "  inflating: Collection5/092.ann     \n",
            "  inflating: Collection5/092.txt     \n",
            "  inflating: Collection5/093.ann     \n",
            "  inflating: Collection5/093.txt     \n",
            "  inflating: Collection5/094.ann     \n",
            "  inflating: Collection5/094.txt     \n",
            "  inflating: Collection5/095.ann     \n",
            "  inflating: Collection5/095.txt     \n",
            "  inflating: Collection5/096.ann     \n",
            "  inflating: Collection5/096.txt     \n",
            "  inflating: Collection5/097.ann     \n",
            "  inflating: Collection5/097.txt     \n",
            "  inflating: Collection5/098.ann     \n",
            "  inflating: Collection5/098.txt     \n",
            "  inflating: Collection5/099.ann     \n",
            "  inflating: Collection5/099.txt     \n",
            "  inflating: Collection5/09_01_13.ann  \n",
            "  inflating: Collection5/09_01_13.txt  \n",
            "  inflating: Collection5/09_01_13a.ann  \n",
            "  inflating: Collection5/09_01_13a.txt  \n",
            "  inflating: Collection5/09_01_13c.ann  \n",
            "  inflating: Collection5/09_01_13c.txt  \n",
            "  inflating: Collection5/09_01_13d.ann  \n",
            "  inflating: Collection5/09_01_13d.txt  \n",
            "  inflating: Collection5/09_01_13e.ann  \n",
            "  inflating: Collection5/09_01_13e.txt  \n",
            "  inflating: Collection5/09_01_13h.ann  \n",
            "  inflating: Collection5/09_01_13h.txt  \n",
            "  inflating: Collection5/09_01_13i.ann  \n",
            "  inflating: Collection5/09_01_13i.txt  \n",
            "  inflating: Collection5/100.ann     \n",
            "  inflating: Collection5/100.txt     \n",
            "  inflating: Collection5/1000.ann    \n",
            "  inflating: Collection5/1000.txt    \n",
            "  inflating: Collection5/1001.ann    \n",
            "  inflating: Collection5/1001.txt    \n",
            "  inflating: Collection5/1002.ann    \n",
            "  inflating: Collection5/1002.txt    \n",
            "  inflating: Collection5/1003.ann    \n",
            "  inflating: Collection5/1003.txt    \n",
            "  inflating: Collection5/1004.ann    \n",
            "  inflating: Collection5/1004.txt    \n",
            "  inflating: Collection5/1005.ann    \n",
            "  inflating: Collection5/1005.txt    \n",
            "  inflating: Collection5/1006.ann    \n",
            "  inflating: Collection5/1006.txt    \n",
            "  inflating: Collection5/1007.ann    \n",
            "  inflating: Collection5/1007.txt    \n",
            "  inflating: Collection5/1008.ann    \n",
            "  inflating: Collection5/1008.txt    \n",
            "  inflating: Collection5/1009.ann    \n",
            "  inflating: Collection5/1009.txt    \n",
            "  inflating: Collection5/101.ann     \n",
            "  inflating: Collection5/101.txt     \n",
            "  inflating: Collection5/1010.ann    \n",
            "  inflating: Collection5/1010.txt    \n",
            "  inflating: Collection5/1011.ann    \n",
            "  inflating: Collection5/1011.txt    \n",
            "  inflating: Collection5/1012.ann    \n",
            "  inflating: Collection5/1012.txt    \n",
            "  inflating: Collection5/1013.ann    \n",
            "  inflating: Collection5/1013.txt    \n",
            "  inflating: Collection5/1014.ann    \n",
            "  inflating: Collection5/1014.txt    \n",
            "  inflating: Collection5/1015.ann    \n",
            "  inflating: Collection5/1015.txt    \n",
            "  inflating: Collection5/1016.ann    \n",
            "  inflating: Collection5/1016.txt    \n",
            "  inflating: Collection5/1017.ann    \n",
            "  inflating: Collection5/1017.txt    \n",
            "  inflating: Collection5/1018.ann    \n",
            "  inflating: Collection5/1018.txt    \n",
            "  inflating: Collection5/1019.ann    \n",
            "  inflating: Collection5/1019.txt    \n",
            "  inflating: Collection5/102.ann     \n",
            "  inflating: Collection5/102.txt     \n",
            "  inflating: Collection5/1020.ann    \n",
            "  inflating: Collection5/1020.txt    \n",
            "  inflating: Collection5/1021.ann    \n",
            "  inflating: Collection5/1021.txt    \n",
            "  inflating: Collection5/1022.ann    \n",
            "  inflating: Collection5/1022.txt    \n",
            "  inflating: Collection5/1023.ann    \n",
            "  inflating: Collection5/1023.txt    \n",
            "  inflating: Collection5/1024.ann    \n",
            "  inflating: Collection5/1024.txt    \n",
            "  inflating: Collection5/1025.ann    \n",
            "  inflating: Collection5/1025.txt    \n",
            "  inflating: Collection5/1026.ann    \n",
            "  inflating: Collection5/1026.txt    \n",
            "  inflating: Collection5/1027.ann    \n",
            "  inflating: Collection5/1027.txt    \n",
            "  inflating: Collection5/1028.ann    \n",
            "  inflating: Collection5/1028.txt    \n",
            "  inflating: Collection5/1029.ann    \n",
            "  inflating: Collection5/1029.txt    \n",
            "  inflating: Collection5/103.ann     \n",
            "  inflating: Collection5/103.txt     \n",
            "  inflating: Collection5/1030.ann    \n",
            "  inflating: Collection5/1030.txt    \n",
            "  inflating: Collection5/1031.ann    \n",
            "  inflating: Collection5/1031.txt    \n",
            "  inflating: Collection5/1032.ann    \n",
            "  inflating: Collection5/1032.txt    \n",
            "  inflating: Collection5/1033.ann    \n",
            "  inflating: Collection5/1033.txt    \n",
            "  inflating: Collection5/1034.ann    \n",
            "  inflating: Collection5/1034.txt    \n",
            "  inflating: Collection5/1035.ann    \n",
            "  inflating: Collection5/1035.txt    \n",
            "  inflating: Collection5/1036.ann    \n",
            "  inflating: Collection5/1036.txt    \n",
            "  inflating: Collection5/1037.ann    \n",
            "  inflating: Collection5/1037.txt    \n",
            "  inflating: Collection5/1038.ann    \n",
            "  inflating: Collection5/1038.txt    \n",
            "  inflating: Collection5/1039.ann    \n",
            "  inflating: Collection5/1039.txt    \n",
            "  inflating: Collection5/104.ann     \n",
            "  inflating: Collection5/104.txt     \n",
            "  inflating: Collection5/1040.ann    \n",
            "  inflating: Collection5/1040.txt    \n",
            "  inflating: Collection5/1041.ann    \n",
            "  inflating: Collection5/1041.txt    \n",
            "  inflating: Collection5/1042.ann    \n",
            "  inflating: Collection5/1042.txt    \n",
            "  inflating: Collection5/1043.ann    \n",
            "  inflating: Collection5/1043.txt    \n",
            "  inflating: Collection5/1044.ann    \n",
            "  inflating: Collection5/1044.txt    \n",
            "  inflating: Collection5/1045.ann    \n",
            "  inflating: Collection5/1045.txt    \n",
            "  inflating: Collection5/1046.ann    \n",
            "  inflating: Collection5/1046.txt    \n",
            "  inflating: Collection5/1047.ann    \n",
            "  inflating: Collection5/1047.txt    \n",
            "  inflating: Collection5/1048.ann    \n",
            "  inflating: Collection5/1048.txt    \n",
            "  inflating: Collection5/1049.ann    \n",
            "  inflating: Collection5/1049.txt    \n",
            "  inflating: Collection5/105.ann     \n",
            "  inflating: Collection5/105.txt     \n",
            "  inflating: Collection5/1050.ann    \n",
            "  inflating: Collection5/1050.txt    \n",
            "  inflating: Collection5/106.ann     \n",
            "  inflating: Collection5/106.txt     \n",
            "  inflating: Collection5/107.ann     \n",
            "  inflating: Collection5/107.txt     \n",
            "  inflating: Collection5/108.ann     \n",
            "  inflating: Collection5/108.txt     \n",
            "  inflating: Collection5/109.ann     \n",
            "  inflating: Collection5/109.txt     \n",
            "  inflating: Collection5/10_01_13a.ann  \n",
            "  inflating: Collection5/10_01_13a.txt  \n",
            "  inflating: Collection5/10_01_13d.ann  \n",
            "  inflating: Collection5/10_01_13d.txt  \n",
            "  inflating: Collection5/10_01_13i.ann  \n",
            "  inflating: Collection5/10_01_13i.txt  \n",
            "  inflating: Collection5/110.ann     \n",
            "  inflating: Collection5/110.txt     \n",
            "  inflating: Collection5/1100.ann    \n",
            "  inflating: Collection5/1100.txt    \n",
            "  inflating: Collection5/1101.ann    \n",
            "  inflating: Collection5/1101.txt    \n",
            "  inflating: Collection5/1102.ann    \n",
            "  inflating: Collection5/1102.txt    \n",
            "  inflating: Collection5/1103.ann    \n",
            "  inflating: Collection5/1103.txt    \n",
            "  inflating: Collection5/1104.ann    \n",
            "  inflating: Collection5/1104.txt    \n",
            "  inflating: Collection5/1105.ann    \n",
            "  inflating: Collection5/1105.txt    \n",
            "  inflating: Collection5/1106.ann    \n",
            "  inflating: Collection5/1106.txt    \n",
            "  inflating: Collection5/1107.ann    \n",
            "  inflating: Collection5/1107.txt    \n",
            "  inflating: Collection5/1108.ann    \n",
            "  inflating: Collection5/1108.txt    \n",
            "  inflating: Collection5/1109.ann    \n",
            "  inflating: Collection5/1109.txt    \n",
            "  inflating: Collection5/111.ann     \n",
            "  inflating: Collection5/111.txt     \n",
            "  inflating: Collection5/1110.ann    \n",
            "  inflating: Collection5/1110.txt    \n",
            "  inflating: Collection5/1111.ann    \n",
            "  inflating: Collection5/1111.txt    \n",
            "  inflating: Collection5/1112.ann    \n",
            "  inflating: Collection5/1112.txt    \n",
            "  inflating: Collection5/1113.ann    \n",
            "  inflating: Collection5/1113.txt    \n",
            "  inflating: Collection5/1114.ann    \n",
            "  inflating: Collection5/1114.txt    \n",
            "  inflating: Collection5/1115.ann    \n",
            "  inflating: Collection5/1115.txt    \n",
            "  inflating: Collection5/1116.ann    \n",
            "  inflating: Collection5/1116.txt    \n",
            "  inflating: Collection5/1117.ann    \n",
            "  inflating: Collection5/1117.txt    \n",
            "  inflating: Collection5/1118.ann    \n",
            "  inflating: Collection5/1118.txt    \n",
            "  inflating: Collection5/1119.ann    \n",
            "  inflating: Collection5/1119.txt    \n",
            "  inflating: Collection5/112.ann     \n",
            "  inflating: Collection5/112.txt     \n",
            "  inflating: Collection5/1120.ann    \n",
            "  inflating: Collection5/1120.txt    \n",
            "  inflating: Collection5/1121.ann    \n",
            "  inflating: Collection5/1121.txt    \n",
            "  inflating: Collection5/1122.ann    \n",
            "  inflating: Collection5/1122.txt    \n",
            "  inflating: Collection5/1123.ann    \n",
            "  inflating: Collection5/1123.txt    \n",
            "  inflating: Collection5/1124.ann    \n",
            "  inflating: Collection5/1124.txt    \n",
            "  inflating: Collection5/1125.ann    \n",
            "  inflating: Collection5/1125.txt    \n",
            "  inflating: Collection5/1126.ann    \n",
            "  inflating: Collection5/1126.txt    \n",
            "  inflating: Collection5/1127.ann    \n",
            "  inflating: Collection5/1127.txt    \n",
            "  inflating: Collection5/1128.ann    \n",
            "  inflating: Collection5/1128.txt    \n",
            "  inflating: Collection5/113.ann     \n",
            "  inflating: Collection5/113.txt     \n",
            "  inflating: Collection5/1130.ann    \n",
            "  inflating: Collection5/1130.txt    \n",
            "  inflating: Collection5/1131.ann    \n",
            "  inflating: Collection5/1131.txt    \n",
            "  inflating: Collection5/1132.ann    \n",
            "  inflating: Collection5/1132.txt    \n",
            "  inflating: Collection5/1133.ann    \n",
            "  inflating: Collection5/1133.txt    \n",
            "  inflating: Collection5/1134.ann    \n",
            "  inflating: Collection5/1134.txt    \n",
            "  inflating: Collection5/1135.ann    \n",
            "  inflating: Collection5/1135.txt    \n",
            "  inflating: Collection5/1136.ann    \n",
            "  inflating: Collection5/1136.txt    \n",
            "  inflating: Collection5/1137.ann    \n",
            "  inflating: Collection5/1137.txt    \n",
            "  inflating: Collection5/1138.ann    \n",
            "  inflating: Collection5/1138.txt    \n",
            "  inflating: Collection5/1139.ann    \n",
            "  inflating: Collection5/1139.txt    \n",
            "  inflating: Collection5/114.ann     \n",
            "  inflating: Collection5/114.txt     \n",
            "  inflating: Collection5/1140.ann    \n",
            "  inflating: Collection5/1140.txt    \n",
            "  inflating: Collection5/1141.ann    \n",
            "  inflating: Collection5/1141.txt    \n",
            "  inflating: Collection5/1142.ann    \n",
            "  inflating: Collection5/1142.txt    \n",
            "  inflating: Collection5/1143.ann    \n",
            "  inflating: Collection5/1143.txt    \n",
            "  inflating: Collection5/1144.ann    \n",
            "  inflating: Collection5/1144.txt    \n",
            "  inflating: Collection5/1145.ann    \n",
            "  inflating: Collection5/1145.txt    \n",
            "  inflating: Collection5/1146.ann    \n",
            "  inflating: Collection5/1146.txt    \n",
            "  inflating: Collection5/1147.ann    \n",
            "  inflating: Collection5/1147.txt    \n",
            "  inflating: Collection5/1148.ann    \n",
            "  inflating: Collection5/1148.txt    \n",
            "  inflating: Collection5/1149.ann    \n",
            "  inflating: Collection5/1149.txt    \n",
            "  inflating: Collection5/115.ann     \n",
            "  inflating: Collection5/115.txt     \n",
            "  inflating: Collection5/1150.ann    \n",
            "  inflating: Collection5/1150.txt    \n",
            "  inflating: Collection5/1151.ann    \n",
            "  inflating: Collection5/1151.txt    \n",
            "  inflating: Collection5/1152.ann    \n",
            "  inflating: Collection5/1152.txt    \n",
            "  inflating: Collection5/1153.ann    \n",
            "  inflating: Collection5/1153.txt    \n",
            "  inflating: Collection5/1154.ann    \n",
            "  inflating: Collection5/1154.txt    \n",
            "  inflating: Collection5/1155.ann    \n",
            "  inflating: Collection5/1155.txt    \n",
            "  inflating: Collection5/1156.ann    \n",
            "  inflating: Collection5/1156.txt    \n",
            "  inflating: Collection5/1157.ann    \n",
            "  inflating: Collection5/1157.txt    \n",
            "  inflating: Collection5/1158.ann    \n",
            "  inflating: Collection5/1158.txt    \n",
            "  inflating: Collection5/1159.ann    \n",
            "  inflating: Collection5/1159.txt    \n",
            "  inflating: Collection5/116.ann     \n",
            "  inflating: Collection5/116.txt     \n",
            "  inflating: Collection5/1160.ann    \n",
            "  inflating: Collection5/1160.txt    \n",
            "  inflating: Collection5/1161.ann    \n",
            "  inflating: Collection5/1161.txt    \n",
            "  inflating: Collection5/1162.ann    \n",
            "  inflating: Collection5/1162.txt    \n",
            "  inflating: Collection5/1163.ann    \n",
            "  inflating: Collection5/1163.txt    \n",
            "  inflating: Collection5/1164.ann    \n",
            "  inflating: Collection5/1164.txt    \n",
            "  inflating: Collection5/1165.ann    \n",
            "  inflating: Collection5/1165.txt    \n",
            "  inflating: Collection5/1166.ann    \n",
            "  inflating: Collection5/1166.txt    \n",
            "  inflating: Collection5/1167.ann    \n",
            "  inflating: Collection5/1167.txt    \n",
            "  inflating: Collection5/1168.ann    \n",
            "  inflating: Collection5/1168.txt    \n",
            "  inflating: Collection5/1169.ann    \n",
            "  inflating: Collection5/1169.txt    \n",
            "  inflating: Collection5/117.ann     \n",
            "  inflating: Collection5/117.txt     \n",
            "  inflating: Collection5/1170.ann    \n",
            "  inflating: Collection5/1170.txt    \n",
            "  inflating: Collection5/1171.ann    \n",
            "  inflating: Collection5/1171.txt    \n",
            "  inflating: Collection5/1172.ann    \n",
            "  inflating: Collection5/1172.txt    \n",
            "  inflating: Collection5/1173.ann    \n",
            "  inflating: Collection5/1173.txt    \n",
            "  inflating: Collection5/1174.ann    \n",
            "  inflating: Collection5/1174.txt    \n",
            "  inflating: Collection5/1175.ann    \n",
            "  inflating: Collection5/1175.txt    \n",
            "  inflating: Collection5/1176.ann    \n",
            "  inflating: Collection5/1176.txt    \n",
            "  inflating: Collection5/1177.ann    \n",
            "  inflating: Collection5/1177.txt    \n",
            "  inflating: Collection5/1178.ann    \n",
            "  inflating: Collection5/1178.txt    \n",
            "  inflating: Collection5/1179.ann    \n",
            "  inflating: Collection5/1179.txt    \n",
            "  inflating: Collection5/118.ann     \n",
            "  inflating: Collection5/118.txt     \n",
            "  inflating: Collection5/1180.ann    \n",
            "  inflating: Collection5/1180.txt    \n",
            "  inflating: Collection5/1181.ann    \n",
            "  inflating: Collection5/1181.txt    \n",
            "  inflating: Collection5/1182.ann    \n",
            "  inflating: Collection5/1182.txt    \n",
            "  inflating: Collection5/1183.ann    \n",
            "  inflating: Collection5/1183.txt    \n",
            "  inflating: Collection5/1184.ann    \n",
            "  inflating: Collection5/1184.txt    \n",
            "  inflating: Collection5/1185.ann    \n",
            "  inflating: Collection5/1185.txt    \n",
            "  inflating: Collection5/1186.ann    \n",
            "  inflating: Collection5/1186.txt    \n",
            "  inflating: Collection5/1187.ann    \n",
            "  inflating: Collection5/1187.txt    \n",
            "  inflating: Collection5/1188.ann    \n",
            "  inflating: Collection5/1188.txt    \n",
            "  inflating: Collection5/1189.ann    \n",
            "  inflating: Collection5/1189.txt    \n",
            "  inflating: Collection5/119.ann     \n",
            "  inflating: Collection5/119.txt     \n",
            "  inflating: Collection5/1190.ann    \n",
            "  inflating: Collection5/1190.txt    \n",
            "  inflating: Collection5/1191.ann    \n",
            "  inflating: Collection5/1191.txt    \n",
            "  inflating: Collection5/1192.ann    \n",
            "  inflating: Collection5/1192.txt    \n",
            "  inflating: Collection5/1193.ann    \n",
            "  inflating: Collection5/1193.txt    \n",
            "  inflating: Collection5/1194.ann    \n",
            "  inflating: Collection5/1194.txt    \n",
            "  inflating: Collection5/1195.ann    \n",
            "  inflating: Collection5/1195.txt    \n",
            "  inflating: Collection5/1196.ann    \n",
            "  inflating: Collection5/1196.txt    \n",
            "  inflating: Collection5/1197.ann    \n",
            "  inflating: Collection5/1197.txt    \n",
            "  inflating: Collection5/1198.ann    \n",
            "  inflating: Collection5/1198.txt    \n",
            "  inflating: Collection5/1199.ann    \n",
            "  inflating: Collection5/1199.txt    \n",
            "  inflating: Collection5/11_01_13b.ann  \n",
            "  inflating: Collection5/11_01_13b.txt  \n",
            "  inflating: Collection5/11_01_13e.ann  \n",
            "  inflating: Collection5/11_01_13e.txt  \n",
            "  inflating: Collection5/120.ann     \n",
            "  inflating: Collection5/120.txt     \n",
            "  inflating: Collection5/1200.ann    \n",
            "  inflating: Collection5/1200.txt    \n",
            "  inflating: Collection5/121.ann     \n",
            "  inflating: Collection5/121.txt     \n",
            "  inflating: Collection5/122.ann     \n",
            "  inflating: Collection5/122.txt     \n",
            "  inflating: Collection5/123.ann     \n",
            "  inflating: Collection5/123.txt     \n",
            "  inflating: Collection5/124.ann     \n",
            "  inflating: Collection5/124.txt     \n",
            "  inflating: Collection5/125.ann     \n",
            "  inflating: Collection5/125.txt     \n",
            "  inflating: Collection5/126.ann     \n",
            "  inflating: Collection5/126.txt     \n",
            "  inflating: Collection5/127.ann     \n",
            "  inflating: Collection5/127.txt     \n",
            "  inflating: Collection5/128.ann     \n",
            "  inflating: Collection5/128.txt     \n",
            "  inflating: Collection5/129.ann     \n",
            "  inflating: Collection5/129.txt     \n",
            "  inflating: Collection5/130.ann     \n",
            "  inflating: Collection5/130.txt     \n",
            "  inflating: Collection5/131.ann     \n",
            "  inflating: Collection5/131.txt     \n",
            "  inflating: Collection5/132.ann     \n",
            "  inflating: Collection5/132.txt     \n",
            "  inflating: Collection5/133.ann     \n",
            "  inflating: Collection5/133.txt     \n",
            "  inflating: Collection5/134.ann     \n",
            "  inflating: Collection5/134.txt     \n",
            "  inflating: Collection5/135.ann     \n",
            "  inflating: Collection5/135.txt     \n",
            "  inflating: Collection5/136.ann     \n",
            "  inflating: Collection5/136.txt     \n",
            "  inflating: Collection5/137.ann     \n",
            "  inflating: Collection5/137.txt     \n",
            "  inflating: Collection5/138.ann     \n",
            "  inflating: Collection5/138.txt     \n",
            "  inflating: Collection5/139.ann     \n",
            "  inflating: Collection5/139.txt     \n",
            "  inflating: Collection5/140.ann     \n",
            "  inflating: Collection5/140.txt     \n",
            "  inflating: Collection5/141.ann     \n",
            "  inflating: Collection5/141.txt     \n",
            "  inflating: Collection5/142.ann     \n",
            "  inflating: Collection5/142.txt     \n",
            "  inflating: Collection5/143.ann     \n",
            "  inflating: Collection5/143.txt     \n",
            "  inflating: Collection5/144.ann     \n",
            "  inflating: Collection5/144.txt     \n",
            "  inflating: Collection5/145.ann     \n",
            "  inflating: Collection5/145.txt     \n",
            "  inflating: Collection5/146.ann     \n",
            "  inflating: Collection5/146.txt     \n",
            "  inflating: Collection5/147.ann     \n",
            "  inflating: Collection5/147.txt     \n",
            "  inflating: Collection5/148.ann     \n",
            "  inflating: Collection5/148.txt     \n",
            "  inflating: Collection5/149.ann     \n",
            "  inflating: Collection5/149.txt     \n",
            "  inflating: Collection5/14_01_13c.ann  \n",
            "  inflating: Collection5/14_01_13c.txt  \n",
            "  inflating: Collection5/14_01_13g.ann  \n",
            "  inflating: Collection5/14_01_13g.txt  \n",
            "  inflating: Collection5/14_01_13i.ann  \n",
            "  inflating: Collection5/14_01_13i.txt  \n",
            "  inflating: Collection5/150.ann     \n",
            "  inflating: Collection5/150.txt     \n",
            "  inflating: Collection5/151.ann     \n",
            "  inflating: Collection5/151.txt     \n",
            "  inflating: Collection5/152.ann     \n",
            "  inflating: Collection5/152.txt     \n",
            "  inflating: Collection5/153.ann     \n",
            "  inflating: Collection5/153.txt     \n",
            "  inflating: Collection5/154.ann     \n",
            "  inflating: Collection5/154.txt     \n",
            "  inflating: Collection5/155.ann     \n",
            "  inflating: Collection5/155.txt     \n",
            "  inflating: Collection5/156.ann     \n",
            "  inflating: Collection5/156.txt     \n",
            "  inflating: Collection5/157.ann     \n",
            "  inflating: Collection5/157.txt     \n",
            "  inflating: Collection5/158.ann     \n",
            "  inflating: Collection5/158.txt     \n",
            "  inflating: Collection5/159.ann     \n",
            "  inflating: Collection5/159.txt     \n",
            "  inflating: Collection5/15_01_13a.ann  \n",
            "  inflating: Collection5/15_01_13a.txt  \n",
            "  inflating: Collection5/15_01_13b.ann  \n",
            "  inflating: Collection5/15_01_13b.txt  \n",
            "  inflating: Collection5/15_01_13e.ann  \n",
            "  inflating: Collection5/15_01_13e.txt  \n",
            "  inflating: Collection5/15_01_13f.ann  \n",
            "  inflating: Collection5/15_01_13f.txt  \n",
            "  inflating: Collection5/160.ann     \n",
            "  inflating: Collection5/160.txt     \n",
            "  inflating: Collection5/161.ann     \n",
            "  inflating: Collection5/161.txt     \n",
            "  inflating: Collection5/162.ann     \n",
            "  inflating: Collection5/162.txt     \n",
            "  inflating: Collection5/163.ann     \n",
            "  inflating: Collection5/163.txt     \n",
            "  inflating: Collection5/164.ann     \n",
            "  inflating: Collection5/164.txt     \n",
            "  inflating: Collection5/165.ann     \n",
            "  inflating: Collection5/165.txt     \n",
            "  inflating: Collection5/166.ann     \n",
            "  inflating: Collection5/166.txt     \n",
            "  inflating: Collection5/167.ann     \n",
            "  inflating: Collection5/167.txt     \n",
            "  inflating: Collection5/168.ann     \n",
            "  inflating: Collection5/168.txt     \n",
            "  inflating: Collection5/169.ann     \n",
            "  inflating: Collection5/169.txt     \n",
            "  inflating: Collection5/170.ann     \n",
            "  inflating: Collection5/170.txt     \n",
            "  inflating: Collection5/171.ann     \n",
            "  inflating: Collection5/171.txt     \n",
            "  inflating: Collection5/172.ann     \n",
            "  inflating: Collection5/172.txt     \n",
            "  inflating: Collection5/173.ann     \n",
            "  inflating: Collection5/173.txt     \n",
            "  inflating: Collection5/174.ann     \n",
            "  inflating: Collection5/174.txt     \n",
            "  inflating: Collection5/175.ann     \n",
            "  inflating: Collection5/175.txt     \n",
            "  inflating: Collection5/176.ann     \n",
            "  inflating: Collection5/176.txt     \n",
            "  inflating: Collection5/177.ann     \n",
            "  inflating: Collection5/177.txt     \n",
            "  inflating: Collection5/178.ann     \n",
            "  inflating: Collection5/178.txt     \n",
            "  inflating: Collection5/179.ann     \n",
            "  inflating: Collection5/179.txt     \n",
            "  inflating: Collection5/180.ann     \n",
            "  inflating: Collection5/180.txt     \n",
            "  inflating: Collection5/181.ann     \n",
            "  inflating: Collection5/181.txt     \n",
            "  inflating: Collection5/182.ann     \n",
            "  inflating: Collection5/182.txt     \n",
            "  inflating: Collection5/183.ann     \n",
            "  inflating: Collection5/183.txt     \n",
            "  inflating: Collection5/184.ann     \n",
            "  inflating: Collection5/184.txt     \n",
            "  inflating: Collection5/185.ann     \n",
            "  inflating: Collection5/185.txt     \n",
            "  inflating: Collection5/186.ann     \n",
            "  inflating: Collection5/186.txt     \n",
            "  inflating: Collection5/187.ann     \n",
            "  inflating: Collection5/187.txt     \n",
            "  inflating: Collection5/188.ann     \n",
            "  inflating: Collection5/188.txt     \n",
            "  inflating: Collection5/189.ann     \n",
            "  inflating: Collection5/189.txt     \n",
            "  inflating: Collection5/190.ann     \n",
            "  inflating: Collection5/190.txt     \n",
            "  inflating: Collection5/191.ann     \n",
            "  inflating: Collection5/191.txt     \n",
            "  inflating: Collection5/192.ann     \n",
            "  inflating: Collection5/192.txt     \n",
            "  inflating: Collection5/193.ann     \n",
            "  inflating: Collection5/193.txt     \n",
            "  inflating: Collection5/194.ann     \n",
            "  inflating: Collection5/194.txt     \n",
            "  inflating: Collection5/195.ann     \n",
            "  inflating: Collection5/195.txt     \n",
            "  inflating: Collection5/196.ann     \n",
            "  inflating: Collection5/196.txt     \n",
            "  inflating: Collection5/197.ann     \n",
            "  inflating: Collection5/197.txt     \n",
            "  inflating: Collection5/198.ann     \n",
            "  inflating: Collection5/198.txt     \n",
            "  inflating: Collection5/199.ann     \n",
            "  inflating: Collection5/199.txt     \n",
            "  inflating: Collection5/19_11_12d.ann  \n",
            "  inflating: Collection5/19_11_12d.txt  \n",
            "  inflating: Collection5/19_11_12h.ann  \n",
            "  inflating: Collection5/19_11_12h.txt  \n",
            "  inflating: Collection5/200.ann     \n",
            "  inflating: Collection5/200.txt     \n",
            "  inflating: Collection5/2001.ann    \n",
            "  inflating: Collection5/2001.txt    \n",
            "  inflating: Collection5/2002.ann    \n",
            "  inflating: Collection5/2002.txt    \n",
            "  inflating: Collection5/2003.ann    \n",
            "  inflating: Collection5/2003.txt    \n",
            "  inflating: Collection5/2004.ann    \n",
            "  inflating: Collection5/2004.txt    \n",
            "  inflating: Collection5/2005.ann    \n",
            "  inflating: Collection5/2005.txt    \n",
            "  inflating: Collection5/2006.ann    \n",
            "  inflating: Collection5/2006.txt    \n",
            "  inflating: Collection5/2007.ann    \n",
            "  inflating: Collection5/2007.txt    \n",
            "  inflating: Collection5/2008.ann    \n",
            "  inflating: Collection5/2008.txt    \n",
            "  inflating: Collection5/2009.ann    \n",
            "  inflating: Collection5/2009.txt    \n",
            "  inflating: Collection5/201.ann     \n",
            "  inflating: Collection5/201.txt     \n",
            "  inflating: Collection5/2010.ann    \n",
            "  inflating: Collection5/2010.txt    \n",
            "  inflating: Collection5/2011.ann    \n",
            "  inflating: Collection5/2011.txt    \n",
            "  inflating: Collection5/2012.ann    \n",
            "  inflating: Collection5/2012.txt    \n",
            "  inflating: Collection5/2013.ann    \n",
            "  inflating: Collection5/2013.txt    \n",
            "  inflating: Collection5/2014.ann    \n",
            "  inflating: Collection5/2014.txt    \n",
            "  inflating: Collection5/2015.ann    \n",
            "  inflating: Collection5/2015.txt    \n",
            "  inflating: Collection5/2016.ann    \n",
            "  inflating: Collection5/2016.txt    \n",
            "  inflating: Collection5/2017.ann    \n",
            "  inflating: Collection5/2017.txt    \n",
            "  inflating: Collection5/2018.ann    \n",
            "  inflating: Collection5/2018.txt    \n",
            "  inflating: Collection5/2019.ann    \n",
            "  inflating: Collection5/2019.txt    \n",
            "  inflating: Collection5/202.ann     \n",
            "  inflating: Collection5/202.txt     \n",
            "  inflating: Collection5/2020.ann    \n",
            "  inflating: Collection5/2020.txt    \n",
            "  inflating: Collection5/2021.ann    \n",
            "  inflating: Collection5/2021.txt    \n",
            "  inflating: Collection5/2022.ann    \n",
            "  inflating: Collection5/2022.txt    \n",
            "  inflating: Collection5/2023.ann    \n",
            "  inflating: Collection5/2023.txt    \n",
            "  inflating: Collection5/2024.ann    \n",
            "  inflating: Collection5/2024.txt    \n",
            "  inflating: Collection5/2025.ann    \n",
            "  inflating: Collection5/2025.txt    \n",
            "  inflating: Collection5/2026.ann    \n",
            "  inflating: Collection5/2026.txt    \n",
            "  inflating: Collection5/2027.ann    \n",
            "  inflating: Collection5/2027.txt    \n",
            "  inflating: Collection5/2028.ann    \n",
            "  inflating: Collection5/2028.txt    \n",
            "  inflating: Collection5/2029.ann    \n",
            "  inflating: Collection5/2029.txt    \n",
            "  inflating: Collection5/203.ann     \n",
            "  inflating: Collection5/203.txt     \n",
            "  inflating: Collection5/2030.ann    \n",
            "  inflating: Collection5/2030.txt    \n",
            "  inflating: Collection5/2031.ann    \n",
            "  inflating: Collection5/2031.txt    \n",
            "  inflating: Collection5/2032.ann    \n",
            "  inflating: Collection5/2032.txt    \n",
            "  inflating: Collection5/2034.ann    \n",
            "  inflating: Collection5/2034.txt    \n",
            "  inflating: Collection5/2035.ann    \n",
            "  inflating: Collection5/2035.txt    \n",
            "  inflating: Collection5/2036.ann    \n",
            "  inflating: Collection5/2036.txt    \n",
            "  inflating: Collection5/2037.ann    \n",
            "  inflating: Collection5/2037.txt    \n",
            "  inflating: Collection5/2038.ann    \n",
            "  inflating: Collection5/2038.txt    \n",
            "  inflating: Collection5/2039.ann    \n",
            "  inflating: Collection5/2039.txt    \n",
            "  inflating: Collection5/204.ann     \n",
            "  inflating: Collection5/204.txt     \n",
            "  inflating: Collection5/2040.ann    \n",
            "  inflating: Collection5/2040.txt    \n",
            "  inflating: Collection5/2041.ann    \n",
            "  inflating: Collection5/2041.txt    \n",
            "  inflating: Collection5/2042.ann    \n",
            "  inflating: Collection5/2042.txt    \n",
            "  inflating: Collection5/2043.ann    \n",
            "  inflating: Collection5/2043.txt    \n",
            "  inflating: Collection5/2044.ann    \n",
            "  inflating: Collection5/2044.txt    \n",
            "  inflating: Collection5/2045.ann    \n",
            "  inflating: Collection5/2045.txt    \n",
            "  inflating: Collection5/2046.ann    \n",
            "  inflating: Collection5/2046.txt    \n",
            "  inflating: Collection5/2047.ann    \n",
            "  inflating: Collection5/2047.txt    \n",
            "  inflating: Collection5/2048.ann    \n",
            "  inflating: Collection5/2048.txt    \n",
            "  inflating: Collection5/2049.ann    \n",
            "  inflating: Collection5/2049.txt    \n",
            "  inflating: Collection5/205.ann     \n",
            "  inflating: Collection5/205.txt     \n",
            "  inflating: Collection5/2050.ann    \n",
            "  inflating: Collection5/2050.txt    \n",
            "  inflating: Collection5/206.ann     \n",
            "  inflating: Collection5/206.txt     \n",
            "  inflating: Collection5/207.ann     \n",
            "  inflating: Collection5/207.txt     \n",
            "  inflating: Collection5/208.ann     \n",
            "  inflating: Collection5/208.txt     \n",
            "  inflating: Collection5/209.ann     \n",
            "  inflating: Collection5/209.txt     \n",
            "  inflating: Collection5/20_11_12a.ann  \n",
            "  inflating: Collection5/20_11_12a.txt  \n",
            "  inflating: Collection5/20_11_12b.ann  \n",
            "  inflating: Collection5/20_11_12b.txt  \n",
            "  inflating: Collection5/20_11_12c.ann  \n",
            "  inflating: Collection5/20_11_12c.txt  \n",
            "  inflating: Collection5/20_11_12d.ann  \n",
            "  inflating: Collection5/20_11_12d.txt  \n",
            "  inflating: Collection5/20_11_12i.ann  \n",
            "  inflating: Collection5/20_11_12i.txt  \n",
            "  inflating: Collection5/210.ann     \n",
            "  inflating: Collection5/210.txt     \n",
            "  inflating: Collection5/211.ann     \n",
            "  inflating: Collection5/211.txt     \n",
            "  inflating: Collection5/212.ann     \n",
            "  inflating: Collection5/212.txt     \n",
            "  inflating: Collection5/213.ann     \n",
            "  inflating: Collection5/213.txt     \n",
            "  inflating: Collection5/214.ann     \n",
            "  inflating: Collection5/214.txt     \n",
            "  inflating: Collection5/215.ann     \n",
            "  inflating: Collection5/215.txt     \n",
            "  inflating: Collection5/216.ann     \n",
            "  inflating: Collection5/216.txt     \n",
            "  inflating: Collection5/217.ann     \n",
            "  inflating: Collection5/217.txt     \n",
            "  inflating: Collection5/218.ann     \n",
            "  inflating: Collection5/218.txt     \n",
            "  inflating: Collection5/219.ann     \n",
            "  inflating: Collection5/219.txt     \n",
            "  inflating: Collection5/21_11_12c.ann  \n",
            "  inflating: Collection5/21_11_12c.txt  \n",
            "  inflating: Collection5/21_11_12h.ann  \n",
            "  inflating: Collection5/21_11_12h.txt  \n",
            "  inflating: Collection5/21_11_12i.ann  \n",
            "  inflating: Collection5/21_11_12i.txt  \n",
            "  inflating: Collection5/21_11_12j.ann  \n",
            "  inflating: Collection5/21_11_12j.txt  \n",
            "  inflating: Collection5/220.ann     \n",
            "  inflating: Collection5/220.txt     \n",
            "  inflating: Collection5/221.ann     \n",
            "  inflating: Collection5/221.txt     \n",
            "  inflating: Collection5/222.ann     \n",
            "  inflating: Collection5/222.txt     \n",
            "  inflating: Collection5/223.ann     \n",
            "  inflating: Collection5/223.txt     \n",
            "  inflating: Collection5/224.ann     \n",
            "  inflating: Collection5/224.txt     \n",
            "  inflating: Collection5/225.ann     \n",
            "  inflating: Collection5/225.txt     \n",
            "  inflating: Collection5/226.ann     \n",
            "  inflating: Collection5/226.txt     \n",
            "  inflating: Collection5/227.ann     \n",
            "  inflating: Collection5/227.txt     \n",
            "  inflating: Collection5/228.ann     \n",
            "  inflating: Collection5/228.txt     \n",
            "  inflating: Collection5/229.ann     \n",
            "  inflating: Collection5/229.txt     \n",
            "  inflating: Collection5/22_11_12a.ann  \n",
            "  inflating: Collection5/22_11_12a.txt  \n",
            "  inflating: Collection5/22_11_12c.ann  \n",
            "  inflating: Collection5/22_11_12c.txt  \n",
            "  inflating: Collection5/22_11_12d.ann  \n",
            "  inflating: Collection5/22_11_12d.txt  \n",
            "  inflating: Collection5/22_11_12g.ann  \n",
            "  inflating: Collection5/22_11_12g.txt  \n",
            "  inflating: Collection5/22_11_12h.ann  \n",
            "  inflating: Collection5/22_11_12h.txt  \n",
            "  inflating: Collection5/22_11_12i.ann  \n",
            "  inflating: Collection5/22_11_12i.txt  \n",
            "  inflating: Collection5/22_11_12j.ann  \n",
            "  inflating: Collection5/22_11_12j.txt  \n",
            "  inflating: Collection5/230.ann     \n",
            "  inflating: Collection5/230.txt     \n",
            "  inflating: Collection5/231.ann     \n",
            "  inflating: Collection5/231.txt     \n",
            "  inflating: Collection5/232.ann     \n",
            "  inflating: Collection5/232.txt     \n",
            "  inflating: Collection5/233.ann     \n",
            "  inflating: Collection5/233.txt     \n",
            "  inflating: Collection5/234.ann     \n",
            "  inflating: Collection5/234.txt     \n",
            "  inflating: Collection5/235.ann     \n",
            "  inflating: Collection5/235.txt     \n",
            "  inflating: Collection5/236.ann     \n",
            "  inflating: Collection5/236.txt     \n",
            "  inflating: Collection5/237.ann     \n",
            "  inflating: Collection5/237.txt     \n",
            "  inflating: Collection5/238.ann     \n",
            "  inflating: Collection5/238.txt     \n",
            "  inflating: Collection5/239.ann     \n",
            "  inflating: Collection5/239.txt     \n",
            "  inflating: Collection5/23_11_12a.ann  \n",
            "  inflating: Collection5/23_11_12a.txt  \n",
            "  inflating: Collection5/23_11_12b.ann  \n",
            "  inflating: Collection5/23_11_12b.txt  \n",
            "  inflating: Collection5/23_11_12c.ann  \n",
            "  inflating: Collection5/23_11_12c.txt  \n",
            "  inflating: Collection5/23_11_12d.ann  \n",
            "  inflating: Collection5/23_11_12d.txt  \n",
            "  inflating: Collection5/23_11_12e.ann  \n",
            "  inflating: Collection5/23_11_12e.txt  \n",
            "  inflating: Collection5/23_11_12f.ann  \n",
            "  inflating: Collection5/23_11_12f.txt  \n",
            "  inflating: Collection5/240.ann     \n",
            "  inflating: Collection5/240.txt     \n",
            "  inflating: Collection5/241.ann     \n",
            "  inflating: Collection5/241.txt     \n",
            "  inflating: Collection5/242.ann     \n",
            "  inflating: Collection5/242.txt     \n",
            "  inflating: Collection5/243.ann     \n",
            "  inflating: Collection5/243.txt     \n",
            "  inflating: Collection5/244.ann     \n",
            "  inflating: Collection5/244.txt     \n",
            "  inflating: Collection5/245.ann     \n",
            "  inflating: Collection5/245.txt     \n",
            "  inflating: Collection5/246.ann     \n",
            "  inflating: Collection5/246.txt     \n",
            "  inflating: Collection5/247.ann     \n",
            "  inflating: Collection5/247.txt     \n",
            "  inflating: Collection5/248.ann     \n",
            "  inflating: Collection5/248.txt     \n",
            "  inflating: Collection5/249.ann     \n",
            "  inflating: Collection5/249.txt     \n",
            "  inflating: Collection5/250.ann     \n",
            "  inflating: Collection5/250.txt     \n",
            "  inflating: Collection5/251.ann     \n",
            "  inflating: Collection5/251.txt     \n",
            "  inflating: Collection5/252.ann     \n",
            "  inflating: Collection5/252.txt     \n",
            "  inflating: Collection5/253.ann     \n",
            "  inflating: Collection5/253.txt     \n",
            "  inflating: Collection5/254.ann     \n",
            "  inflating: Collection5/254.txt     \n",
            "  inflating: Collection5/255.ann     \n",
            "  inflating: Collection5/255.txt     \n",
            "  inflating: Collection5/256.ann     \n",
            "  inflating: Collection5/256.txt     \n",
            "  inflating: Collection5/257.ann     \n",
            "  inflating: Collection5/257.txt     \n",
            "  inflating: Collection5/258.ann     \n",
            "  inflating: Collection5/258.txt     \n",
            "  inflating: Collection5/259.ann     \n",
            "  inflating: Collection5/259.txt     \n",
            "  inflating: Collection5/25_12_12a.ann  \n",
            "  inflating: Collection5/25_12_12a.txt  \n",
            "  inflating: Collection5/25_12_12c.ann  \n",
            "  inflating: Collection5/25_12_12c.txt  \n",
            "  inflating: Collection5/25_12_12d.ann  \n",
            "  inflating: Collection5/25_12_12d.txt  \n",
            "  inflating: Collection5/25_12_12e.ann  \n",
            "  inflating: Collection5/25_12_12e.txt  \n",
            "  inflating: Collection5/260.ann     \n",
            "  inflating: Collection5/260.txt     \n",
            "  inflating: Collection5/261.ann     \n",
            "  inflating: Collection5/261.txt     \n",
            "  inflating: Collection5/262.ann     \n",
            "  inflating: Collection5/262.txt     \n",
            "  inflating: Collection5/263.ann     \n",
            "  inflating: Collection5/263.txt     \n",
            "  inflating: Collection5/264.ann     \n",
            "  inflating: Collection5/264.txt     \n",
            "  inflating: Collection5/265.ann     \n",
            "  inflating: Collection5/265.txt     \n",
            "  inflating: Collection5/266.ann     \n",
            "  inflating: Collection5/266.txt     \n",
            "  inflating: Collection5/267.ann     \n",
            "  inflating: Collection5/267.txt     \n",
            "  inflating: Collection5/268.ann     \n",
            "  inflating: Collection5/268.txt     \n",
            "  inflating: Collection5/269.ann     \n",
            "  inflating: Collection5/269.txt     \n",
            "  inflating: Collection5/26_11_12b.ann  \n",
            "  inflating: Collection5/26_11_12b.txt  \n",
            "  inflating: Collection5/26_11_12c.ann  \n",
            "  inflating: Collection5/26_11_12c.txt  \n",
            "  inflating: Collection5/26_11_12e.ann  \n",
            "  inflating: Collection5/26_11_12e.txt  \n",
            "  inflating: Collection5/26_11_12f.ann  \n",
            "  inflating: Collection5/26_11_12f.txt  \n",
            "  inflating: Collection5/270.ann     \n",
            "  inflating: Collection5/270.txt     \n",
            "  inflating: Collection5/271.ann     \n",
            "  inflating: Collection5/271.txt     \n",
            "  inflating: Collection5/272.ann     \n",
            "  inflating: Collection5/272.txt     \n",
            "  inflating: Collection5/273.ann     \n",
            "  inflating: Collection5/273.txt     \n",
            "  inflating: Collection5/274.ann     \n",
            "  inflating: Collection5/274.txt     \n",
            "  inflating: Collection5/275.ann     \n",
            "  inflating: Collection5/275.txt     \n",
            "  inflating: Collection5/276.ann     \n",
            "  inflating: Collection5/276.txt     \n",
            "  inflating: Collection5/277.ann     \n",
            "  inflating: Collection5/277.txt     \n",
            "  inflating: Collection5/278.ann     \n",
            "  inflating: Collection5/278.txt     \n",
            "  inflating: Collection5/279.ann     \n",
            "  inflating: Collection5/279.txt     \n",
            "  inflating: Collection5/27_11_12a.ann  \n",
            "  inflating: Collection5/27_11_12a.txt  \n",
            "  inflating: Collection5/27_11_12c.ann  \n",
            "  inflating: Collection5/27_11_12c.txt  \n",
            "  inflating: Collection5/27_11_12d.ann  \n",
            "  inflating: Collection5/27_11_12d.txt  \n",
            "  inflating: Collection5/27_11_12e.ann  \n",
            "  inflating: Collection5/27_11_12e.txt  \n",
            "  inflating: Collection5/27_11_12j.ann  \n",
            "  inflating: Collection5/27_11_12j.txt  \n",
            "  inflating: Collection5/280.ann     \n",
            "  inflating: Collection5/280.txt     \n",
            "  inflating: Collection5/281.ann     \n",
            "  inflating: Collection5/281.txt     \n",
            "  inflating: Collection5/282.ann     \n",
            "  inflating: Collection5/282.txt     \n",
            "  inflating: Collection5/283.ann     \n",
            "  inflating: Collection5/283.txt     \n",
            "  inflating: Collection5/284.ann     \n",
            "  inflating: Collection5/284.txt     \n",
            "  inflating: Collection5/285.ann     \n",
            "  inflating: Collection5/285.txt     \n",
            "  inflating: Collection5/286.ann     \n",
            "  inflating: Collection5/286.txt     \n",
            "  inflating: Collection5/287.ann     \n",
            "  inflating: Collection5/287.txt     \n",
            "  inflating: Collection5/288.ann     \n",
            "  inflating: Collection5/288.txt     \n",
            "  inflating: Collection5/289.ann     \n",
            "  inflating: Collection5/289.txt     \n",
            "  inflating: Collection5/28_11_12a.ann  \n",
            "  inflating: Collection5/28_11_12a.txt  \n",
            "  inflating: Collection5/28_11_12f.ann  \n",
            "  inflating: Collection5/28_11_12f.txt  \n",
            "  inflating: Collection5/28_11_12g.ann  \n",
            "  inflating: Collection5/28_11_12g.txt  \n",
            "  inflating: Collection5/28_11_12h.ann  \n",
            "  inflating: Collection5/28_11_12h.txt  \n",
            "  inflating: Collection5/28_11_12i.ann  \n",
            "  inflating: Collection5/28_11_12i.txt  \n",
            "  inflating: Collection5/28_11_12j.ann  \n",
            "  inflating: Collection5/28_11_12j.txt  \n",
            "  inflating: Collection5/290.ann     \n",
            "  inflating: Collection5/290.txt     \n",
            "  inflating: Collection5/291.ann     \n",
            "  inflating: Collection5/291.txt     \n",
            "  inflating: Collection5/292.ann     \n",
            "  inflating: Collection5/292.txt     \n",
            "  inflating: Collection5/293.ann     \n",
            "  inflating: Collection5/293.txt     \n",
            "  inflating: Collection5/294.ann     \n",
            "  inflating: Collection5/294.txt     \n",
            "  inflating: Collection5/295.ann     \n",
            "  inflating: Collection5/295.txt     \n",
            "  inflating: Collection5/296.ann     \n",
            "  inflating: Collection5/296.txt     \n",
            "  inflating: Collection5/297.ann     \n",
            "  inflating: Collection5/297.txt     \n",
            "  inflating: Collection5/298.ann     \n",
            "  inflating: Collection5/298.txt     \n",
            "  inflating: Collection5/299.ann     \n",
            "  inflating: Collection5/299.txt     \n",
            "  inflating: Collection5/29_11_12a.ann  \n",
            "  inflating: Collection5/29_11_12a.txt  \n",
            "  inflating: Collection5/29_11_12b.ann  \n",
            "  inflating: Collection5/29_11_12b.txt  \n",
            "  inflating: Collection5/300.ann     \n",
            "  inflating: Collection5/300.txt     \n",
            "  inflating: Collection5/301.ann     \n",
            "  inflating: Collection5/301.txt     \n",
            "  inflating: Collection5/302.ann     \n",
            "  inflating: Collection5/302.txt     \n",
            "  inflating: Collection5/303.ann     \n",
            "  inflating: Collection5/303.txt     \n",
            "  inflating: Collection5/304.ann     \n",
            "  inflating: Collection5/304.txt     \n",
            "  inflating: Collection5/305.ann     \n",
            "  inflating: Collection5/305.txt     \n",
            "  inflating: Collection5/306.ann     \n",
            "  inflating: Collection5/306.txt     \n",
            "  inflating: Collection5/307.ann     \n",
            "  inflating: Collection5/307.txt     \n",
            "  inflating: Collection5/308.ann     \n",
            "  inflating: Collection5/308.txt     \n",
            "  inflating: Collection5/309.ann     \n",
            "  inflating: Collection5/309.txt     \n",
            "  inflating: Collection5/30_11_12b.ann  \n",
            "  inflating: Collection5/30_11_12b.txt  \n",
            "  inflating: Collection5/30_11_12h.ann  \n",
            "  inflating: Collection5/30_11_12h.txt  \n",
            "  inflating: Collection5/30_11_12i.ann  \n",
            "  inflating: Collection5/30_11_12i.txt  \n",
            "  inflating: Collection5/310.ann     \n",
            "  inflating: Collection5/310.txt     \n",
            "  inflating: Collection5/311.ann     \n",
            "  inflating: Collection5/311.txt     \n",
            "  inflating: Collection5/312.ann     \n",
            "  inflating: Collection5/312.txt     \n",
            "  inflating: Collection5/313.ann     \n",
            "  inflating: Collection5/313.txt     \n",
            "  inflating: Collection5/314.ann     \n",
            "  inflating: Collection5/314.txt     \n",
            "  inflating: Collection5/315.ann     \n",
            "  inflating: Collection5/315.txt     \n",
            "  inflating: Collection5/316.ann     \n",
            "  inflating: Collection5/316.txt     \n",
            "  inflating: Collection5/317.ann     \n",
            "  inflating: Collection5/317.txt     \n",
            "  inflating: Collection5/318.ann     \n",
            "  inflating: Collection5/318.txt     \n",
            "  inflating: Collection5/319.ann     \n",
            "  inflating: Collection5/319.txt     \n",
            "  inflating: Collection5/320.ann     \n",
            "  inflating: Collection5/320.txt     \n",
            "  inflating: Collection5/321.ann     \n",
            "  inflating: Collection5/321.txt     \n",
            "  inflating: Collection5/322.ann     \n",
            "  inflating: Collection5/322.txt     \n",
            "  inflating: Collection5/323.ann     \n",
            "  inflating: Collection5/323.txt     \n",
            "  inflating: Collection5/324.ann     \n",
            "  inflating: Collection5/324.txt     \n",
            "  inflating: Collection5/325.ann     \n",
            "  inflating: Collection5/325.txt     \n",
            "  inflating: Collection5/326.ann     \n",
            "  inflating: Collection5/326.txt     \n",
            "  inflating: Collection5/327.ann     \n",
            "  inflating: Collection5/327.txt     \n",
            "  inflating: Collection5/328.ann     \n",
            "  inflating: Collection5/328.txt     \n",
            "  inflating: Collection5/329.ann     \n",
            "  inflating: Collection5/329.txt     \n",
            "  inflating: Collection5/330.ann     \n",
            "  inflating: Collection5/330.txt     \n",
            "  inflating: Collection5/331.ann     \n",
            "  inflating: Collection5/331.txt     \n",
            "  inflating: Collection5/332.ann     \n",
            "  inflating: Collection5/332.txt     \n",
            "  inflating: Collection5/333.ann     \n",
            "  inflating: Collection5/333.txt     \n",
            "  inflating: Collection5/334.ann     \n",
            "  inflating: Collection5/334.txt     \n",
            "  inflating: Collection5/335.ann     \n",
            "  inflating: Collection5/335.txt     \n",
            "  inflating: Collection5/336.ann     \n",
            "  inflating: Collection5/336.txt     \n",
            "  inflating: Collection5/337.ann     \n",
            "  inflating: Collection5/337.txt     \n",
            "  inflating: Collection5/338.ann     \n",
            "  inflating: Collection5/338.txt     \n",
            "  inflating: Collection5/339.ann     \n",
            "  inflating: Collection5/339.txt     \n",
            "  inflating: Collection5/340.ann     \n",
            "  inflating: Collection5/340.txt     \n",
            "  inflating: Collection5/341.ann     \n",
            "  inflating: Collection5/341.txt     \n",
            "  inflating: Collection5/342.ann     \n",
            "  inflating: Collection5/342.txt     \n",
            "  inflating: Collection5/343.ann     \n",
            "  inflating: Collection5/343.txt     \n",
            "  inflating: Collection5/344.ann     \n",
            "  inflating: Collection5/344.txt     \n",
            "  inflating: Collection5/345.ann     \n",
            "  inflating: Collection5/345.txt     \n",
            "  inflating: Collection5/346.ann     \n",
            "  inflating: Collection5/346.txt     \n",
            "  inflating: Collection5/347.ann     \n",
            "  inflating: Collection5/347.txt     \n",
            "  inflating: Collection5/348.ann     \n",
            "  inflating: Collection5/348.txt     \n",
            "  inflating: Collection5/349.ann     \n",
            "  inflating: Collection5/349.txt     \n",
            "  inflating: Collection5/350.ann     \n",
            "  inflating: Collection5/350.txt     \n",
            "  inflating: Collection5/351.ann     \n",
            "  inflating: Collection5/351.txt     \n",
            "  inflating: Collection5/352.ann     \n",
            "  inflating: Collection5/352.txt     \n",
            "  inflating: Collection5/353.ann     \n",
            "  inflating: Collection5/353.txt     \n",
            "  inflating: Collection5/354.ann     \n",
            "  inflating: Collection5/354.txt     \n",
            "  inflating: Collection5/355.ann     \n",
            "  inflating: Collection5/355.txt     \n",
            "  inflating: Collection5/356.ann     \n",
            "  inflating: Collection5/356.txt     \n",
            "  inflating: Collection5/357.ann     \n",
            "  inflating: Collection5/357.txt     \n",
            "  inflating: Collection5/358.ann     \n",
            "  inflating: Collection5/358.txt     \n",
            "  inflating: Collection5/359.ann     \n",
            "  inflating: Collection5/359.txt     \n",
            "  inflating: Collection5/360.ann     \n",
            "  inflating: Collection5/360.txt     \n",
            "  inflating: Collection5/361.ann     \n",
            "  inflating: Collection5/361.txt     \n",
            "  inflating: Collection5/362.ann     \n",
            "  inflating: Collection5/362.txt     \n",
            "  inflating: Collection5/363.ann     \n",
            "  inflating: Collection5/363.txt     \n",
            "  inflating: Collection5/364.ann     \n",
            "  inflating: Collection5/364.txt     \n",
            "  inflating: Collection5/365.ann     \n",
            "  inflating: Collection5/365.txt     \n",
            "  inflating: Collection5/366.ann     \n",
            "  inflating: Collection5/366.txt     \n",
            "  inflating: Collection5/367.ann     \n",
            "  inflating: Collection5/367.txt     \n",
            "  inflating: Collection5/368.ann     \n",
            "  inflating: Collection5/368.txt     \n",
            "  inflating: Collection5/369.ann     \n",
            "  inflating: Collection5/369.txt     \n",
            "  inflating: Collection5/370.ann     \n",
            "  inflating: Collection5/370.txt     \n",
            "  inflating: Collection5/371.ann     \n",
            "  inflating: Collection5/371.txt     \n",
            "  inflating: Collection5/372.ann     \n",
            "  inflating: Collection5/372.txt     \n",
            "  inflating: Collection5/373.ann     \n",
            "  inflating: Collection5/373.txt     \n",
            "  inflating: Collection5/374.ann     \n",
            "  inflating: Collection5/374.txt     \n",
            "  inflating: Collection5/375.ann     \n",
            "  inflating: Collection5/375.txt     \n",
            "  inflating: Collection5/376.ann     \n",
            "  inflating: Collection5/376.txt     \n",
            "  inflating: Collection5/377.ann     \n",
            "  inflating: Collection5/377.txt     \n",
            "  inflating: Collection5/378.ann     \n",
            "  inflating: Collection5/378.txt     \n",
            "  inflating: Collection5/379.ann     \n",
            "  inflating: Collection5/379.txt     \n",
            "  inflating: Collection5/380.ann     \n",
            "  inflating: Collection5/380.txt     \n",
            "  inflating: Collection5/381.ann     \n",
            "  inflating: Collection5/381.txt     \n",
            "  inflating: Collection5/382.ann     \n",
            "  inflating: Collection5/382.txt     \n",
            "  inflating: Collection5/383.ann     \n",
            "  inflating: Collection5/383.txt     \n",
            "  inflating: Collection5/384.ann     \n",
            "  inflating: Collection5/384.txt     \n",
            "  inflating: Collection5/385.ann     \n",
            "  inflating: Collection5/385.txt     \n",
            "  inflating: Collection5/386.ann     \n",
            "  inflating: Collection5/386.txt     \n",
            "  inflating: Collection5/387.ann     \n",
            "  inflating: Collection5/387.txt     \n",
            "  inflating: Collection5/388.ann     \n",
            "  inflating: Collection5/388.txt     \n",
            "  inflating: Collection5/389.ann     \n",
            "  inflating: Collection5/389.txt     \n",
            "  inflating: Collection5/390.ann     \n",
            "  inflating: Collection5/390.txt     \n",
            "  inflating: Collection5/391.ann     \n",
            "  inflating: Collection5/391.txt     \n",
            "  inflating: Collection5/392.ann     \n",
            "  inflating: Collection5/392.txt     \n",
            "  inflating: Collection5/393.ann     \n",
            "  inflating: Collection5/393.txt     \n",
            "  inflating: Collection5/394.ann     \n",
            "  inflating: Collection5/394.txt     \n",
            "  inflating: Collection5/395.ann     \n",
            "  inflating: Collection5/395.txt     \n",
            "  inflating: Collection5/396.ann     \n",
            "  inflating: Collection5/396.txt     \n",
            "  inflating: Collection5/397.ann     \n",
            "  inflating: Collection5/397.txt     \n",
            "  inflating: Collection5/398.ann     \n",
            "  inflating: Collection5/398.txt     \n",
            "  inflating: Collection5/399.ann     \n",
            "  inflating: Collection5/399.txt     \n",
            "  inflating: Collection5/400.ann     \n",
            "  inflating: Collection5/400.txt     \n",
            "  inflating: Collection5/401.ann     \n",
            "  inflating: Collection5/401.txt     \n",
            "  inflating: Collection5/402.ann     \n",
            "  inflating: Collection5/402.txt     \n",
            "  inflating: Collection5/403.ann     \n",
            "  inflating: Collection5/403.txt     \n",
            "  inflating: Collection5/404.ann     \n",
            "  inflating: Collection5/404.txt     \n",
            "  inflating: Collection5/405.ann     \n",
            "  inflating: Collection5/405.txt     \n",
            "  inflating: Collection5/406.ann     \n",
            "  inflating: Collection5/406.txt     \n",
            "  inflating: Collection5/407.ann     \n",
            "  inflating: Collection5/407.txt     \n",
            "  inflating: Collection5/408.ann     \n",
            "  inflating: Collection5/408.txt     \n",
            "  inflating: Collection5/409.ann     \n",
            "  inflating: Collection5/409.txt     \n",
            "  inflating: Collection5/410.ann     \n",
            "  inflating: Collection5/410.txt     \n",
            "  inflating: Collection5/411.ann     \n",
            "  inflating: Collection5/411.txt     \n",
            "  inflating: Collection5/412.ann     \n",
            "  inflating: Collection5/412.txt     \n",
            "  inflating: Collection5/413.ann     \n",
            "  inflating: Collection5/413.txt     \n",
            "  inflating: Collection5/414.ann     \n",
            "  inflating: Collection5/414.txt     \n",
            "  inflating: Collection5/415.ann     \n",
            "  inflating: Collection5/415.txt     \n",
            "  inflating: Collection5/416.ann     \n",
            "  inflating: Collection5/416.txt     \n",
            "  inflating: Collection5/417.ann     \n",
            "  inflating: Collection5/417.txt     \n",
            "  inflating: Collection5/418.ann     \n",
            "  inflating: Collection5/418.txt     \n",
            "  inflating: Collection5/419.ann     \n",
            "  inflating: Collection5/419.txt     \n",
            "  inflating: Collection5/420.ann     \n",
            "  inflating: Collection5/420.txt     \n",
            "  inflating: Collection5/421.ann     \n",
            "  inflating: Collection5/421.txt     \n",
            "  inflating: Collection5/422.ann     \n",
            "  inflating: Collection5/422.txt     \n",
            "  inflating: Collection5/423.ann     \n",
            "  inflating: Collection5/423.txt     \n",
            "  inflating: Collection5/424.ann     \n",
            "  inflating: Collection5/424.txt     \n",
            "  inflating: Collection5/425.ann     \n",
            "  inflating: Collection5/425.txt     \n",
            "  inflating: Collection5/426.ann     \n",
            "  inflating: Collection5/426.txt     \n",
            "  inflating: Collection5/427.ann     \n",
            "  inflating: Collection5/427.txt     \n",
            "  inflating: Collection5/428.ann     \n",
            "  inflating: Collection5/428.txt     \n",
            "  inflating: Collection5/429.ann     \n",
            "  inflating: Collection5/429.txt     \n",
            "  inflating: Collection5/430.ann     \n",
            "  inflating: Collection5/430.txt     \n",
            "  inflating: Collection5/431.ann     \n",
            "  inflating: Collection5/431.txt     \n",
            "  inflating: Collection5/432.ann     \n",
            "  inflating: Collection5/432.txt     \n",
            "  inflating: Collection5/433.ann     \n",
            "  inflating: Collection5/433.txt     \n",
            "  inflating: Collection5/434.ann     \n",
            "  inflating: Collection5/434.txt     \n",
            "  inflating: Collection5/435.ann     \n",
            "  inflating: Collection5/435.txt     \n",
            "  inflating: Collection5/436.ann     \n",
            "  inflating: Collection5/436.txt     \n",
            "  inflating: Collection5/437.ann     \n",
            "  inflating: Collection5/437.txt     \n",
            "  inflating: Collection5/438.ann     \n",
            "  inflating: Collection5/438.txt     \n",
            "  inflating: Collection5/439.ann     \n",
            "  inflating: Collection5/439.txt     \n",
            "  inflating: Collection5/440.ann     \n",
            "  inflating: Collection5/440.txt     \n",
            "  inflating: Collection5/441.ann     \n",
            "  inflating: Collection5/441.txt     \n",
            "  inflating: Collection5/442.ann     \n",
            "  inflating: Collection5/442.txt     \n",
            "  inflating: Collection5/443.ann     \n",
            "  inflating: Collection5/443.txt     \n",
            "  inflating: Collection5/444.ann     \n",
            "  inflating: Collection5/444.txt     \n",
            "  inflating: Collection5/445.ann     \n",
            "  inflating: Collection5/445.txt     \n",
            "  inflating: Collection5/446.ann     \n",
            "  inflating: Collection5/446.txt     \n",
            "  inflating: Collection5/447.ann     \n",
            "  inflating: Collection5/447.txt     \n",
            "  inflating: Collection5/448.ann     \n",
            "  inflating: Collection5/448.txt     \n",
            "  inflating: Collection5/449.ann     \n",
            "  inflating: Collection5/449.txt     \n",
            "  inflating: Collection5/450.ann     \n",
            "  inflating: Collection5/450.txt     \n",
            "  inflating: Collection5/451.ann     \n",
            "  inflating: Collection5/451.txt     \n",
            "  inflating: Collection5/452.ann     \n",
            "  inflating: Collection5/452.txt     \n",
            "  inflating: Collection5/453.ann     \n",
            "  inflating: Collection5/453.txt     \n",
            "  inflating: Collection5/454.ann     \n",
            "  inflating: Collection5/454.txt     \n",
            "  inflating: Collection5/455.ann     \n",
            "  inflating: Collection5/455.txt     \n",
            "  inflating: Collection5/457.ann     \n",
            "  inflating: Collection5/457.txt     \n",
            "  inflating: Collection5/458.ann     \n",
            "  inflating: Collection5/458.txt     \n",
            "  inflating: Collection5/459.ann     \n",
            "  inflating: Collection5/459.txt     \n",
            "  inflating: Collection5/460.ann     \n",
            "  inflating: Collection5/460.txt     \n",
            "  inflating: Collection5/461.ann     \n",
            "  inflating: Collection5/461.txt     \n",
            "  inflating: Collection5/462.ann     \n",
            "  inflating: Collection5/462.txt     \n",
            "  inflating: Collection5/463.ann     \n",
            "  inflating: Collection5/463.txt     \n",
            "  inflating: Collection5/464.ann     \n",
            "  inflating: Collection5/464.txt     \n",
            "  inflating: Collection5/465.ann     \n",
            "  inflating: Collection5/465.txt     \n",
            "  inflating: Collection5/466.ann     \n",
            "  inflating: Collection5/466.txt     \n",
            "  inflating: Collection5/467.ann     \n",
            "  inflating: Collection5/467.txt     \n",
            "  inflating: Collection5/468.ann     \n",
            "  inflating: Collection5/468.txt     \n",
            "  inflating: Collection5/469.ann     \n",
            "  inflating: Collection5/469.txt     \n",
            "  inflating: Collection5/470.ann     \n",
            "  inflating: Collection5/470.txt     \n",
            "  inflating: Collection5/471.ann     \n",
            "  inflating: Collection5/471.txt     \n",
            "  inflating: Collection5/472.ann     \n",
            "  inflating: Collection5/472.txt     \n",
            "  inflating: Collection5/473.ann     \n",
            "  inflating: Collection5/473.txt     \n",
            "  inflating: Collection5/474.ann     \n",
            "  inflating: Collection5/474.txt     \n",
            "  inflating: Collection5/475.ann     \n",
            "  inflating: Collection5/475.txt     \n",
            "  inflating: Collection5/476.ann     \n",
            "  inflating: Collection5/476.txt     \n",
            "  inflating: Collection5/477.ann     \n",
            "  inflating: Collection5/477.txt     \n",
            "  inflating: Collection5/478.ann     \n",
            "  inflating: Collection5/478.txt     \n",
            "  inflating: Collection5/479.ann     \n",
            "  inflating: Collection5/479.txt     \n",
            "  inflating: Collection5/480.ann     \n",
            "  inflating: Collection5/480.txt     \n",
            "  inflating: Collection5/481.ann     \n",
            "  inflating: Collection5/481.txt     \n",
            "  inflating: Collection5/482.ann     \n",
            "  inflating: Collection5/482.txt     \n",
            "  inflating: Collection5/483.ann     \n",
            "  inflating: Collection5/483.txt     \n",
            "  inflating: Collection5/484.ann     \n",
            "  inflating: Collection5/484.txt     \n",
            "  inflating: Collection5/485.ann     \n",
            "  inflating: Collection5/485.txt     \n",
            "  inflating: Collection5/486.ann     \n",
            "  inflating: Collection5/486.txt     \n",
            "  inflating: Collection5/487.ann     \n",
            "  inflating: Collection5/487.txt     \n",
            "  inflating: Collection5/488.ann     \n",
            "  inflating: Collection5/488.txt     \n",
            "  inflating: Collection5/489.ann     \n",
            "  inflating: Collection5/489.txt     \n",
            "  inflating: Collection5/490.ann     \n",
            "  inflating: Collection5/490.txt     \n",
            "  inflating: Collection5/491.ann     \n",
            "  inflating: Collection5/491.txt     \n",
            "  inflating: Collection5/492.ann     \n",
            "  inflating: Collection5/492.txt     \n",
            "  inflating: Collection5/493.ann     \n",
            "  inflating: Collection5/493.txt     \n",
            "  inflating: Collection5/494.ann     \n",
            "  inflating: Collection5/494.txt     \n",
            "  inflating: Collection5/495.ann     \n",
            "  inflating: Collection5/495.txt     \n",
            "  inflating: Collection5/496.ann     \n",
            "  inflating: Collection5/496.txt     \n",
            "  inflating: Collection5/497.ann     \n",
            "  inflating: Collection5/497.txt     \n",
            "  inflating: Collection5/498.ann     \n",
            "  inflating: Collection5/498.txt     \n",
            "  inflating: Collection5/499.ann     \n",
            "  inflating: Collection5/499.txt     \n",
            "  inflating: Collection5/500.ann     \n",
            "  inflating: Collection5/500.txt     \n",
            "  inflating: Collection5/501.ann     \n",
            "  inflating: Collection5/501.txt     \n",
            "  inflating: Collection5/502.ann     \n",
            "  inflating: Collection5/502.txt     \n",
            "  inflating: Collection5/503.ann     \n",
            "  inflating: Collection5/503.txt     \n",
            "  inflating: Collection5/504.ann     \n",
            "  inflating: Collection5/504.txt     \n",
            "  inflating: Collection5/505.ann     \n",
            "  inflating: Collection5/505.txt     \n",
            "  inflating: Collection5/506.ann     \n",
            "  inflating: Collection5/506.txt     \n",
            "  inflating: Collection5/507.ann     \n",
            "  inflating: Collection5/507.txt     \n",
            "  inflating: Collection5/508.ann     \n",
            "  inflating: Collection5/508.txt     \n",
            "  inflating: Collection5/509.ann     \n",
            "  inflating: Collection5/509.txt     \n",
            "  inflating: Collection5/510.ann     \n",
            "  inflating: Collection5/510.txt     \n",
            "  inflating: Collection5/511.ann     \n",
            "  inflating: Collection5/511.txt     \n",
            "  inflating: Collection5/512.ann     \n",
            "  inflating: Collection5/512.txt     \n",
            "  inflating: Collection5/513.ann     \n",
            "  inflating: Collection5/513.txt     \n",
            "  inflating: Collection5/514.ann     \n",
            "  inflating: Collection5/514.txt     \n",
            "  inflating: Collection5/515.ann     \n",
            "  inflating: Collection5/515.txt     \n",
            "  inflating: Collection5/516.ann     \n",
            "  inflating: Collection5/516.txt     \n",
            "  inflating: Collection5/517.ann     \n",
            "  inflating: Collection5/517.txt     \n",
            "  inflating: Collection5/518.ann     \n",
            "  inflating: Collection5/518.txt     \n",
            "  inflating: Collection5/519.ann     \n",
            "  inflating: Collection5/519.txt     \n",
            "  inflating: Collection5/520.ann     \n",
            "  inflating: Collection5/520.txt     \n",
            "  inflating: Collection5/521.ann     \n",
            "  inflating: Collection5/521.txt     \n",
            "  inflating: Collection5/522.ann     \n",
            "  inflating: Collection5/522.txt     \n",
            "  inflating: Collection5/523.ann     \n",
            "  inflating: Collection5/523.txt     \n",
            "  inflating: Collection5/524.ann     \n",
            "  inflating: Collection5/524.txt     \n",
            "  inflating: Collection5/525.ann     \n",
            "  inflating: Collection5/525.txt     \n",
            "  inflating: Collection5/526.ann     \n",
            "  inflating: Collection5/526.txt     \n",
            "  inflating: Collection5/527.ann     \n",
            "  inflating: Collection5/527.txt     \n",
            "  inflating: Collection5/528.ann     \n",
            "  inflating: Collection5/528.txt     \n",
            "  inflating: Collection5/529.ann     \n",
            "  inflating: Collection5/529.txt     \n",
            "  inflating: Collection5/530.ann     \n",
            "  inflating: Collection5/530.txt     \n",
            "  inflating: Collection5/531.ann     \n",
            "  inflating: Collection5/531.txt     \n",
            "  inflating: Collection5/532.ann     \n",
            "  inflating: Collection5/532.txt     \n",
            "  inflating: Collection5/533 (!).ann  \n",
            "  inflating: Collection5/533 (!).txt  \n",
            "  inflating: Collection5/534.ann     \n",
            "  inflating: Collection5/534.txt     \n",
            "  inflating: Collection5/535.ann     \n",
            "  inflating: Collection5/535.txt     \n",
            "  inflating: Collection5/536.ann     \n",
            "  inflating: Collection5/536.txt     \n",
            "  inflating: Collection5/537.ann     \n",
            "  inflating: Collection5/537.txt     \n",
            "  inflating: Collection5/538.ann     \n",
            "  inflating: Collection5/538.txt     \n",
            "  inflating: Collection5/539.ann     \n",
            "  inflating: Collection5/539.txt     \n",
            "  inflating: Collection5/540.ann     \n",
            "  inflating: Collection5/540.txt     \n",
            "  inflating: Collection5/541.ann     \n",
            "  inflating: Collection5/541.txt     \n",
            "  inflating: Collection5/542.ann     \n",
            "  inflating: Collection5/542.txt     \n",
            "  inflating: Collection5/543.ann     \n",
            "  inflating: Collection5/543.txt     \n",
            "  inflating: Collection5/544.ann     \n",
            "  inflating: Collection5/544.txt     \n",
            "  inflating: Collection5/545.ann     \n",
            "  inflating: Collection5/545.txt     \n",
            "  inflating: Collection5/546.ann     \n",
            "  inflating: Collection5/546.txt     \n",
            "  inflating: Collection5/547.ann     \n",
            "  inflating: Collection5/547.txt     \n",
            "  inflating: Collection5/548.ann     \n",
            "  inflating: Collection5/548.txt     \n",
            "  inflating: Collection5/549.ann     \n",
            "  inflating: Collection5/549.txt     \n",
            "  inflating: Collection5/550.ann     \n",
            "  inflating: Collection5/550.txt     \n",
            "  inflating: Collection5/551.ann     \n",
            "  inflating: Collection5/551.txt     \n",
            "  inflating: Collection5/552.ann     \n",
            "  inflating: Collection5/552.txt     \n",
            "  inflating: Collection5/553.ann     \n",
            "  inflating: Collection5/553.txt     \n",
            "  inflating: Collection5/554.ann     \n",
            "  inflating: Collection5/554.txt     \n",
            "  inflating: Collection5/555 (!).ann  \n",
            "  inflating: Collection5/555 (!).txt  \n",
            "  inflating: Collection5/556.ann     \n",
            "  inflating: Collection5/556.txt     \n",
            "  inflating: Collection5/557.ann     \n",
            "  inflating: Collection5/557.txt     \n",
            "  inflating: Collection5/558.ann     \n",
            "  inflating: Collection5/558.txt     \n",
            "  inflating: Collection5/559.ann     \n",
            "  inflating: Collection5/559.txt     \n",
            "  inflating: Collection5/560.ann     \n",
            "  inflating: Collection5/560.txt     \n",
            "  inflating: Collection5/561.ann     \n",
            "  inflating: Collection5/561.txt     \n",
            "  inflating: Collection5/562.ann     \n",
            "  inflating: Collection5/562.txt     \n",
            "  inflating: Collection5/563.ann     \n",
            "  inflating: Collection5/563.txt     \n",
            "  inflating: Collection5/564.ann     \n",
            "  inflating: Collection5/564.txt     \n",
            "  inflating: Collection5/565.ann     \n",
            "  inflating: Collection5/565.txt     \n",
            "  inflating: Collection5/567.ann     \n",
            "  inflating: Collection5/567.txt     \n",
            "  inflating: Collection5/568.ann     \n",
            "  inflating: Collection5/568.txt     \n",
            "  inflating: Collection5/569.ann     \n",
            "  inflating: Collection5/569.txt     \n",
            "  inflating: Collection5/570.ann     \n",
            "  inflating: Collection5/570.txt     \n",
            "  inflating: Collection5/571.ann     \n",
            "  inflating: Collection5/571.txt     \n",
            "  inflating: Collection5/572.ann     \n",
            "  inflating: Collection5/572.txt     \n",
            "  inflating: Collection5/574.ann     \n",
            "  inflating: Collection5/574.txt     \n",
            "  inflating: Collection5/575.ann     \n",
            "  inflating: Collection5/575.txt     \n",
            "  inflating: Collection5/576.ann     \n",
            "  inflating: Collection5/576.txt     \n",
            "  inflating: Collection5/577.ann     \n",
            "  inflating: Collection5/577.txt     \n",
            "  inflating: Collection5/578.ann     \n",
            "  inflating: Collection5/578.txt     \n",
            "  inflating: Collection5/579.ann     \n",
            "  inflating: Collection5/579.txt     \n",
            "  inflating: Collection5/581.ann     \n",
            "  inflating: Collection5/581.txt     \n",
            "  inflating: Collection5/582.ann     \n",
            "  inflating: Collection5/582.txt     \n",
            "  inflating: Collection5/583.ann     \n",
            "  inflating: Collection5/583.txt     \n",
            "  inflating: Collection5/584 (!).ann  \n",
            "  inflating: Collection5/584 (!).txt  \n",
            "  inflating: Collection5/585.ann     \n",
            "  inflating: Collection5/585.txt     \n",
            "  inflating: Collection5/586.ann     \n",
            "  inflating: Collection5/586.txt     \n",
            "  inflating: Collection5/587.ann     \n",
            "  inflating: Collection5/587.txt     \n",
            "  inflating: Collection5/588.ann     \n",
            "  inflating: Collection5/588.txt     \n",
            "  inflating: Collection5/589.ann     \n",
            "  inflating: Collection5/589.txt     \n",
            "  inflating: Collection5/590.ann     \n",
            "  inflating: Collection5/590.txt     \n",
            "  inflating: Collection5/591.ann     \n",
            "  inflating: Collection5/591.txt     \n",
            "  inflating: Collection5/592.ann     \n",
            "  inflating: Collection5/592.txt     \n",
            "  inflating: Collection5/593.ann     \n",
            "  inflating: Collection5/593.txt     \n",
            "  inflating: Collection5/594.ann     \n",
            "  inflating: Collection5/594.txt     \n",
            "  inflating: Collection5/595.ann     \n",
            "  inflating: Collection5/595.txt     \n",
            "  inflating: Collection5/596.ann     \n",
            "  inflating: Collection5/596.txt     \n",
            "  inflating: Collection5/597.ann     \n",
            "  inflating: Collection5/597.txt     \n",
            "  inflating: Collection5/598 (!).ann  \n",
            "  inflating: Collection5/598 (!).txt  \n",
            "  inflating: Collection5/599.ann     \n",
            "  inflating: Collection5/599.txt     \n",
            "  inflating: Collection5/600.ann     \n",
            "  inflating: Collection5/600.txt     \n",
            "  inflating: Collection5/601.ann     \n",
            "  inflating: Collection5/601.txt     \n",
            "  inflating: Collection5/602.ann     \n",
            "  inflating: Collection5/602.txt     \n",
            "  inflating: Collection5/610.ann     \n",
            "  inflating: Collection5/610.txt     \n",
            "  inflating: Collection5/611.ann     \n",
            "  inflating: Collection5/611.txt     \n",
            "  inflating: Collection5/612.ann     \n",
            "  inflating: Collection5/612.txt     \n",
            "  inflating: Collection5/613.ann     \n",
            "  inflating: Collection5/613.txt     \n",
            "  inflating: Collection5/614.ann     \n",
            "  inflating: Collection5/614.txt     \n",
            "  inflating: Collection5/615.ann     \n",
            "  inflating: Collection5/615.txt     \n",
            "  inflating: Collection5/616.ann     \n",
            "  inflating: Collection5/616.txt     \n",
            "  inflating: Collection5/617.ann     \n",
            "  inflating: Collection5/617.txt     \n",
            "  inflating: Collection5/618.ann     \n",
            "  inflating: Collection5/618.txt     \n",
            "  inflating: Collection5/619.ann     \n",
            "  inflating: Collection5/619.txt     \n",
            "  inflating: Collection5/620.ann     \n",
            "  inflating: Collection5/620.txt     \n",
            "  inflating: Collection5/621.ann     \n",
            "  inflating: Collection5/621.txt     \n",
            "  inflating: Collection5/622.ann     \n",
            "  inflating: Collection5/622.txt     \n",
            "  inflating: Collection5/623.ann     \n",
            "  inflating: Collection5/623.txt     \n",
            "  inflating: Collection5/624.ann     \n",
            "  inflating: Collection5/624.txt     \n",
            "  inflating: Collection5/625.ann     \n",
            "  inflating: Collection5/625.txt     \n",
            "  inflating: Collection5/626.ann     \n",
            "  inflating: Collection5/626.txt     \n",
            "  inflating: Collection5/627.ann     \n",
            "  inflating: Collection5/627.txt     \n",
            "  inflating: Collection5/628.ann     \n",
            "  inflating: Collection5/628.txt     \n",
            "  inflating: Collection5/629.ann     \n",
            "  inflating: Collection5/629.txt     \n",
            "  inflating: Collection5/630.ann     \n",
            "  inflating: Collection5/630.txt     \n",
            "  inflating: Collection5/631.ann     \n",
            "  inflating: Collection5/631.txt     \n",
            "  inflating: Collection5/632.ann     \n",
            "  inflating: Collection5/632.txt     \n",
            "  inflating: Collection5/633.ann     \n",
            "  inflating: Collection5/633.txt     \n",
            "  inflating: Collection5/abdulatipov.ann  \n",
            "  inflating: Collection5/abdulatipov.txt  \n",
            "  inflating: Collection5/artjakov.ann  \n",
            "  inflating: Collection5/artjakov.txt  \n",
            "  inflating: Collection5/Avtovaz.ann  \n",
            "  inflating: Collection5/Avtovaz.txt  \n",
            "  inflating: Collection5/blokhin.ann  \n",
            "  inflating: Collection5/blokhin.txt  \n",
            "  inflating: Collection5/chaves.ann  \n",
            "  inflating: Collection5/chaves.txt  \n",
            "  inflating: Collection5/chirkunov.ann  \n",
            "  inflating: Collection5/chirkunov.txt  \n",
            "  inflating: Collection5/kamchatka.ann  \n",
            "  inflating: Collection5/kamchatka.txt  \n",
            "  inflating: Collection5/klinton.ann  \n",
            "  inflating: Collection5/klinton.txt  \n",
            "  inflating: Collection5/kuleshov.ann  \n",
            "  inflating: Collection5/kuleshov.txt  \n",
            "  inflating: Collection5/last_01.ann  \n",
            "  inflating: Collection5/last_01.txt  \n",
            "  inflating: Collection5/last_02.ann  \n",
            "  inflating: Collection5/last_02.txt  \n",
            "  inflating: Collection5/last_03.ann  \n",
            "  inflating: Collection5/last_03.txt  \n",
            "  inflating: Collection5/last_04.ann  \n",
            "  inflating: Collection5/last_04.txt  \n",
            "  inflating: Collection5/last_05.ann  \n",
            "  inflating: Collection5/last_05.txt  \n",
            "  inflating: Collection5/last_06.ann  \n",
            "  inflating: Collection5/last_06.txt  \n",
            "  inflating: Collection5/last_07_new.ann  \n",
            "  inflating: Collection5/last_07_new.txt  \n",
            "  inflating: Collection5/last_08.ann  \n",
            "  inflating: Collection5/last_08.txt  \n",
            "  inflating: Collection5/last_09.ann  \n",
            "  inflating: Collection5/last_09.txt  \n",
            "  inflating: Collection5/last_10.ann  \n",
            "  inflating: Collection5/last_10.txt  \n",
            "  inflating: Collection5/last_11.ann  \n",
            "  inflating: Collection5/last_11.txt  \n",
            "  inflating: Collection5/last_12.ann  \n",
            "  inflating: Collection5/last_12.txt  \n",
            "  inflating: Collection5/last_13.ann  \n",
            "  inflating: Collection5/last_13.txt  \n",
            "  inflating: Collection5/last_14.ann  \n",
            "  inflating: Collection5/last_14.txt  \n",
            "  inflating: Collection5/last_15.ann  \n",
            "  inflating: Collection5/last_15.txt  \n",
            "  inflating: Collection5/last_16.ann  \n",
            "  inflating: Collection5/last_16.txt  \n",
            "  inflating: Collection5/last_17.ann  \n",
            "  inflating: Collection5/last_17.txt  \n",
            "  inflating: Collection5/last_18.ann  \n",
            "  inflating: Collection5/last_18.txt  \n",
            "  inflating: Collection5/last_19.ann  \n",
            "  inflating: Collection5/last_19.txt  \n",
            "  inflating: Collection5/last_20.ann  \n",
            "  inflating: Collection5/last_20.txt  \n",
            "  inflating: Collection5/last_21.ann  \n",
            "  inflating: Collection5/last_21.txt  \n",
            "  inflating: Collection5/last_22.ann  \n",
            "  inflating: Collection5/last_22.txt  \n",
            "  inflating: Collection5/last_23.ann  \n",
            "  inflating: Collection5/last_23.txt  \n",
            "  inflating: Collection5/last_24.ann  \n",
            "  inflating: Collection5/last_24.txt  \n",
            "  inflating: Collection5/last_25.ann  \n",
            "  inflating: Collection5/last_25.txt  \n",
            "  inflating: Collection5/last_26.ann  \n",
            "  inflating: Collection5/last_26.txt  \n",
            "  inflating: Collection5/last_27.ann  \n",
            "  inflating: Collection5/last_27.txt  \n",
            "  inflating: Collection5/last_28.ann  \n",
            "  inflating: Collection5/last_28.txt  \n",
            "  inflating: Collection5/last_29.ann  \n",
            "  inflating: Collection5/last_29.txt  \n",
            "  inflating: Collection5/last_30_new.ann  \n",
            "  inflating: Collection5/last_30_new.txt  \n",
            "  inflating: Collection5/last_31.ann  \n",
            "  inflating: Collection5/last_31.txt  \n",
            "  inflating: Collection5/last_32.ann  \n",
            "  inflating: Collection5/last_32.txt  \n",
            "  inflating: Collection5/last_33.ann  \n",
            "  inflating: Collection5/last_33.txt  \n",
            "  inflating: Collection5/last_34.ann  \n",
            "  inflating: Collection5/last_34.txt  \n",
            "  inflating: Collection5/last_35.ann  \n",
            "  inflating: Collection5/last_35.txt  \n",
            "  inflating: Collection5/last_36.ann  \n",
            "  inflating: Collection5/last_36.txt  \n",
            "  inflating: Collection5/last_37.ann  \n",
            "  inflating: Collection5/last_37.txt  \n",
            "  inflating: Collection5/last_38.ann  \n",
            "  inflating: Collection5/last_38.txt  \n",
            "  inflating: Collection5/last_39.ann  \n",
            "  inflating: Collection5/last_39.txt  \n",
            "  inflating: Collection5/last_40.ann  \n",
            "  inflating: Collection5/last_40.txt  \n",
            "  inflating: Collection5/last_41.ann  \n",
            "  inflating: Collection5/last_41.txt  \n",
            "  inflating: Collection5/last_42.ann  \n",
            "  inflating: Collection5/last_42.txt  \n",
            "  inflating: Collection5/last_43.ann  \n",
            "  inflating: Collection5/last_43.txt  \n",
            "  inflating: Collection5/last_44.ann  \n",
            "  inflating: Collection5/last_44.txt  \n",
            "  inflating: Collection5/last_45.ann  \n",
            "  inflating: Collection5/last_45.txt  \n",
            "  inflating: Collection5/last_46.ann  \n",
            "  inflating: Collection5/last_46.txt  \n",
            "  inflating: Collection5/last_47.ann  \n",
            "  inflating: Collection5/last_47.txt  \n",
            "  inflating: Collection5/last_48.ann  \n",
            "  inflating: Collection5/last_48.txt  \n",
            "  inflating: Collection5/last_49.ann  \n",
            "  inflating: Collection5/last_49.txt  \n",
            "  inflating: Collection5/last_50.ann  \n",
            "  inflating: Collection5/last_50.txt  \n",
            "  inflating: Collection5/last_51.ann  \n",
            "  inflating: Collection5/last_51.txt  \n",
            "  inflating: Collection5/last_52.ann  \n",
            "  inflating: Collection5/last_52.txt  \n",
            "  inflating: Collection5/last_53.ann  \n",
            "  inflating: Collection5/last_53.txt  \n",
            "  inflating: Collection5/last_54.ann  \n",
            "  inflating: Collection5/last_54.txt  \n",
            "  inflating: Collection5/last_55.ann  \n",
            "  inflating: Collection5/last_55.txt  \n",
            "  inflating: Collection5/last_56.ann  \n",
            "  inflating: Collection5/last_56.txt  \n",
            "  inflating: Collection5/last_57.ann  \n",
            "  inflating: Collection5/last_57.txt  \n",
            "  inflating: Collection5/last_58.ann  \n",
            "  inflating: Collection5/last_58.txt  \n",
            "  inflating: Collection5/last_59.ann  \n",
            "  inflating: Collection5/last_59.txt  \n",
            "  inflating: Collection5/last_60.ann  \n",
            "  inflating: Collection5/last_60.txt  \n",
            "  inflating: Collection5/last_61.ann  \n",
            "  inflating: Collection5/last_61.txt  \n",
            "  inflating: Collection5/last_62.ann  \n",
            "  inflating: Collection5/last_62.txt  \n",
            "  inflating: Collection5/last_63.ann  \n",
            "  inflating: Collection5/last_63.txt  \n",
            "  inflating: Collection5/last_64.ann  \n",
            "  inflating: Collection5/last_64.txt  \n",
            "  inflating: Collection5/last_65.ann  \n",
            "  inflating: Collection5/last_65.txt  \n",
            "  inflating: Collection5/last_66.ann  \n",
            "  inflating: Collection5/last_66.txt  \n",
            "  inflating: Collection5/last_67.ann  \n",
            "  inflating: Collection5/last_67.txt  \n",
            "  inflating: Collection5/last_68.ann  \n",
            "  inflating: Collection5/last_68.txt  \n",
            "  inflating: Collection5/last_69.ann  \n",
            "  inflating: Collection5/last_69.txt  \n",
            "  inflating: Collection5/last_70.ann  \n",
            "  inflating: Collection5/last_70.txt  \n",
            "  inflating: Collection5/last_71.ann  \n",
            "  inflating: Collection5/last_71.txt  \n",
            "  inflating: Collection5/last_72.ann  \n",
            "  inflating: Collection5/last_72.txt  \n",
            "  inflating: Collection5/last_73.ann  \n",
            "  inflating: Collection5/last_73.txt  \n",
            "  inflating: Collection5/last_74.ann  \n",
            "  inflating: Collection5/last_74.txt  \n",
            "  inflating: Collection5/last_75.ann  \n",
            "  inflating: Collection5/last_75.txt  \n",
            "  inflating: Collection5/lenoblast.ann  \n",
            "  inflating: Collection5/lenoblast.txt  \n",
            "  inflating: Collection5/maykl dzhekson.ann  \n",
            "  inflating: Collection5/maykl dzhekson.txt  \n",
            "  inflating: Collection5/mvd.ann     \n",
            "  inflating: Collection5/mvd.txt     \n",
            "  inflating: Collection5/mvd2.ann    \n",
            "  inflating: Collection5/mvd2.txt    \n",
            "  inflating: Collection5/rosobrnadzor.ann  \n",
            "  inflating: Collection5/rosobrnadzor.txt  \n",
            "  inflating: Collection5/ryadovoy chelah.ann  \n",
            "  inflating: Collection5/ryadovoy chelah.txt  \n",
            "  inflating: Collection5/semenenko.ann  \n",
            "  inflating: Collection5/semenenko.txt  \n",
            "  inflating: Collection5/shojgu1.ann  \n",
            "  inflating: Collection5/shojgu1.txt  \n",
            "  inflating: Collection5/shojgu3.ann  \n",
            "  inflating: Collection5/shojgu3.txt  \n",
            "  inflating: Collection5/shojgu4.ann  \n",
            "  inflating: Collection5/shojgu4.txt  \n",
            "  inflating: Collection5/shojgu6.ann  \n",
            "  inflating: Collection5/shojgu6.txt  \n",
            "  inflating: Collection5/si_tzjanpin.ann  \n",
            "  inflating: Collection5/si_tzjanpin.txt  \n",
            "  inflating: Collection5/sobjanin2.ann  \n",
            "  inflating: Collection5/sobjanin2.txt  \n",
            "  inflating: Collection5/turkmenija.ann  \n",
            "  inflating: Collection5/turkmenija.txt  \n",
            "  inflating: Collection5/uchitel.ann  \n",
            "  inflating: Collection5/uchitel.txt  \n"
          ]
        }
      ],
      "source": [
        "!wget http://www.labinform.ru/pub/named_entities/collection5.zip\n",
        "!unzip collection5.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим и импортируем библиотеки необходимые для решения поставленной задачи."
      ],
      "metadata": {
        "id": "qZJSJYQbt3iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install natasha corus\n",
        "!pip -q install spacy\n",
        "!python -m spacy download ru_core_news_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDxSeJldus-q",
        "outputId": "7a3f502d-bc99-47f7-93d8-0dcb923387ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting corus\n",
            "  Downloading corus-0.9.0-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 109 kB/s \n",
            "\u001b[?25hCollecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 31.3 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: intervaltree, docopt\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=9f37b54eacc05dfe1c6278a95a6e82859cdebe79bda0aafbe57a47c78034f02c\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=d46cf067e181e3d2b2c26ede540b9f1396ca11c69202aaba0ec978d4d6cbe513\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built intervaltree docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, razdel, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha, corus\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed corus-0.9.0 dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.4.0/ru_core_news_md-3.4.0-py3-none-any.whl (41.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymorphy2>=0.9 in /usr/local/lib/python3.7/dist-packages (from ru-core-news-md==3.4.0) (0.9.1)\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from ru-core-news-md==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-md==3.4.0) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2>=0.9->ru-core-news-md==3.4.0) (2.4.417127.4579844)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: ru-core-news-md\n",
            "Successfully installed ru-core-news-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_ru')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "from corus import load_ne5\n",
        "from razdel import tokenize\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94tFqrMKw8vG",
        "outputId": "89329a1d-a270-49bf-f9b3-3e5179188f1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'Collection5/'\n",
        "records = load_ne5(dir)\n",
        "next(records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YyXlG2kvTSi",
        "outputId": "957761ce-b6c7-47a0-85dd-25f00bfff2cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ne5Markup(\n",
              "    id='249',\n",
              "    text='Д.Медведев уволил 15 высокопоставленных милиционеров\\r\\n\\r\\nДмитрий Медведев провел новые увольнения в Министерстве внутренних дел. Как сообщает пресс-служба главы государства, президент подписал 1 апреля указ об освобождении от должностей 15 высокопоставленных милиционеров.\\r\\n\\r\\nО том, связаны ли причины отставок с переаттестацией милиционеров в полицейских, не сообщается.\\r\\n\\r\\nСреди уволенных:\\r\\n\\r\\n- начальник Национального центрального бюро Интерпола при МВД РФ, генерал-майор милиции Тимур Лахонин;\\r\\n\\r\\n- заместитель начальника ГУВД по Москве по экономической безопасности, генерал-лейтенант милиции Виктор Васильев;\\r\\n\\r\\n- начальник управления по налоговым преступлениям Главного управления внутренних дел по Москве, полковник милиции Юрий Васильев;\\r\\n\\r\\n- начальник управления по борьбе с экономическими преступлениями Главного управления внутренних дел по Москве, генерал-майор милиции Александр Данилов;\\r\\n\\r\\n- начальник управления внутренних дел по Восточному административному округу Главного управления внутренних дел по Москве, генерал-майор милиции Евгений Дубенский.\\r\\n\\r\\nОт должностей также освобождены: генерал-майор милиции Самвел Маилян, генерал-майор юстиции Александр Березутский, генерал-майор милиции Азат Нурутдинов, генерал-лейтенант внутренней службы Леонид Рогачев, полковник внутренней службы Валерий Бутылин, полковник милиции Геннадий Дулькин, генерал-лейтенант милиции Алик Хабибуллин, генерал-майор милиции Ильдар Хуснуллин, генерал-майор милиции Вячеслав Новиков, генерал-майор милиции Владимир Филиппов.\\r\\n\\r\\n',\n",
              "    spans=[Ne5Span(\n",
              "         index='T1',\n",
              "         type='PER',\n",
              "         start=0,\n",
              "         stop=10,\n",
              "         text='Д.Медведев'\n",
              "     ), Ne5Span(\n",
              "         index='T2',\n",
              "         type='PER',\n",
              "         start=56,\n",
              "         stop=72,\n",
              "         text='Дмитрий Медведев'\n",
              "     ), Ne5Span(\n",
              "         index='T3',\n",
              "         type='ORG',\n",
              "         start=99,\n",
              "         stop=126,\n",
              "         text='Министерстве внутренних дел'\n",
              "     ), Ne5Span(\n",
              "         index='T4',\n",
              "         type='ORG',\n",
              "         start=406,\n",
              "         stop=437,\n",
              "         text='Национального центрального бюро'\n",
              "     ), Ne5Span(\n",
              "         index='T5',\n",
              "         type='ORG',\n",
              "         start=438,\n",
              "         stop=447,\n",
              "         text='Интерпола'\n",
              "     ), Ne5Span(\n",
              "         index='T6',\n",
              "         type='ORG',\n",
              "         start=452,\n",
              "         stop=455,\n",
              "         text='МВД'\n",
              "     ), Ne5Span(\n",
              "         index='T7',\n",
              "         type='GEOPOLIT',\n",
              "         start=456,\n",
              "         stop=458,\n",
              "         text='РФ'\n",
              "     ), Ne5Span(\n",
              "         index='T8',\n",
              "         type='PER',\n",
              "         start=482,\n",
              "         stop=495,\n",
              "         text='Тимур Лахонин'\n",
              "     ), Ne5Span(\n",
              "         index='T9',\n",
              "         type='ORG',\n",
              "         start=525,\n",
              "         stop=529,\n",
              "         text='ГУВД'\n",
              "     ), Ne5Span(\n",
              "         index='T10',\n",
              "         type='LOC',\n",
              "         start=533,\n",
              "         stop=539,\n",
              "         text='Москве'\n",
              "     ), Ne5Span(\n",
              "         index='T11',\n",
              "         type='PER',\n",
              "         start=597,\n",
              "         stop=612,\n",
              "         text='Виктор Васильев'\n",
              "     ), Ne5Span(\n",
              "         index='T12',\n",
              "         type='ORG',\n",
              "         start=667,\n",
              "         stop=701,\n",
              "         text='Главного управления внутренних дел'\n",
              "     ), Ne5Span(\n",
              "         index='T13',\n",
              "         type='LOC',\n",
              "         start=705,\n",
              "         stop=711,\n",
              "         text='Москве'\n",
              "     ), Ne5Span(\n",
              "         index='T14',\n",
              "         type='PER',\n",
              "         start=731,\n",
              "         stop=744,\n",
              "         text='Юрий Васильев'\n",
              "     ), Ne5Span(\n",
              "         index='T15',\n",
              "         type='ORG',\n",
              "         start=814,\n",
              "         stop=848,\n",
              "         text='Главного управления внутренних дел'\n",
              "     ), Ne5Span(\n",
              "         index='T16',\n",
              "         type='LOC',\n",
              "         start=852,\n",
              "         stop=858,\n",
              "         text='Москве'\n",
              "     ), Ne5Span(\n",
              "         index='T17',\n",
              "         type='PER',\n",
              "         start=882,\n",
              "         stop=899,\n",
              "         text='Александр Данилов'\n",
              "     ), Ne5Span(\n",
              "         index='T18',\n",
              "         type='LOC',\n",
              "         start=945,\n",
              "         stop=980,\n",
              "         text='Восточному административному округу'\n",
              "     ), Ne5Span(\n",
              "         index='T19',\n",
              "         type='ORG',\n",
              "         start=981,\n",
              "         stop=1015,\n",
              "         text='Главного управления внутренних дел'\n",
              "     ), Ne5Span(\n",
              "         index='T20',\n",
              "         type='LOC',\n",
              "         start=1019,\n",
              "         stop=1025,\n",
              "         text='Москве'\n",
              "     ), Ne5Span(\n",
              "         index='T21',\n",
              "         type='PER',\n",
              "         start=1049,\n",
              "         stop=1066,\n",
              "         text='Евгений Дубенский'\n",
              "     ), Ne5Span(\n",
              "         index='T22',\n",
              "         type='PER',\n",
              "         start=1126,\n",
              "         stop=1139,\n",
              "         text='Самвел Маилян'\n",
              "     ), Ne5Span(\n",
              "         index='T23',\n",
              "         type='PER',\n",
              "         start=1163,\n",
              "         stop=1184,\n",
              "         text='Александр Березутский'\n",
              "     ), Ne5Span(\n",
              "         index='T24',\n",
              "         type='PER',\n",
              "         start=1208,\n",
              "         stop=1223,\n",
              "         text='Азат Нурутдинов'\n",
              "     ), Ne5Span(\n",
              "         index='T25',\n",
              "         type='PER',\n",
              "         start=1261,\n",
              "         stop=1275,\n",
              "         text='Леонид Рогачев'\n",
              "     ), Ne5Span(\n",
              "         index='T26',\n",
              "         type='PER',\n",
              "         start=1305,\n",
              "         stop=1320,\n",
              "         text='Валерий Бутылин'\n",
              "     ), Ne5Span(\n",
              "         index='T27',\n",
              "         type='PER',\n",
              "         start=1340,\n",
              "         stop=1356,\n",
              "         text='Геннадий Дулькин'\n",
              "     ), Ne5Span(\n",
              "         index='T28',\n",
              "         type='PER',\n",
              "         start=1384,\n",
              "         stop=1399,\n",
              "         text='Алик Хабибуллин'\n",
              "     ), Ne5Span(\n",
              "         index='T29',\n",
              "         type='PER',\n",
              "         start=1423,\n",
              "         stop=1439,\n",
              "         text='Ильдар Хуснуллин'\n",
              "     ), Ne5Span(\n",
              "         index='T30',\n",
              "         type='PER',\n",
              "         start=1463,\n",
              "         stop=1479,\n",
              "         text='Вячеслав Новиков'\n",
              "     ), Ne5Span(\n",
              "         index='T31',\n",
              "         type='PER',\n",
              "         start=1503,\n",
              "         stop=1520,\n",
              "         text='Владимир Филиппов'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **nltk**"
      ],
      "metadata": {
        "id": "ICBms6U80yMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Насколько я понял несмотря на то, что библиотекой **nltk** поддерживается определение тэгов для русскоязычных текстов распознавание именнованых сущностей в ее рамках реализовано исключительно для английского языка. Как видим на примере ниже, метод *ne_chunk* не выдал результатов для документа из датасета."
      ],
      "metadata": {
        "id": "gD3ZvAiz77px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "records = load_ne5(dir)\n",
        "for ix, rec in enumerate(records):\n",
        "  print(rec.text)\n",
        "  print('\\nИменованные сущности:')\n",
        "  for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text), lang='rus')):\n",
        "    if hasattr(chunk, 'label'):\n",
        "      print(f'{chunk} - {chunk.label()}')\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MredNR_n8nRO",
        "outputId": "61eba873-437f-4e42-adbc-75cd0e97d24b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Д.Медведев уволил 15 высокопоставленных милиционеров\r\n",
            "\r\n",
            "Дмитрий Медведев провел новые увольнения в Министерстве внутренних дел. Как сообщает пресс-служба главы государства, президент подписал 1 апреля указ об освобождении от должностей 15 высокопоставленных милиционеров.\r\n",
            "\r\n",
            "О том, связаны ли причины отставок с переаттестацией милиционеров в полицейских, не сообщается.\r\n",
            "\r\n",
            "Среди уволенных:\r\n",
            "\r\n",
            "- начальник Национального центрального бюро Интерпола при МВД РФ, генерал-майор милиции Тимур Лахонин;\r\n",
            "\r\n",
            "- заместитель начальника ГУВД по Москве по экономической безопасности, генерал-лейтенант милиции Виктор Васильев;\r\n",
            "\r\n",
            "- начальник управления по налоговым преступлениям Главного управления внутренних дел по Москве, полковник милиции Юрий Васильев;\r\n",
            "\r\n",
            "- начальник управления по борьбе с экономическими преступлениями Главного управления внутренних дел по Москве, генерал-майор милиции Александр Данилов;\r\n",
            "\r\n",
            "- начальник управления внутренних дел по Восточному административному округу Главного управления внутренних дел по Москве, генерал-майор милиции Евгений Дубенский.\r\n",
            "\r\n",
            "От должностей также освобождены: генерал-майор милиции Самвел Маилян, генерал-майор юстиции Александр Березутский, генерал-майор милиции Азат Нурутдинов, генерал-лейтенант внутренней службы Леонид Рогачев, полковник внутренней службы Валерий Бутылин, полковник милиции Геннадий Дулькин, генерал-лейтенант милиции Алик Хабибуллин, генерал-майор милиции Ильдар Хуснуллин, генерал-майор милиции Вячеслав Новиков, генерал-майор милиции Владимир Филиппов.\r\n",
            "\r\n",
            "\n",
            "\n",
            "Именованные сущности:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **spacy**"
      ],
      "metadata": {
        "id": "WouWujEo05lN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проведем необходимую предобработку данных и создадим файлы с BIO разметкой."
      ],
      "metadata": {
        "id": "Ks_g3arKKNJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "for rec in records:\n",
        "    words = []\n",
        "    labels = []\n",
        "    idx_ent = -1\n",
        "    len_ents = len(rec.spans)\n",
        "    rec_entities = sorted(rec.spans, key=lambda v: v.start)\n",
        "    ent = None\n",
        "    is_start = None\n",
        "    for token in tokenize(rec.text):\n",
        "        type_ent = 'OUT'\n",
        "        if len_ents == 0:\n",
        "            words.append(token.text)\n",
        "            labels.append(type_ent)\n",
        "            continue\n",
        "\n",
        "        if (idx_ent == -1) or (idx_ent + 1 < len_ents and token.start > ent.stop):\n",
        "            idx_ent += 1\n",
        "            ent = rec_entities[idx_ent]\n",
        "            is_start = True\n",
        "\n",
        "        if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = 'B-' + ent.type if is_start else 'I-' + ent.type\n",
        "                is_start = False\n",
        "        words.append(token.text)\n",
        "        labels.append(type_ent)\n",
        "    \n",
        "    docs.append([words, labels])"
      ],
      "metadata": {
        "id": "7e8JuJTS7EdX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0][0])\n",
        "print(docs[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuMIp_zW72u4",
        "outputId": "d4d66232-fcab-48ae-dbd9-b6fb0d78f572"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Врио', 'главы', 'Дагестана', 'намерен', 'бороться', 'с', 'клановостью', 'во', 'власти', 'Временно', 'исполняющий', 'обязанности', 'президента', 'Дагестана', 'Рамазан', 'Абдулатипов', 'намерен', 'бороться', 'с', 'так', 'называемой', '\"', 'клановостью', '\"', 'во', 'властных', 'структурах', 'республики', '.', 'Он', 'отметил', ',', 'что', 'порой', 'такие', 'кланы', 'во', 'власти', 'превращаются', 'в', 'банды', '.', 'Рамазан', 'Абдулатипов', '.', 'Архив', 'Временно', 'исполняющий', 'обязанности', 'президента', 'Дагестана', 'Рамазан', 'Абдулатипов', 'намерен', 'бороться', 'с', 'так', 'называемой', '\"', 'клановостью', '\"', 'во', 'властных', 'структурах', 'республики', '.', '\"', 'Другое', 'дело', ',', 'когда', 'мне', 'говорят', ',', 'мол', ',', 'надо', 'свою', 'команду', 'создавать', '…', 'И', 'вы', 'знаете', '—', 'вот', 'сначала', 'создают', 'команду', ',', 'а', 'потом', 'эта', 'команда', 'превращается', 'в', 'банду', '.', 'Это', 'и', 'есть', 'клановость', '.', 'Они', 'не', 'родственники', ',', 'они', 'люди', 'разных', 'корней', ',', 'но', 'в', 'своей', 'коррупции', ',', 'в', 'своем', 'бандитизме', 'они', 'поддерживают', 'друг', 'друга', '.', '<', '…', '>', 'Национальную', 'или', 'религиозную', 'тему', 'они', 'вытаскивают', ',', 'когда', 'надо', 'что-то', 'прикрыть', '.', 'Не', 'раскрыть', 'свою', 'самобытность', ',', 'а', 'прикрыть', 'свою', 'грязь', '.', 'Конечно', ',', 'с', 'этим', 'я', 'намерен', 'жестко', 'бороться', '\"', ',', '—', 'сказал', 'Абдулатипов', 'в', 'интервью', 'газете', '\"', 'Московской', 'комсомолец', '\"', ',', 'опубликованном', 'в', 'четверг', '.', 'Абдулатипов', 'стал', 'временно', 'исполняющим', 'обязанности', 'главы', 'республики', 'в', 'конце', 'января', '.', 'Выборы', 'в', 'Дагестане', 'намечены', 'на', 'сентябрь', ',', 'глава', 'субъекта', 'будет', 'избираться', 'депутатами', 'народного', 'собрания', 'из', 'трех', 'кандидатов', ',', 'представленных', 'президентом', 'России', '.', 'Первого', 'июня', 'в', 'Махачкале', 'был', 'задержан', 'мэр', 'этого', 'города', 'Саид', 'Амиров', '.', 'Он', 'обвиняется', 'в', 'организации', 'убийства', 'следователя', 'Арсена', 'Гаджибекова', '.', 'После', 'задержания', 'Амирова', 'вертолетом', 'доставили', 'в', 'аэропорт', ',', 'а', 'затем', 'самолетом', 'в', 'Москву', ',', 'где', 'суд', 'арестовал', 'его', 'на', 'два', 'месяца', '.', 'Мэр', 'своей', 'вины', 'не', 'признает', '.']\n",
            "['OUT', 'OUT', 'B-LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-LOC', 'B-PER', 'I-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-PER', 'I-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-LOC', 'B-PER', 'I-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'B-MEDIA', 'I-MEDIA', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-GEOPOLIT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-PER', 'I-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-PER', 'I-PER', 'OUT', 'OUT', 'OUT', 'B-PER', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'B-LOC', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT', 'OUT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_coeff = 0.75\n",
        "\n",
        "with open('c5.bio', 'w') as w:\n",
        "    with open('c5_train.bio', 'w') as w1:\n",
        "        with open('c5_valid.bio', 'w') as w2:\n",
        "            for irec, rec in enumerate(docs):\n",
        "                for line in map(lambda vl: '\\t'.join(vl) + '\\n', zip(*rec)):\n",
        "                    w.write(line)\n",
        "                    if irec < len(docs) * training_coeff:\n",
        "                        w1.write(line)\n",
        "                    else:\n",
        "                        w2.write(line)\n",
        "                w.write('\\n')\n",
        "                if irec < len(docs) * training_coeff:\n",
        "                    w1.write('\\n')\n",
        "                else:\n",
        "                    w2.write('\\n')"
      ],
      "metadata": {
        "id": "8ue1vPXRxGyF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init config base_config.cfg -F -p  ner -l ru\n",
        "!python -m spacy init fill-config base_config.cfg config.cfg\n",
        "# !python -m spacy convert c5.bio . -t json -c ner\n",
        "!python -m spacy convert c5_train.bio . -c ner\n",
        "!python -m spacy convert c5_valid.bio . -c ner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBnwEKHO8sjO",
        "outputId": "431d1684-c64a-4878-af06-bd023bd7de3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
            "install the spacy-transformers package and re-run this command. The config\n",
            "generated now does not use transformers.\u001b[0m\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: ru\n",
            "- Pipeline: ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "base_config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train base_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "\u001b[38;5;3m⚠ Nothing to auto-fill: base config is already complete\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (750 documents): c5_train.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n",
            "into documents with `-n 10`.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (249 documents): c5_valid.spacy\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем модель."
      ],
      "metadata": {
        "id": "6Ah9AXvo1AGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train c5_train.spacy --paths.dev c5_valid.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE9uvNCKBTkX",
        "outputId": "01fea43a-749f-43f4-ca74-430418523940"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-08-30 19:10:30,795] [INFO] Set up nlp object from config\n",
            "[2022-08-30 19:10:30,805] [INFO] Pipeline: ['tok2vec', 'ner']\n",
            "[2022-08-30 19:10:30,811] [INFO] Created vocabulary\n",
            "[2022-08-30 19:10:30,813] [INFO] Finished initializing nlp object\n",
            "[2022-08-30 19:10:34,770] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  --------  ------  ------  ------  ------\n",
            "  0       0          0.00    456.85   79.70   78.73   80.70    0.80\n",
            "  0     200       1983.08   9789.79   93.89   94.58   93.22    0.94\n",
            "  0     400       1882.58   4973.84   95.66   95.81   95.51    0.96\n",
            "  0     600        544.96   4176.36   96.85   97.07   96.62    0.97\n",
            "  1     800        442.35   2654.19   96.95   96.66   97.25    0.97\n",
            "  1    1000        494.89   2130.90   97.16   97.12   97.20    0.97\n",
            "  1    1200        639.52   2552.18   97.43   97.25   97.60    0.97\n",
            "  1    1400        604.96   2187.56   97.59   97.69   97.49    0.98\n",
            "  2    1600        400.59   1400.26   97.57   97.50   97.64    0.98\n",
            "  2    1800        653.70   1720.20   97.56   97.41   97.71    0.98\n",
            "  2    2000        509.53   1464.32   97.56   97.58   97.53    0.98\n",
            "  2    2200        594.39   1587.61   97.80   97.67   97.93    0.98\n",
            "  3    2400        504.50   1065.92   97.76   97.59   97.94    0.98\n",
            "  3    2600        573.38   1105.09   97.80   97.83   97.76    0.98\n",
            "  3    2800        717.22   1257.21   97.83   97.85   97.81    0.98\n",
            "  4    3000        749.25   1272.73   97.82   97.78   97.86    0.98\n",
            "  4    3200        598.71    866.74   97.61   97.53   97.69    0.98\n",
            "  4    3400        898.90   1021.60   97.67   97.55   97.80    0.98\n",
            "  5    3600        986.26   1144.48   97.86   97.68   98.03    0.98\n",
            "  5    3800        691.05    860.46   97.94   97.93   97.96    0.98\n",
            "  5    4000       1181.27   1389.46   97.97   97.85   98.09    0.98\n",
            "  6    4200       1047.12   1136.05   97.93   97.89   97.97    0.98\n",
            "  7    4400       1082.91   1145.18   97.82   97.88   97.77    0.98\n",
            "  8    4600       1086.12   1096.12   98.06   97.95   98.16    0.98\n",
            "  9    4800       1220.93    968.09   98.05   97.92   98.18    0.98\n",
            "  9    5000       1174.21    887.49   97.93   97.84   98.02    0.98\n",
            " 10    5200       1011.52    718.78   98.03   97.91   98.16    0.98\n",
            " 11    5400        939.58    618.69   97.99   97.92   98.05    0.98\n",
            " 12    5600       1139.15    701.56   97.88   97.83   97.92    0.98\n",
            " 13    5800       1105.02    641.73   97.95   97.81   98.09    0.98\n",
            " 14    6000       1258.66    598.37   98.07   98.02   98.12    0.98\n",
            " 14    6200       1352.74    563.49   98.01   98.01   98.01    0.98\n",
            " 15    6400       1576.62    626.93   97.96   97.93   97.99    0.98\n",
            " 16    6600       1345.56    497.29   97.97   97.80   98.14    0.98\n",
            " 17    6800       1345.06    544.48   98.00   97.92   98.08    0.98\n",
            " 18    7000       1597.97    548.84   97.89   97.86   97.93    0.98\n",
            " 19    7200       1682.66    491.47   97.90   97.83   97.97    0.98\n",
            " 19    7400       1877.33    489.28   97.92   97.87   97.97    0.98\n",
            " 20    7600       1969.02    520.88   97.91   97.83   98.00    0.98\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на результаты обученной модели."
      ],
      "metadata": {
        "id": "tiOEC8ZZK07x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy evaluate output/model-last c5_valid.spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWNHGj8WBkx3",
        "outputId": "179c33c6-9b93-4344-cb70-a05af1dc421b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[38;5;4mℹ To switch to GPU 0, use the option: --gpu-id 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     -    \n",
            "NER P   97.83\n",
            "NER R   98.00\n",
            "NER F   97.91\n",
            "SPEED   26893\n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "               P       R       F\n",
            "T          99.09   99.30   99.20\n",
            "ORG        80.64   82.48   81.55\n",
            "PER        93.45   91.47   92.45\n",
            "GEOPOLIT   90.98   90.52   90.75\n",
            "MEDIA      82.62   82.40   82.51\n",
            "LOC        80.89   82.04   81.46\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **deeppavlov**"
      ],
      "metadata": {
        "id": "_gE05rOL1HEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде всего установим библиотеку и все необходимые зависимости."
      ],
      "metadata": {
        "id": "aR2_3L2hK9d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deeppavlov\n",
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5OoHYJdQCM1-",
        "outputId": "390b9f13-2611-41d5-9aa3-a715c7d7dc9c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeppavlov\n",
            "  Downloading deeppavlov-0.17.4-py3-none-any.whl (878 kB)\n",
            "\u001b[K     |████████████████████████████████| 878 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.7\n",
            "  Downloading uvicorn-0.11.7-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "  Downloading scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 54.4 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 51.6 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 70.3 MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting overrides==2.7.0\n",
            "  Downloading overrides-2.7.0.tar.gz (4.5 kB)\n",
            "Collecting pytz==2019.1\n",
            "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 53.2 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "  Downloading ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654 kB)\n",
            "\u001b[K     |████████████████████████████████| 654 kB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.4.417127.4579844)\n",
            "Collecting requests==2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.17.3)\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting pyopenssl==22.0.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting pydantic==1.3\n",
            "  Downloading pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 48.0 MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 51.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
            "\u001b[K     |████████████████████████████████| 859 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.62.0\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14.0\n",
            "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 62.0 MB/s \n",
            "\u001b[?25hCollecting prometheus-client==0.7.1\n",
            "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
            "Collecting Cython==0.29.14\n",
            "  Downloading Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 63.7 MB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting fastapi==0.47.1\n",
            "  Downloading fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "  Downloading aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 28 kB/s \n",
            "\u001b[?25hRequirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (1.8.1)\n",
            "Collecting aiormq<4,>=3.2.0\n",
            "  Downloading aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "  Downloading starlette-0.12.9.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting cryptography>=35.0\n",
            "  Downloading cryptography-37.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2022.6.15)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.1.0)\n",
            "Collecting websockets==8.*\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 12.2 MB/s \n",
            "\u001b[?25hCollecting httptools==0.1.*\n",
            "  Downloading httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 107.1 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading pamqp-2.3.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.1.1)\n",
            "Building wheels for collected packages: nltk, overrides, prometheus-client, pytelegrambotapi, sacremoses, starlette\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=22b8830fb61576506c767e7ee1f054c2110829e227fd3d3be2f606986d822733\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-py3-none-any.whl size=5603 sha256=60c608bb4e5821e03adc79b7c9c733daf5534db520cfc240d30461f0f473ec62\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/87/45/bfdacf6c3b8233b6e8d519edcbd1cf297ad5ff5f0bf84bb9c1\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41405 sha256=ef05d723960355e9f9ac0838dde66a2fe4f12b9a3858126c8b0d9384aa409663\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/0c/26/59ba285bf65dc79d195e9b25e2ddde4c61070422729b0cd914\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=47176 sha256=a5dd17802e1408d39417d1fffc138928b616745fde4cf2f2512cdaebbfa7bcfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/7c/54/8eddf2369ef1b9190e2ee6dc2b40df54b6c65529a38790fdd4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=ff7fe113ddb68cefc4124632c64d83149c929441ab04d8c66a2e5e6f011ed6d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ff/0e/e00ff1e22100702ac8b24e709551ae0fb29db9ffc843510a64\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-py3-none-any.whl size=57252 sha256=078b0a5e663fc4ef1e3b15223bd5f44fdc6f550177fe0c13ee4c135dccc6a13d\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/78/be/f57ed5aed7cd222abdb24e3186b5c9f1074184fcc0a295102b\n",
            "Successfully built nltk overrides prometheus-client pytelegrambotapi sacremoses starlette\n",
            "Installing collected packages: idna, pamqp, numpy, websockets, uvloop, tqdm, starlette, scipy, requests, pytz, pymorphy2-dicts, pydantic, httptools, h11, cryptography, aiormq, uvicorn, scikit-learn, sacremoses, rusenttokenize, ruamel.yaml, pytelegrambotapi, pyopenssl, pymorphy2, prometheus-client, pandas, overrides, nltk, h5py, filelock, fastapi, Cython, aio-pika, deeppavlov\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.2.1\n",
            "    Uninstalling pytz-2022.2.1:\n",
            "      Successfully uninstalled pytz-2022.2.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.9.2\n",
            "    Uninstalling pydantic-1.9.2:\n",
            "      Successfully uninstalled pydantic-1.9.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pymorphy2\n",
            "    Found existing installation: pymorphy2 0.9.1\n",
            "    Uninstalling pymorphy2-0.9.1:\n",
            "      Successfully uninstalled pymorphy2-0.9.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.32\n",
            "    Uninstalling Cython-0.29.32:\n",
            "      Successfully uninstalled Cython-0.29.32\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 0.25.3 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.0 which is incompatible.\n",
            "thinc 8.1.0 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.0 which is incompatible.\n",
            "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
            "ru-core-news-md 3.4.0 requires pymorphy2>=0.9, but you have pymorphy2 0.8 which is incompatible.\n",
            "prophet 1.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.0 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.0 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.18.0 which is incompatible.\n",
            "jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests>=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.0 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.18.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-37.0.4 deeppavlov-0.17.4 fastapi-0.47.1 filelock-3.0.12 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pyopenssl-22.0.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 scipy-1.4.1 starlette-0.12.9 tqdm-4.62.0 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "idna",
                  "nltk",
                  "numpy",
                  "pandas",
                  "pytz",
                  "requests",
                  "scipy",
                  "sklearn",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-30 19:53:09.768 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.5 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.0)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 66.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.47.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.8.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=dac58f0292db76a569d5b0c7ad9f753e475dbefbedc240e09b008a9174a36b91\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-qew9xv7k\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-qew9xv7k\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=23593 sha256=d239b121865d10f74b4f29460d6cfbd549b72d94747d9bbd137f00fb832212be\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u0d5qxeb/wheels/44/29/b2/ee614cb7f97ba5c2d220029eaede3af4b74331ad31d6e2f4eb\n",
            "Successfully built bert-dp\n",
            "Installing collected packages: bert-dp\n",
            "Successfully installed bert-dp-1.0\n",
            "2022-08-30 19:53:48.658 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15.5 in /usr/local/lib/python3.7/dist-packages (1.15.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.47.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.18.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.8.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 85.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 104.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.8)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как модели в библиотеке deeppavlov весьма тяжелые и долго обучаются, будем работать только с небольшой выборкой из нашего датасета."
      ],
      "metadata": {
        "id": "YjK8_bBVLGle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "id": "LliDv0DJUtpC",
        "outputId": "b431eb3b-6f42-4614-e7af-b179dd90d788"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.18.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.47.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model\n",
        "\n",
        "ner_model = build_model(configs.ner.ner_few_shot_ru, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6lGzgcdcR3A",
        "outputId": "dacae2f0-4961-49bf-a08c-8d1476847797"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-30 19:59:16.2 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz download because of matching hashes\n",
            "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz download because of matching hashes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deeppavlov.core.commands.utils import parse_config\n",
        "config_dict = parse_config(configs.ner.ner_few_shot_ru)\n",
        "print(config_dict['dataset_reader']['data_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oi0pbnhPn4s",
        "outputId": "6d2e3fad-d452-4c4b-9a2b-40a0d6679c7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~/.deeppavlov/downloads/ner_few_shot_data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.deeppavlov/downloads/ner_few_shot_data"
      ],
      "metadata": {
        "id": "LERbYiwiV32m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/root/.deeppavlov/downloads/ner_few_shot_data/all.txt', 'w') as w:\n",
        "    with open('/root/.deeppavlov/downloads/ner_few_shot_data/train.txt', 'w') as w1:\n",
        "        with open('/root/.deeppavlov/downloads/ner_few_shot_data/valid.txt', 'w') as w2:\n",
        "          with open('/root/.deeppavlov/downloads/ner_few_shot_data/test.txt', 'w') as w3:\n",
        "            for irec, rec in enumerate(docs):\n",
        "                for line in map(lambda vl: '\\t'.join(vl) + '\\n', zip(*rec)):\n",
        "                    w.write(line)\n",
        "                    if irec < 40:\n",
        "                        w1.write(line)\n",
        "                    elif irec < 45:\n",
        "                        w2.write(line)\n",
        "                    elif irec < 50:\n",
        "                        w3.write(line)\n",
        "                w.write('\\n')\n",
        "                if irec < 40:\n",
        "                    w1.write(line)\n",
        "                elif irec < 45:\n",
        "                    w2.write(line)\n",
        "                elif irec < 50:\n",
        "                    w3.write(line)"
      ],
      "metadata": {
        "id": "g4lKrbGXFoiJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.deeppavlov/downloads/ner_few_shot_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AK2FCiBit8M",
        "outputId": "2121979f-2ef1-4629-85eb-c83a0c66a053"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all.txt  test.txt  train.txt  valid.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m deeppavlov train ner_few_shot_ru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCQlf5VFQRb5",
        "outputId": "b41eb74b-a45c-43ff-b2b5-0d799f627db1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-30 20:04:46.30 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_few_shot_ru' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_few_shot_ru.json'\n",
            "2022-08-30 20:04:46.264 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: FitTrainer got additional init parameters ['epochs', 'validation_patience', 'val_every_n_epochs', 'log_every_n_epochs'] that will be ignored:\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
            "2022-08-30 20:04:47.893 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/embedders/elmo_embedder.py:186: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/embedders/elmo_embedder.py:188: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2022-08-30 20:04:50.408548: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2022-08-30 20:04:50.412998: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2249995000 Hz\n",
            "2022-08-30 20:04:50.413217: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a87480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2022-08-30 20:04:50.413242: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2022-08-30 20:04:50.415362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2022-08-30 20:04:50.423176: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-08-30 20:04:50.423209: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (9a46b7527935): /proc/driver/nvidia/version does not exist\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/embedders/elmo_embedder.py:190: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/models/embedders/elmo_embedder.py:198: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "2022-08-30 20:04:51.698863: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1524514816 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1524514816 bytes == 0x900bc000 @  0x7f1a4cdc6b6b 0x7f1a4cde6379 0x7f19eea5fee7 0x7f19ee84d51f 0x7f19ee71805b 0x7f19ee6dda36 0x7f19ee6de8c3 0x7f19ee6dea93 0x7f19f7147b89 0x7f19ee991e0c 0x7f19ee984575 0x7f19eea42021 0x7f19eea3f718 0x7f1a4b6c66df 0x7f1a4c7a86db 0x7f1a4cae161f\n",
            "2022-08-30 20:04:53.445187: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 103944192 exceeds 10% of system memory.\n",
            "2022-08-30 20:04:53.528889: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 203557376 exceeds 10% of system memory.\n",
            "2022-08-30 20:04:53.675636: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 779581440 exceeds 10% of system memory.\n",
            "2022-08-30 20:04:54.137338: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 398452736 exceeds 10% of system memory.\n",
            "2022-08-30 20:08:01.760 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "{\"valid\": {\"eval_examples_count\": 1, \"metrics\": {\"ner_f1\": 85.3767}, \"time_spent\": \"0:00:33\"}}\n",
            "{\"test\": {\"eval_examples_count\": 1, \"metrics\": {\"ner_f1\": 78.9866}, \"time_spent\": \"0:00:29\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deeppavlov import train_model\n",
        "ner_model = train_model(configs.ner.ner_few_shot_ru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THQ7nPJZp_9R",
        "outputId": "d73d377f-82a3-4bce-a204-51775c79da27"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-30 20:13:00.394 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: FitTrainer got additional init parameters ['epochs', 'validation_patience', 'val_every_n_epochs', 'log_every_n_epochs'] that will be ignored:\n",
            "INFO:deeppavlov.core.trainers.fit_trainer:FitTrainer got additional init parameters ['epochs', 'validation_patience', 'val_every_n_epochs', 'log_every_n_epochs'] that will be ignored:\n",
            "2022-08-30 20:13:00.402 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "INFO:deeppavlov.core.data.simple_vocab:[loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "2022-08-30 20:13:00.418 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "INFO:deeppavlov.core.data.simple_vocab:[saving vocabulary to /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "2022-08-30 20:16:16.807 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "INFO:deeppavlov.core.data.simple_vocab:[loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"valid\": {\"eval_examples_count\": 1, \"metrics\": {\"ner_f1\": 85.3767}, \"time_spent\": \"0:00:33\"}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-30 20:17:19.966 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n",
            "INFO:deeppavlov.core.data.simple_vocab:[loading vocabulary from /root/.deeppavlov/models/ner_fs/tag.dict]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"test\": {\"eval_examples_count\": 1, \"metrics\": {\"ner_f1\": 78.9866}, \"time_spent\": \"0:00:29\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_t = [docs[i][1] for i in range(45, 50)]\n",
        "y_true = [item for sublist in y_t for item in sublist]"
      ],
      "metadata": {
        "id": "Qfb0wEo_v1YL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(np.array(y_true))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYMX1bLawEzc",
        "outputId": "861921ae-84d1-4cf2-b8c4-061d452de032"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['B-GEOPOLIT', 'B-LOC', 'B-MEDIA', 'B-ORG', 'B-PER', 'I-LOC',\n",
              "       'I-MEDIA', 'I-ORG', 'I-PER', 'OUT'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_p = []\n",
        "for i in range(45, 50):\n",
        "  y_p.append(ner_model(docs[i][0])[1])\n",
        "\n",
        "y_pred = [item for sublist in y_p for item in sublist]\n",
        "y_pred = [item for sublist in y_pred for item in sublist]"
      ],
      "metadata": {
        "id": "cg6iNbukw6GM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(np.array(y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEJLA9Hxymgn",
        "outputId": "4075300c-36e0-43f9-942e-717d867a52ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['B-B-GEOPOLIT', 'B-B-LOC', 'B-B-MEDIA', 'B-B-ORG', 'B-B-PER',\n",
              "       'B-I-MEDIA', 'B-I-PER', 'B-OUT'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array(y_true)\n",
        "y_true[np.char.endswith(y_true, 'MEDIA')]='MEDIA'\n",
        "y_true[np.char.endswith(y_true, 'GEOPOLIT')]='GEOPOLIT'\n",
        "y_true[np.char.endswith(y_true, 'LOC')]='LOC'\n",
        "y_true[np.char.endswith(y_true, 'PER')]='PER'\n",
        "y_true[np.char.endswith(y_true, 'ORG')]='ORG'\n",
        "y_true[np.char.endswith(y_true, 'OUT')]='OUT'\n",
        "np.unique(y_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klAMqd0qaUCN",
        "outputId": "113f47db-9905-4473-e024-72d9557b1da0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array(y_pred)\n",
        "y_pred[np.char.endswith(y_pred, 'MEDIA')]='MEDIA'\n",
        "y_pred[np.char.endswith(y_pred, 'GEOPOLIT')]='GEOPOLIT'\n",
        "y_pred[np.char.endswith(y_pred, 'LOC')]='LOC'\n",
        "y_pred[np.char.endswith(y_pred, 'PER')]='PER'\n",
        "y_pred[np.char.endswith(y_pred, 'ORG')]='ORG'\n",
        "y_pred[np.char.endswith(y_pred, 'OUT')]='OUT'\n",
        "np.unique(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoP_Fjv6aGN0",
        "outputId": "5e5532b4-8fba-4296-ceb6-ec26f25638eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype='<U12')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZdc628V2C7J",
        "outputId": "cfdff89a-da46-48c9-e7de-530006dc9780"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    GEOPOLIT       0.87      0.81      0.84        16\n",
            "         LOC       0.41      0.39      0.40        31\n",
            "       MEDIA       1.00      0.64      0.78        11\n",
            "         ORG       0.94      0.20      0.33        86\n",
            "         OUT       0.93      1.00      0.96      1238\n",
            "         PER       0.95      0.78      0.85       121\n",
            "\n",
            "    accuracy                           0.92      1503\n",
            "   macro avg       0.85      0.63      0.69      1503\n",
            "weighted avg       0.92      0.92      0.90      1503\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим для классов `GEOPOLIT`, `PER`, `MEDIA` результаты получились довольно неплохими даже несмотря на небольшое количество элементов для обучения. Тем не менее на примере `LOC` и `ORG` видно, что модель требуется в более внушительном объеме данных для обучения и получения по-настоящему хороших результатов."
      ],
      "metadata": {
        "id": "glIllHtLbDVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Нейронные сети**"
      ],
      "metadata": {
        "id": "Dczc4KjA2ksk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==2.2.0\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn import model_selection, preprocessing, linear_model"
      ],
      "metadata": {
        "id": "Z76s2kPF3Bn5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'Collection5/'\n",
        "records = load_ne5(dir)\n",
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        type_ent = 'OUT'\n",
        "        for ent in rec.spans:\n",
        "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = ent.type\n",
        "                break\n",
        "        words.append([token.text, type_ent])\n",
        "    words_docs.extend(words)"
      ],
      "metadata": {
        "id": "sG313Cvy4BZk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
        "df_words['tag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jSpGvQa2snV",
        "outputId": "00eccf4d-9998-4a14-a17a-0a4b8ac43241"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OUT         219214\n",
              "PER          21200\n",
              "ORG          13651\n",
              "LOC           4568\n",
              "GEOPOLIT      4356\n",
              "MEDIA         2482\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_words.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "c5l85Dyf2vJE",
        "outputId": "f1850bad-881e-45f3-edf4-260bc343bab9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       word  tag\n",
              "0         Д  PER\n",
              "1         .  PER\n",
              "2  Медведев  PER"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0011c259-b164-4ec8-9312-f34197500f54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Д</td>\n",
              "      <td>PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "      <td>PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Медведев</td>\n",
              "      <td>PER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0011c259-b164-4ec8-9312-f34197500f54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0011c259-b164-4ec8-9312-f34197500f54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0011c259-b164-4ec8-9312-f34197500f54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
        "\n",
        "# labelEncode целевую переменную\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "metadata": {
        "id": "iEanVceF3Dqo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "train_data = train_data.batch(16)\n",
        "valid_data = valid_data.batch(16)\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "l701LsFO3T94"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    return input_data\n",
        "\n",
        "vocab_size = 30000\n",
        "seq_len = 10\n",
        "\n",
        "# без соседних токенов \n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    # ngrams=(1, 3),\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "\n",
        "# с соседними токенами\n",
        "vectorize_layer_n13 = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    ngrams=(1, 3),\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "vectorize_layer_n4 = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    ngrams=4,\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ],
      "metadata": {
        "id": "rwfonxry3Wm5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "\n",
        "modeln = Sequential([\n",
        "    vectorize_layer,\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    Conv1D(embedding_dim, 3),\n",
        "    Conv1D(embedding_dim, 2),\n",
        "    GRU(350),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "p3P3QzUW3bYH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeln.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VX2SgXgQAfhg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeln.fit(train_data, validation_data=valid_data, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzOOuNhqBA6Q",
        "outputId": "070415ed-7b9c-4e39-b10f-e708aa562d13"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "12444/12444 [==============================] - 465s 37ms/step - loss: 0.3329 - accuracy: 0.9007 - val_loss: 0.2307 - val_accuracy: 0.9318\n",
            "Epoch 2/3\n",
            "12444/12444 [==============================] - 480s 39ms/step - loss: 0.1543 - accuracy: 0.9538 - val_loss: 0.2540 - val_accuracy: 0.9395\n",
            "Epoch 3/3\n",
            "12444/12444 [==============================] - 529s 43ms/step - loss: 0.1251 - accuracy: 0.9624 - val_loss: 0.2816 - val_accuracy: 0.9394\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0d0e37dd10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer_n13.adapt(text_data)\n",
        "\n",
        "modeln13 = Sequential([\n",
        "    vectorize_layer_n13,\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    Conv1D(embedding_dim, 3),\n",
        "    Conv1D(embedding_dim, 2),\n",
        "    GRU(350),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "modeln13.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modeln13.fit(train_data, validation_data=valid_data, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHanBf7XhnXq",
        "outputId": "2200ee0a-2452-490d-8053-d0cbe98a53ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "12444/12444 [==============================] - 477s 38ms/step - loss: 0.3215 - accuracy: 0.9055 - val_loss: 0.2315 - val_accuracy: 0.9332\n",
            "Epoch 2/3\n",
            "12444/12444 [==============================] - 455s 37ms/step - loss: 0.1491 - accuracy: 0.9559 - val_loss: 0.2307 - val_accuracy: 0.9388\n",
            "Epoch 3/3\n",
            "12444/12444 [==============================] - 461s 37ms/step - loss: 0.1230 - accuracy: 0.9625 - val_loss: 0.4162 - val_accuracy: 0.8903\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0d0b51b2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer_n4.adapt(text_data)\n",
        "\n",
        "modeln4 = Sequential([\n",
        "    vectorize_layer_n4,\n",
        "    Embedding(vocab_size, embedding_dim),\n",
        "    Conv1D(embedding_dim, 3),\n",
        "    Conv1D(embedding_dim, 2),\n",
        "    GRU(350),\n",
        "    Dense(200, activation='relu'),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "modeln4.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "modeln4.fit(train_data, validation_data=valid_data, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OavJ--Ah0FO",
        "outputId": "fadb2439-b392-4849-e6a4-9a503183062e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "12444/12444 [==============================] - 456s 37ms/step - loss: 0.3275 - accuracy: 0.9030 - val_loss: 0.2316 - val_accuracy: 0.9328\n",
            "Epoch 2/3\n",
            "12444/12444 [==============================] - 464s 37ms/step - loss: 0.1521 - accuracy: 0.9544 - val_loss: 0.2333 - val_accuracy: 0.9375\n",
            "Epoch 3/3\n",
            "12444/12444 [==============================] - 461s 37ms/step - loss: 0.1261 - accuracy: 0.9623 - val_loss: 0.2298 - val_accuracy: 0.9390\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0d09dadf90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим метрики accuracy для всех сетей получились довольно близкими, построим на остальные метрики. "
      ],
      "metadata": {
        "id": "fZ7Rkbo0uYsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_predict_n = modeln.predict(valid_data)\n",
        "labels_predict_n13 = modeln13.predict(valid_data)\n",
        "labels_predict_n4 = modeln4.predict(valid_data)"
      ],
      "metadata": {
        "id": "s1HJ2cPZFwUW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_preds_n = np.argmax(tf.nn.softmax(labels_predict_n), axis=1)\n",
        "class_preds_n13 = np.argmax(tf.nn.softmax(labels_predict_n13), axis=1)\n",
        "class_preds_n4 = np.argmax(tf.nn.softmax(labels_predict_n4), axis=1)"
      ],
      "metadata": {
        "id": "ImFyjvx-urss"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_y = encoder.inverse_transform(valid_y)\n",
        "class_preds_n = encoder.inverse_transform(class_preds_n)\n",
        "class_preds_n13 = encoder.inverse_transform(class_preds_n13)\n",
        "class_preds_n4 = encoder.inverse_transform(class_preds_n4)"
      ],
      "metadata": {
        "id": "c_nguFiivH5J"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(valid_y, class_preds_n))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KlZDFMDGTFZ",
        "outputId": "ccef13ca-139b-4d0d-80c1-08c116e231ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    GEOPOLIT       0.89      0.88      0.88      1079\n",
            "         LOC       0.83      0.80      0.81      1161\n",
            "       MEDIA       0.93      0.69      0.79       633\n",
            "         ORG       0.85      0.56      0.67      3397\n",
            "         OUT       0.94      0.99      0.97     54751\n",
            "         PER       0.96      0.72      0.83      5347\n",
            "\n",
            "    accuracy                           0.94     66368\n",
            "   macro avg       0.90      0.77      0.83     66368\n",
            "weighted avg       0.94      0.94      0.94     66368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(valid_y, class_preds_n13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnVNfjZ8GTPT",
        "outputId": "038e21db-32ad-40b6-ca1b-c31c0cfff931"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    GEOPOLIT       0.89      0.86      0.88      1079\n",
            "         LOC       0.81      0.80      0.81      1161\n",
            "       MEDIA       0.94      0.73      0.82       633\n",
            "         ORG       0.83      0.55      0.67      3397\n",
            "         OUT       0.97      0.92      0.94     54751\n",
            "         PER       0.49      0.88      0.63      5347\n",
            "\n",
            "    accuracy                           0.89     66368\n",
            "   macro avg       0.82      0.79      0.79     66368\n",
            "weighted avg       0.92      0.89      0.90     66368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(valid_y, class_preds_n4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-shX1aCF6X3",
        "outputId": "cf35498d-385a-41a3-bab2-9ae5c21f9282"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    GEOPOLIT       0.90      0.87      0.88      1079\n",
            "         LOC       0.85      0.78      0.81      1161\n",
            "       MEDIA       0.92      0.73      0.82       633\n",
            "         ORG       0.86      0.55      0.67      3397\n",
            "         OUT       0.94      0.99      0.97     54751\n",
            "         PER       0.98      0.70      0.82      5347\n",
            "\n",
            "    accuracy                           0.94     66368\n",
            "   macro avg       0.91      0.77      0.83     66368\n",
            "weighted avg       0.94      0.94      0.93     66368\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим показатели precision/recall/f1 тоже довольно близки для построенных сетей, однако, на мой взгляд, первая сеть, которая не берет в расчет соседние токены показала лучшие результаты. Довольно близка к ней по качеству сеть, построенная с параметром ngrams=4. Она даже лучше справляется с детектированием класса `MEDIA`.\n",
        "\n",
        "Тем не менее стоит отметить, что модель построенная с помощью библиотеки __spacy__ показала самые лучшие результаты."
      ],
      "metadata": {
        "id": "73JEla7Vvx6h"
      }
    }
  ]
}