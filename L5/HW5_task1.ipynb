{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_task1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Домашнее задание 5"
      ],
      "metadata": {
        "id": "MRnlbcVgJkVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1.** Написать теггер на данных с русским языком\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
        "2. написать свой теггер как на занятии, попробовать разные векторайзеры добавить знание не только букв но и слов\n",
        "3. сравнить все реализованные методы, сделать выводы\n"
      ],
      "metadata": {
        "id": "drdGBumEJpvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решение:"
      ],
      "metadata": {
        "id": "8QvqmCqyVmGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим и импортируем необходимые библиотеки."
      ],
      "metadata": {
        "id": "txqCpSXZm_BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyconll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhQxLIgFPQi7",
        "outputId": "d569a123-0050-4c48-8cc6-1f3fb76bcfa9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAkiRYo-JiMo",
        "outputId": "cb71e98a-1538-4449-a8cb-8d7f4c6225c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('tagsets')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from nltk.tag import DefaultTagger\n",
        "from nltk.tag import UnigramTagger\n",
        "from nltk.tag import BigramTagger, TrigramTagger\n",
        "from nltk.tag import RegexpTagger\n",
        "\n",
        "import pyconll\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим датасет и соберем обучающую и тестовую выборки."
      ],
      "metadata": {
        "id": "bEzligyWnNCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset_ru\n",
        "\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NsnpCCiO5Gp",
        "outputId": "ffbd2301-e906-4e4c-937b-ae92562943ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-27 18:42:19--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736565 (39M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  38.85M   190MB/s    in 0.2s    \n",
            "\n",
            "2022-08-27 18:42:20 (190 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’ saved [40736565/40736565]\n",
            "\n",
            "--2022-08-27 18:42:20--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42819806 (41M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  40.84M   198MB/s    in 0.2s    \n",
            "\n",
            "2022-08-27 18:42:21 (198 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’ saved [42819806/42819806]\n",
            "\n",
            "--2022-08-27 18:42:21--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32367464 (31M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  30.87M   182MB/s    in 0.2s    \n",
            "\n",
            "2022-08-27 18:42:22 (182 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’ saved [32367464/32367464]\n",
            "\n",
            "--2022-08-27 18:42:22--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14704579 (14M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  14.02M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-08-27 18:42:22 (121 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_train = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-a.conllu')\n",
        "full_train_b = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-b.conllu')\n",
        "full_train_c = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-c.conllu')\n",
        "\n",
        "# Общая обучающая выборка\n",
        "full_train.extend([*full_train_b, *full_train_c])\n",
        "\n",
        "full_test = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-dev.conllu')"
      ],
      "metadata": {
        "id": "_tYtTRIEPff_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример текста из тестового датасета с соответсвующими тэгами."
      ],
      "metadata": {
        "id": "MXg9r6oZoRA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in full_test[:1]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYh6KtHLUT_n",
        "outputId": "d53fad3d-2ab5-410a-aec5-4b1fe3820869"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Алгоритм NOUN\n",
            ", PUNCT\n",
            "от ADP\n",
            "имени NOUN\n",
            "учёного NOUN\n",
            "аль PART\n",
            "- PUNCT\n",
            "Хорезми PROPN\n",
            ", PUNCT\n",
            "- PUNCT\n",
            "точный ADJ\n",
            "набор NOUN\n",
            "инструкций NOUN\n",
            ", PUNCT\n",
            "описывающих VERB\n",
            "порядок NOUN\n",
            "действий NOUN\n",
            "исполнителя NOUN\n",
            "для ADP\n",
            "достижения NOUN\n",
            "результата NOUN\n",
            "решения NOUN\n",
            "задачи NOUN\n",
            "за ADP\n",
            "конечное ADJ\n",
            "время NOUN\n",
            ". PUNCT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразовываем выборки для удобства дальнейшего использования при обучении и тестировании."
      ],
      "metadata": {
        "id": "MNZi0Ir5pRxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "metadata": {
        "id": "pmVq7DqaRo8h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем разные стандартные методы для определения тэгов слов."
      ],
      "metadata": {
        "id": "rjw5u-blmYR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(fdata_sent_test[50]), unigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "GeaztuQ5TK8h",
        "outputId": "8524b2ce-004c-4f55-8845-1f54df6c2ef8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', 'PRON'),\n",
              " ('алгорифм', None),\n",
              " ('(', 'PUNCT'),\n",
              " ('кстати', 'ADV'),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', 'VERB'),\n",
              " ('написание', 'NOUN'),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', 'ADP'),\n",
              " ('фиту', None),\n",
              " (')', 'PUNCT'),\n",
              " ('производится', 'VERB'),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('арабского', 'ADJ'),\n",
              " ('слова', 'NOUN'),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', 'SCONJ'),\n",
              " ('есть', 'VERB'),\n",
              " ('корень', 'NOUN'),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8782863467673677"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(fdata_sent_test[50]), bigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "colA3J_dXQCv",
        "outputId": "5bdb899e-a498-4bac-b787-81d52b016e60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', 'PRON'),\n",
              " ('алгорифм', None),\n",
              " ('(', 'PUNCT'),\n",
              " ('кстати', 'ADV'),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', None),\n",
              " ('написание', None),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', 'ADP'),\n",
              " ('фиту', None),\n",
              " (')', 'PUNCT'),\n",
              " ('производится', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('арабского', 'ADJ'),\n",
              " ('слова', 'NOUN'),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', 'SCONJ'),\n",
              " ('есть', 'VERB'),\n",
              " ('корень', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.7101308678950452"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(fdata_sent_test[50]), trigram_tagger.evaluate(fdata_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "QyRNo_0TXYH8",
        "outputId": "c27016b0-2274-4593-ea02-533e29291aad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('В', 'ADP'),\n",
              " ('нём', None),\n",
              " ('алгорифм', None),\n",
              " ('(', None),\n",
              " ('кстати', None),\n",
              " (',', 'PUNCT'),\n",
              " ('до', 'ADP'),\n",
              " ('революции', 'NOUN'),\n",
              " ('использовалось', None),\n",
              " ('написание', None),\n",
              " ('алгорифм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('через', None),\n",
              " ('фиту', None),\n",
              " (')', None),\n",
              " ('производится', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('от', None),\n",
              " ('арабского', None),\n",
              " ('слова', None),\n",
              " ('Аль-Горетм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('то', None),\n",
              " ('есть', None),\n",
              " ('корень', None),\n",
              " ('\"', 'PUNCT'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4067191874470994"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим самые лучшие результаты показывает UnigramTagger, который не использует соседние слова для определения тэга слова.\n",
        "\n",
        "Попробуем создать комбинацию тэггеров, чтобы уменьшить количество нераспознаваемых слов, и посмотрим, как это повлияет на точность модели."
      ],
      "metadata": {
        "id": "c3EBk9YOqT1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "# В качестве бэкофф опции будем использовать тэг существительного\n",
        "backoff = DefaultTagger('NOUN') \n",
        "tag = backoff_tagger(fdata_train,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "tag.evaluate(fdata_test) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peXMw_y2YpvO",
        "outputId": "f59ebe78-1112-464f-df56-623a154ed2c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9119799466111075"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим благодаря этому подходу удалось добиться улучшения точности модели."
      ],
      "metadata": {
        "id": "2z6RMSXornWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь построим модель тэггера на основе разных векторайзеров, которая будет фактически классифицировать каждое слово из датасета. Для этого для начала подготовим обучающий и тестовый наборы данных. "
      ],
      "metadata": {
        "id": "9DMMqDzZtxEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "metadata": {
        "id": "BzfpUSN0f9Q6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)\n",
        "test_enc_labels = le.transform(test_label)"
      ],
      "metadata": {
        "id": "0cgHGW-LgARH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fRC0ngNgEwG",
        "outputId": "9011e87b-572b-4bfa-ff95-43d245bbaae3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
              "       'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'],\n",
              "      dtype='<U5')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим несколько векторайзеров и обучим модель на их основе. Затем сравним получившиеся результаты."
      ],
      "metadata": {
        "id": "DKdOUH9NyQgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(2, 15), analyzer='char', n_features=65536)\n",
        "tvectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
        "cvectorizer = CountVectorizer(ngram_range=(2, 13), analyzer='char')"
      ],
      "metadata": {
        "id": "w5nKmQuXgHHI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xh_train = hvectorizer.fit_transform(train_tok)\n",
        "Xh_test = hvectorizer.transform(test_tok)\n",
        "\n",
        "Xt_train = tvectorizer.fit_transform(train_tok)\n",
        "Xt_test = tvectorizer.transform(test_tok)\n",
        "\n",
        "Xc_train = cvectorizer.fit_transform(train_tok)\n",
        "Xc_test = cvectorizer.transform(test_tok)"
      ],
      "metadata": {
        "id": "TPRhejj_gLDv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xh_train, train_enc_labels)\n",
        "pred = lr.predict(Xh_test)\n",
        "print(f'Accuracy для модели тэггера на основе HashingVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt09cNWdgPP-",
        "outputId": "0a571ccf-f4f1-4b67-f606-d54de02b5907"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе HashingVectorizer - 0.8609.\n",
            "CPU times: user 22min 9s, sys: 2min 50s, total: 24min 59s\n",
            "Wall time: 21min 55s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xt_train, train_enc_labels)\n",
        "pred = lr.predict(Xt_test)\n",
        "print(f'Accuracy для модели тэггера на основе TfidfVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq5VDTepnfhE",
        "outputId": "76663c69-0948-45da-c69e-fd838333b4ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе TfidfVectorizer - 0.7563.\n",
            "CPU times: user 16min 12s, sys: 3min 2s, total: 19min 14s\n",
            "Wall time: 15min 38s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lr = LogisticRegression(random_state=42, max_iter=500)\n",
        "lr.fit(Xc_train, train_enc_labels)\n",
        "pred = lr.predict(Xc_test)\n",
        "print(f'Accuracy для модели тэггера на основе CountVectorizer - {accuracy_score(test_enc_labels, pred):.4f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsd-S0sMz8Ua",
        "outputId": "3151e5b4-c8a2-44ba-de7d-4526b1353535"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy для модели тэггера на основе CountVectorizer - 0.8695.\n",
            "CPU times: user 53min 7s, sys: 9min, total: 1h 2min 8s\n",
            "Wall time: 53min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим наилучшая точность получилась среди векторайзеров получилась для модели CountVectorizer на основе букв - 0.8695. Тем не менее комбинация стандартных тэггеров рассмотренная ранее позволила добиться еще большей точности в определении тэгов слов - 0.9120. "
      ],
      "metadata": {
        "id": "DfKhJRP52hdY"
      }
    }
  ]
}